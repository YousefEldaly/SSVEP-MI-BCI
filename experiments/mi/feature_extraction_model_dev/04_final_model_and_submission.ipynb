{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98188,"databundleVersionId":12673416,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom scipy.signal import butter, filtfilt, iirnotch\n\n# Configuration\nBASE_PATH = '/kaggle/input/mtcaic3'\nOUTPUT_DIR = './preprocessed'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Filter design\ndef design_filters(fs=250.0):\n    # Bandpass 1-40 Hz\n    bp_b, bp_a = butter(4, [1/(fs/2), 40/(fs/2)], btype='band')\n    # Notch at 50 Hz\n    notch_b, notch_a = iirnotch(50/(fs/2), Q=30)\n    return bp_b, bp_a, notch_b, notch_a\n\n# Preprocessing steps for one trial\ndef preprocess_trial(df):\n    eeg_cols = ['FZ','C3','CZ','C4','PZ','PO7','OZ','PO8']\n    motion_cols = ['AccX','AccY','AccZ','Gyro1','Gyro2','Gyro3']\n    val_col = 'Validation'\n\n    # 1) Motion artifact detection\n    motion_mag = np.sqrt((df[motion_cols]**2).sum(axis=1))\n    mot_thresh = np.percentile(motion_mag, 95)\n    bad_mask = motion_mag > mot_thresh\n\n    # 2) Mask EEG\n    data = df[eeg_cols].copy().values\n    data[bad_mask, :] = np.nan\n    data[df[val_col] == 0, :] = np.nan\n\n    # 3) Interpolation\n    for ch in range(data.shape[1]):\n        col = data[:, ch]\n        nans = np.isnan(col)\n        if nans.all():\n            continue\n        idx = np.arange(len(col))\n        data[nans, ch] = np.interp(idx[nans], idx[~nans], col[~nans])\n\n    # 4) Filtering\n    bp_b, bp_a, notch_b, notch_a = design_filters()\n    for ch in range(data.shape[1]):\n        data[:, ch] = filtfilt(bp_b, bp_a, data[:, ch])\n        data[:, ch] = filtfilt(notch_b, notch_a, data[:, ch])\n\n    # 5) Baseline correction (first 0.5s)\n    bs = int(0.5 * 250)\n    baseline = data[:bs].mean(axis=0)\n    data -= baseline\n    return data\n\n# Load index DataFrame\ndef load_index(fname, label_col=True):\n    df = pd.read_csv(os.path.join(BASE_PATH, fname))\n    cols = ['id','subject_id','task','trial_session','trial'] + (['label'] if label_col else [])\n    return df[cols]\n\n# Process a split, grouping by task\ndef process_split(df, has_label=True):\n    data_dict = {'MI': {'X': [], 'y': []}, 'SSVEP': {'X': [], 'y': []}}\n    for _, row in df.iterrows():\n        # Identify folder\n        idx = row['id']\n        split = 'train' if idx <= 4800 else 'validation' if idx <= 4900 else 'test'\n        # Load EEGdata\n        path = os.path.join(BASE_PATH, row['task'], split, row['subject_id'], str(row['trial_session']), 'EEGdata.csv')\n        df_eeg = pd.read_csv(path)\n        # Extract correct segment\n        n_samp = 2250 if row['task']=='MI' else 1750\n        start = (row['trial']-1)*n_samp\n        seg = df_eeg.iloc[int(start):int(start+n_samp)].reset_index(drop=True)\n        proc = preprocess_trial(seg)\n        data_dict[row['task']]['X'].append(proc.T)\n        if has_label:\n            data_dict[row['task']]['y'].append(row['label'])\n    # Stack\n    for t in data_dict:\n        X = np.stack(data_dict[t]['X'])\n        y = np.array(data_dict[t]['y']) if has_label else None\n        data_dict[t]['X'] = X\n        data_dict[t]['y'] = y\n    return data_dict\n\n# Execute processing and save\nfor fname, label_col in [('train.csv', True), ('validation.csv', True), ('test.csv', False)]:\n    df_idx = load_index(fname, label_col)\n    results = process_split(df_idx, label_col)\n    for task, d in results.items():\n        out_file = f\"{os.path.splitext(fname)[0]}_{task}.npz\"\n        path = os.path.join(OUTPUT_DIR, out_file)\n        if label_col:\n            np.savez_compressed(path, X=d['X'], y=d['y'])\n        else:\n            np.savez_compressed(path, X=d['X'])\n        print(f\"Saved {out_file}: X={d['X'].shape}\" + (f\", y={d['y'].shape}\" if d['y'] is not None else ''))\nprint('Preprocessing complete.')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:18:53.559765Z","iopub.execute_input":"2025-06-29T10:18:53.560434Z","iopub.status.idle":"2025-06-29T10:18:56.628201Z","shell.execute_reply.started":"2025-06-29T10:18:53.560399Z","shell.execute_reply":"2025-06-29T10:18:56.627303Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/156545177.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_col\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'validation.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mdf_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mout_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{os.path.splitext(fname)[0]}_{task}.npz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/156545177.py\u001b[0m in \u001b[0;36mprocess_split\u001b[0;34m(df, has_label)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trial'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_samp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_eeg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn_samp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_label\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/156545177.py\u001b[0m in \u001b[0;36mpreprocess_trial\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mnans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mnans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# 4) Filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36minterp\u001b[0;34m(x, xp, fp, left, right, period)\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minterp_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# ▶️ Kaggle‑runnable cell: MI CNN training\nimport os\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\n\n# 1) Load preprocessed MI data\ntrain = np.load('/kaggle/working/preprocessed/train_MI.npz')\nX_train, y_train = train['X'], train['y']\nval   = np.load('/kaggle/working/preprocessed/validation_MI.npz')\nX_val,   y_val   = val['X'],   val['y']\n\n# ensure float32 and channel‑first → channel‑last for Keras\nX_train = X_train.astype('float32').transpose(0,2,1)\nX_val   = X_val.astype('float32').transpose(0,2,1)\n\n# one‑hot labels\nnum_classes = len(np.unique(y_train))\ny_train = keras.utils.to_categorical(y_train == 'Right', num_classes=2)\ny_val   = keras.utils.to_categorical(y_val   == 'Right', num_classes=2)\n\n# 2) Data augmentation generator\ndef aug_generator(X, y, batch_size=32):\n    n_samples = X.shape[0]\n    while True:\n        idx = np.random.randint(0, n_samples, batch_size)\n        batch_x = X[idx].copy()\n        batch_y = y[idx]\n        # add small Gaussian noise\n        noise = np.random.normal(0, 0.01, batch_x.shape)\n        batch_x += noise\n        # random time shift up to ±50 samples\n        shifts = np.random.randint(-50, 51, batch_size)\n        for i, s in enumerate(shifts):\n            batch_x[i] = np.roll(batch_x[i], s, axis=0)\n        yield batch_x, batch_y\n\ntrain_gen = aug_generator(X_train, y_train, batch_size=64)\n\n# 3) Build 1D‑CNN model\ninput_shape = X_train.shape[1:]  # (samples, channels)\nmodel = keras.Sequential([\n    layers.Input(input_shape),\n    layers.Conv1D(32, 5, activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPool1D(2),\n    layers.Conv1D(64, 5, activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPool1D(2),\n    layers.Conv1D(128, 5, activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(num_classes, activation='softmax'),\n])\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-3),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\nmodel.summary()\n\n# 4) Training with callbacks\nckpt = keras.callbacks.ModelCheckpoint(\n    'best_mi_cnn.h5', monitor='val_accuracy', save_best_only=True\n)\nhist = model.fit(\n    train_gen,\n    steps_per_epoch=len(X_train)//64,\n    validation_data=(X_val, y_val),\n    epochs=50,\n    callbacks=[ckpt]\n)\n\n# 5) Plot training curves\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nplt.plot(hist.history['loss'], label='train')\nplt.plot(hist.history['val_loss'], label='val')\nplt.title('Loss'); plt.legend()\nplt.subplot(1,2,2)\nplt.plot(hist.history['accuracy'], label='train')\nplt.plot(hist.history['val_accuracy'], label='val')\nplt.title('Accuracy'); plt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T07:16:18.015887Z","iopub.execute_input":"2025-06-26T07:16:18.016142Z","iopub.status.idle":"2025-06-26T07:18:05.690802Z","shell.execute_reply.started":"2025-06-26T07:16:18.016122Z","shell.execute_reply":"2025-06-26T07:18:05.690067Z"}},"outputs":[{"name":"stderr","text":"2025-06-26 07:16:19.415914: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750922179.589465      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750922179.640203      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nI0000 00:00:1750922193.650741      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1750922193.651313      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2250\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │           \u001b[38;5;34m1,312\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2250\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1125\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1125\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m10,304\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1125\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m562\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m562\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │          \u001b[38;5;34m41,088\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m562\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m130\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,312</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">562</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">562</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">562</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,986\u001b[0m (242.13 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,986</span> (242.13 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61,538\u001b[0m (240.38 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,538</span> (240.38 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1750922198.723475     102 service.cc:148] XLA service 0x7db3ec056d20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1750922198.724152     102 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1750922198.724172     102 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1750922199.153826     102 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 4/37\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.4990 - loss: 0.8233","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1750922202.539321     102 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 87ms/step - accuracy: 0.4985 - loss: 0.8108 - val_accuracy: 0.5200 - val_loss: 0.8597\nEpoch 2/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.5299 - loss: 0.7415 - val_accuracy: 0.5800 - val_loss: 0.8089\nEpoch 3/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.4841 - loss: 0.7319 - val_accuracy: 0.5800 - val_loss: 0.6922\nEpoch 4/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.4833 - loss: 0.7169 - val_accuracy: 0.4800 - val_loss: 0.7108\nEpoch 5/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5119 - loss: 0.7050 - val_accuracy: 0.5200 - val_loss: 0.7849\nEpoch 6/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5072 - loss: 0.7002 - val_accuracy: 0.4200 - val_loss: 0.7442\nEpoch 7/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.5201 - loss: 0.6937 - val_accuracy: 0.5000 - val_loss: 0.7481\nEpoch 8/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.5063 - loss: 0.7025 - val_accuracy: 0.5400 - val_loss: 0.7305\nEpoch 9/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5565 - loss: 0.6887 - val_accuracy: 0.5000 - val_loss: 0.6968\nEpoch 10/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.5414 - loss: 0.6848 - val_accuracy: 0.6200 - val_loss: 0.7385\nEpoch 11/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5398 - loss: 0.6858 - val_accuracy: 0.4200 - val_loss: 0.7060\nEpoch 12/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.4939 - loss: 0.7042 - val_accuracy: 0.5800 - val_loss: 0.7353\nEpoch 13/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5383 - loss: 0.6904 - val_accuracy: 0.4400 - val_loss: 0.7316\nEpoch 14/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.5338 - loss: 0.6885 - val_accuracy: 0.4400 - val_loss: 0.7207\nEpoch 15/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.5285 - loss: 0.6885 - val_accuracy: 0.5200 - val_loss: 0.7171\nEpoch 16/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5294 - loss: 0.6941 - val_accuracy: 0.4600 - val_loss: 0.7202\nEpoch 17/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5355 - loss: 0.6861 - val_accuracy: 0.5400 - val_loss: 0.7226\nEpoch 18/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5685 - loss: 0.6788 - val_accuracy: 0.5400 - val_loss: 0.8618\nEpoch 19/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5534 - loss: 0.6840 - val_accuracy: 0.4800 - val_loss: 0.7239\nEpoch 20/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.5539 - loss: 0.6874 - val_accuracy: 0.5000 - val_loss: 0.7252\nEpoch 21/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5452 - loss: 0.6840 - val_accuracy: 0.4800 - val_loss: 0.7212\nEpoch 22/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.5469 - loss: 0.6877 - val_accuracy: 0.6000 - val_loss: 0.7422\nEpoch 23/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.5504 - loss: 0.6861 - val_accuracy: 0.5000 - val_loss: 0.7576\nEpoch 24/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.5511 - loss: 0.6784 - val_accuracy: 0.5600 - val_loss: 0.7622\nEpoch 25/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5737 - loss: 0.6715 - val_accuracy: 0.5600 - val_loss: 0.8645\nEpoch 26/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5628 - loss: 0.6906 - val_accuracy: 0.4400 - val_loss: 0.8939\nEpoch 27/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5386 - loss: 0.6851 - val_accuracy: 0.5200 - val_loss: 0.7844\nEpoch 28/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5427 - loss: 0.6877 - val_accuracy: 0.5200 - val_loss: 0.8106\nEpoch 29/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5586 - loss: 0.6806 - val_accuracy: 0.5200 - val_loss: 0.8767\nEpoch 30/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5417 - loss: 0.6871 - val_accuracy: 0.6000 - val_loss: 0.7462\nEpoch 31/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.5550 - loss: 0.6755 - val_accuracy: 0.5600 - val_loss: 1.0243\nEpoch 32/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.5531 - loss: 0.6801 - val_accuracy: 0.4600 - val_loss: 0.8408\nEpoch 33/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5783 - loss: 0.6789 - val_accuracy: 0.5200 - val_loss: 0.9184\nEpoch 34/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5762 - loss: 0.6702 - val_accuracy: 0.5800 - val_loss: 0.7913\nEpoch 35/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5685 - loss: 0.6764 - val_accuracy: 0.4600 - val_loss: 0.8840\nEpoch 36/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5633 - loss: 0.6844 - val_accuracy: 0.4800 - val_loss: 0.9632\nEpoch 37/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5476 - loss: 0.6804 - val_accuracy: 0.4600 - val_loss: 0.8761\nEpoch 38/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5727 - loss: 0.6748 - val_accuracy: 0.5600 - val_loss: 0.8413\nEpoch 39/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5711 - loss: 0.6728 - val_accuracy: 0.5200 - val_loss: 0.7661\nEpoch 40/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5806 - loss: 0.6693 - val_accuracy: 0.4400 - val_loss: 0.8138\nEpoch 41/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.5594 - loss: 0.6782 - val_accuracy: 0.4600 - val_loss: 0.7760\nEpoch 42/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.5944 - loss: 0.6679 - val_accuracy: 0.5200 - val_loss: 0.8312\nEpoch 43/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.5832 - loss: 0.6632 - val_accuracy: 0.6000 - val_loss: 0.8108\nEpoch 44/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5909 - loss: 0.6650 - val_accuracy: 0.5400 - val_loss: 0.8447\nEpoch 45/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5871 - loss: 0.6711 - val_accuracy: 0.5400 - val_loss: 0.8737\nEpoch 46/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.5722 - loss: 0.6767 - val_accuracy: 0.5800 - val_loss: 0.8697\nEpoch 47/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5730 - loss: 0.6663 - val_accuracy: 0.4400 - val_loss: 1.0085\nEpoch 48/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5761 - loss: 0.6676 - val_accuracy: 0.5200 - val_loss: 0.8946\nEpoch 49/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5916 - loss: 0.6645 - val_accuracy: 0.5000 - val_loss: 1.0739\nEpoch 50/50\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5756 - loss: 0.6770 - val_accuracy: 0.5000 - val_loss: 0.8348\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x400 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# ▶️ Kaggle‑runnable: MI CNN with CSP (time‑series) + classification report\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report, f1_score\nfrom mne.decoding import CSP\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# 1) Load preprocessed MI data\ntrain = np.load('/kaggle/working/preprocessed/train_MI.npz')\nX_train, y_train = train['X'], train['y']\nval   = np.load('/kaggle/working/preprocessed/validation_MI.npz')\nX_val,   y_val   = val['X'],   val['y']\n\n# X_train: (n_trials, n_channels, n_samples)\n# Convert labels to binary\ny_train_bin = (y_train == 'Right').astype(int)\ny_val_bin   = (y_val   == 'Right').astype(int)\n\n# 2) Fit CSP on raw epochs\nn_components = 4  # pick 4 spatial filters\ncsp = CSP(n_components=n_components, reg=None, log=False, norm_trace=False)\ncsp.fit(X_train, y_train_bin)\n\n# 3) Manually apply filters to get time-series components\n# csp.filters_ is shape (n_components, n_channels)\nfilters = csp.filters_[:n_components]\ndef apply_csp(X):\n    # X: epochs x channels x times\n    return np.stack([filters.dot(epoch) for epoch in X], axis=0)\n\nX_train_csp = apply_csp(X_train)  # (n_trials, n_components, n_samples)\nX_val_csp   = apply_csp(X_val)\n\n# 4) Prepare for Keras (timesteps, channels)\n# transpose to (n_trials, n_samples, n_components)\nX_train_csp = X_train_csp.astype('float32').transpose(0, 2, 1)\nX_val_csp   = X_val_csp.astype('float32').transpose(0, 2, 1)\n\n# One-hot encode labels\ny_train_oh = keras.utils.to_categorical(y_train_bin, 2)\ny_val_oh   = keras.utils.to_categorical(y_val_bin,   2)\n\n# 5) Augmentation generator (light noise)\ndef aug_generator(X, y, batch_size=32):\n    n = X.shape[0]\n    while True:\n        idx = np.random.randint(0, n, batch_size)\n        bx = X[idx].copy()\n        by = y[idx]\n        bx += np.random.normal(0, 0.005, bx.shape)\n        yield bx, by\n\ntrain_gen = aug_generator(X_train_csp, y_train_oh, batch_size=64)\n\n# 6) Build and compile CNN\ninput_shape = X_train_csp.shape[1:]\nmodel = keras.Sequential([\n    layers.Input(input_shape),\n    layers.Conv1D(32, 5, activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPool1D(2),\n    layers.Conv1D(64, 5, activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPool1D(2),\n    layers.Conv1D(128,5, activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPool1D(2),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4)),\n    layers.Dropout(0.6),\n    layers.Dense(2, activation='softmax'),\n])\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-4),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\nmodel.summary()\n\n# 7) Callbacks for F1, early stopping, and logging\nclass F1Callback(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        preds = np.argmax(self.model.predict(X_val_csp, verbose=0), axis=1)\n        f1 = f1_score(y_val_bin, preds)\n        logs['val_f1'] = f1\n        print(f\" — val_f1: {f1:.4f}\")\n\ncsv_logger = keras.callbacks.CSVLogger('training_log.csv')\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_f1', mode='max', patience=10, restore_best_weights=True)\ncheckpoint = keras.callbacks.ModelCheckpoint('best_mi_cnn_csp.h5', monitor='val_f1', mode='max', save_best_only=True)\n\n# 8) Train\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=len(X_train_csp)//64,\n    validation_data=(X_val_csp, y_val_oh),\n    epochs=100,\n    callbacks=[F1Callback(), csv_logger, early_stop, checkpoint]\n)\n\n# 9) Final classification report on validation\nbest = keras.models.load_model('best_mi_cnn_csp.h5')\nval_preds = np.argmax(best.predict(X_val_csp), axis=1)\n\nprint(\"\\nValidation Classification Report:\")\nprint(classification_report(y_val_bin, val_preds, target_names=['Left','Right']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:34:24.017589Z","iopub.execute_input":"2025-06-26T06:34:24.017844Z","iopub.status.idle":"2025-06-26T06:36:55.217233Z","shell.execute_reply.started":"2025-06-26T06:34:24.017823Z","shell.execute_reply":"2025-06-26T06:36:55.216337Z"}},"outputs":[{"name":"stdout","text":"Computing rank from data with rank=None\n    Using tolerance 7.4e+03 (2.2e-16 eps * 8 dim * 4.2e+18  max singular value)\n    Estimated rank (data): 8\n    data: rank 8 computed from 8 data channels with 0 projectors\nReducing data rank from 8 -> 8\nEstimating class=0 covariance using EMPIRICAL\nDone.\nEstimating class=1 covariance using EMPIRICAL\nDone.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2250\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │             \u001b[38;5;34m672\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2250\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1125\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1125\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m10,304\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1125\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m562\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m562\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │          \u001b[38;5;34m41,088\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m562\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_6 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m281\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m130\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">562</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">562</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">562</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">281</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,346\u001b[0m (239.63 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,346</span> (239.63 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m60,898\u001b[0m (237.88 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">60,898</span> (237.88 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step - accuracy: 0.5272 - loss: 0.8340 — val_f1: 0.5172\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 339ms/step - accuracy: 0.5269 - loss: 0.8335 - val_accuracy: 0.4400 - val_loss: 0.7051 - val_f1: 0.5172\nEpoch 2/100\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - accuracy: 0.5096 - loss: 0.7781 — val_f1: 0.5538\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 303ms/step - accuracy: 0.5098 - loss: 0.7782 - val_accuracy: 0.4200 - val_loss: 0.7104 - val_f1: 0.5538\nEpoch 3/100\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - accuracy: 0.5489 - loss: 0.7502 — val_f1: 0.5000\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 302ms/step - accuracy: 0.5483 - loss: 0.7503 - val_accuracy: 0.4400 - val_loss: 0.7131 - val_f1: 0.5000\nEpoch 4/100\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.4770 - loss: 0.7814 — val_f1: 0.4828\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 310ms/step - accuracy: 0.4775 - loss: 0.7812 - val_accuracy: 0.4000 - val_loss: 0.7146 - val_f1: 0.4828\nEpoch 5/100\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - accuracy: 0.5022 - loss: 0.7595 — val_f1: 0.5185\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 298ms/step - accuracy: 0.5025 - loss: 0.7594 - val_accuracy: 0.4800 - val_loss: 0.7020 - val_f1: 0.5185\nEpoch 6/100\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - accuracy: 0.5334 - loss: 0.7475 — val_f1: 0.3000\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 300ms/step - accuracy: 0.5332 - loss: 0.7474 - val_accuracy: 0.4400 - val_loss: 0.6988 - val_f1: 0.3000\nEpoch 7/100\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.5333 - loss: 0.7235 — val_f1: 0.3902\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 313ms/step - accuracy: 0.5332 - loss: 0.7236 - val_accuracy: 0.5000 - val_loss: 0.7090 - val_f1: 0.3902\nEpoch 8/100\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.5147 - loss: 0.7354 — val_f1: 0.3415\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 303ms/step - accuracy: 0.5149 - loss: 0.7353 - val_accuracy: 0.4600 - val_loss: 0.7162 - val_f1: 0.3415\nEpoch 9/100\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.5123 - loss: 0.7301 — val_f1: 0.2941\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 300ms/step - accuracy: 0.5125 - loss: 0.7301 - val_accuracy: 0.5200 - val_loss: 0.7064 - val_f1: 0.2941\nEpoch 10/100\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.4895 - loss: 0.7416 — val_f1: 0.2857\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 313ms/step - accuracy: 0.4900 - loss: 0.7413 - val_accuracy: 0.4000 - val_loss: 0.7339 - val_f1: 0.2857\nEpoch 11/100\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 0.5430 - loss: 0.7157 — val_f1: 0.3913\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 300ms/step - accuracy: 0.5428 - loss: 0.7159 - val_accuracy: 0.4400 - val_loss: 0.7027 - val_f1: 0.3913\nEpoch 12/100\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.5138 - loss: 0.7125 — val_f1: 0.4103\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 304ms/step - accuracy: 0.5144 - loss: 0.7124 - val_accuracy: 0.5400 - val_loss: 0.6784 - val_f1: 0.4103\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n\nValidation Classification Report:\n              precision    recall  f1-score   support\n\n        Left       0.43      0.11      0.17        28\n       Right       0.42      0.82      0.55        22\n\n    accuracy                           0.42        50\n   macro avg       0.42      0.46      0.36        50\nweighted avg       0.42      0.42      0.34        50\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ▶️ Kaggle‑runnable: MI CNN w/ CSP, extended early stopping, save both models, reports\nimport os\nimport numpy as np\nfrom sklearn.metrics import classification_report, f1_score\nfrom mne.decoding import CSP\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# 1) Load preprocessed MI data\ntrain = np.load('/kaggle/working/preprocessed/train_MI.npz')\nX_train, y_train = train['X'], train['y']\nval   = np.load('/kaggle/working/preprocessed/validation_MI.npz')\nX_val,   y_val   = val['X'],   val['y']\n\n# Binary labels\ny_train_bin = (y_train == 'Right').astype(int)\ny_val_bin   = (y_val   == 'Right').astype(int)\n\n# 2) CSP\nn_comp = 4\ncsp = CSP(n_components=n_comp, reg=None, log=False, norm_trace=False)\ncsp.fit(X_train, y_train_bin)\nfilters = csp.filters_[:n_comp]\ndef apply_csp(X):\n    return np.stack([filters.dot(ep) for ep in X], axis=0)\n\nX_train_csp = apply_csp(X_train)\nX_val_csp   = apply_csp(X_val)\n\n# 3) Prepare for Keras: (trials, timesteps, components)\nX_train_csp = X_train_csp.astype('float32').transpose(0,2,1)\nX_val_csp   = X_val_csp.astype('float32').transpose(0,2,1)\ny_train_oh = keras.utils.to_categorical(y_train_bin, 2)\ny_val_oh   = keras.utils.to_categorical(y_val_bin,   2)\n\n# 4) Augmentor\ndef aug_gen(X, y, bs=32):\n    n = X.shape[0]\n    while True:\n        idx = np.random.randint(n, size=bs)\n        bx, by = X[idx].copy(), y[idx]\n        bx += np.random.normal(0, 0.005, bx.shape)\n        yield bx, by\n\ntrain_gen = aug_gen(X_train_csp, y_train_oh, bs=64)\n\n# 5) Build CNN\ninp_shape = X_train_csp.shape[1:]\nmodel = keras.Sequential([\n    layers.Input(inp_shape),\n    layers.Conv1D(32,5,activation='relu',padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPool1D(2),\n    layers.Conv1D(64,5,activation='relu',padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPool1D(2),\n    layers.Conv1D(128,5,activation='relu',padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPool1D(2),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(64,activation='relu',kernel_regularizer=keras.regularizers.l2(1e-4)),\n    layers.Dropout(0.6),\n    layers.Dense(2,activation='softmax'),\n])\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-4),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\nmodel.summary()\n\n# 6) Callbacks\nclass F1Callback(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        preds = np.argmax(self.model.predict(X_val_csp, verbose=0), axis=1)\n        logs['val_f1'] = f1_score(y_val_bin, preds)\n        print(f\" — val_f1: {logs['val_f1']:.4f}\")\n\n# CSV log\ncsv_logger = keras.callbacks.CSVLogger('training_log.csv')\n# Early stopping with patience=20 on val_f1\nes = keras.callbacks.EarlyStopping(\n    monitor='val_f1', mode='max', patience=20, restore_best_weights=False\n)\n# Checkpoints\nckpt_best = keras.callbacks.ModelCheckpoint(\n    'best_model.h5', monitor='val_f1', save_best_only=True, mode='max'\n)\n\n# 7) Train\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=len(X_train_csp)//64,\n    validation_data=(X_val_csp, y_val_oh),\n    epochs=200,\n    callbacks=[F1Callback(), csv_logger, es, ckpt_best]\n)\n\n# 8) Save final model\nmodel.save('final_model.h5')\n\n# 9) Reports\n# Best model\nbest = keras.models.load_model('best_model.h5')\nbest_preds = np.argmax(best.predict(X_val_csp), axis=1)\nprint(\"=== Best Model Classification Report ===\")\nprint(classification_report(y_val_bin, best_preds, target_names=['Left','Right']))\n\n# Final model\nfinal_preds = np.argmax(model.predict(X_val_csp), axis=1)\nprint(\"=== Final Model Classification Report ===\")\nprint(classification_report(y_val_bin, final_preds, target_names=['Left','Right']))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:36:55.218132Z","iopub.execute_input":"2025-06-26T06:36:55.218393Z","execution_failed":"2025-06-26T06:40:13.010Z"}},"outputs":[{"name":"stdout","text":"Computing rank from data with rank=None\n    Using tolerance 7.4e+03 (2.2e-16 eps * 8 dim * 4.2e+18  max singular value)\n    Estimated rank (data): 8\n    data: rank 8 computed from 8 data channels with 0 projectors\nReducing data rank from 8 -> 8\nEstimating class=0 covariance using EMPIRICAL\nDone.\nEstimating class=1 covariance using EMPIRICAL\nDone.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2250\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │             \u001b[38;5;34m672\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2250\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_7 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1125\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1125\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m10,304\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_10               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1125\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_8 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m562\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m562\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │          \u001b[38;5;34m41,088\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_11               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m562\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_9 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m281\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m130\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_10               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">562</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">562</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_11               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">562</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">281</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,346\u001b[0m (239.63 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,346</span> (239.63 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m60,898\u001b[0m (237.88 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">60,898</span> (237.88 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step - accuracy: 0.5205 - loss: 0.8933 — val_f1: 0.4490\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 326ms/step - accuracy: 0.5203 - loss: 0.8928 - val_accuracy: 0.4600 - val_loss: 0.7035 - val_f1: 0.4490\nEpoch 2/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step - accuracy: 0.4941 - loss: 0.8508 — val_f1: 0.5763\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 320ms/step - accuracy: 0.4941 - loss: 0.8504 - val_accuracy: 0.5000 - val_loss: 0.7071 - val_f1: 0.5763\nEpoch 3/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 0.5208 - loss: 0.7842 — val_f1: 0.6129\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 300ms/step - accuracy: 0.5207 - loss: 0.7845 - val_accuracy: 0.5200 - val_loss: 0.7135 - val_f1: 0.6129\nEpoch 4/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step - accuracy: 0.5190 - loss: 0.7840 — val_f1: 0.5484\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 301ms/step - accuracy: 0.5191 - loss: 0.7838 - val_accuracy: 0.4400 - val_loss: 0.7217 - val_f1: 0.5484\nEpoch 5/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - accuracy: 0.5263 - loss: 0.7698 — val_f1: 0.5574\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 316ms/step - accuracy: 0.5263 - loss: 0.7698 - val_accuracy: 0.4600 - val_loss: 0.7292 - val_f1: 0.5574\nEpoch 6/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.5532 - loss: 0.7367 — val_f1: 0.5397\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 300ms/step - accuracy: 0.5529 - loss: 0.7371 - val_accuracy: 0.4200 - val_loss: 0.7406 - val_f1: 0.5397\nEpoch 7/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step - accuracy: 0.5149 - loss: 0.7554 — val_f1: 0.5246\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 302ms/step - accuracy: 0.5153 - loss: 0.7550 - val_accuracy: 0.4200 - val_loss: 0.7472 - val_f1: 0.5246\nEpoch 8/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.5332 - loss: 0.7388 — val_f1: 0.5085\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 315ms/step - accuracy: 0.5330 - loss: 0.7388 - val_accuracy: 0.4200 - val_loss: 0.7772 - val_f1: 0.5085\nEpoch 9/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step - accuracy: 0.5207 - loss: 0.7353 — val_f1: 0.5000\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 301ms/step - accuracy: 0.5205 - loss: 0.7355 - val_accuracy: 0.4800 - val_loss: 0.7988 - val_f1: 0.5000\nEpoch 10/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.5355 - loss: 0.7238 — val_f1: 0.3750\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 300ms/step - accuracy: 0.5353 - loss: 0.7239 - val_accuracy: 0.4000 - val_loss: 0.8382 - val_f1: 0.3750\nEpoch 11/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step - accuracy: 0.5152 - loss: 0.7408 — val_f1: 0.3750\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 317ms/step - accuracy: 0.5155 - loss: 0.7405 - val_accuracy: 0.4000 - val_loss: 0.8817 - val_f1: 0.3750\nEpoch 12/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - accuracy: 0.5160 - loss: 0.7313 — val_f1: 0.5000\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 302ms/step - accuracy: 0.5160 - loss: 0.7314 - val_accuracy: 0.4800 - val_loss: 0.8438 - val_f1: 0.5000\nEpoch 13/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.5443 - loss: 0.7074 — val_f1: 0.4167\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 303ms/step - accuracy: 0.5446 - loss: 0.7072 - val_accuracy: 0.4400 - val_loss: 0.8204 - val_f1: 0.4167\nEpoch 14/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.5462 - loss: 0.7133 — val_f1: 0.4151\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 311ms/step - accuracy: 0.5461 - loss: 0.7134 - val_accuracy: 0.3800 - val_loss: 0.8633 - val_f1: 0.4151\nEpoch 15/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.5556 - loss: 0.7092 — val_f1: 0.3478\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 294ms/step - accuracy: 0.5554 - loss: 0.7092 - val_accuracy: 0.4000 - val_loss: 0.8902 - val_f1: 0.3478\nEpoch 16/200\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.5570 - loss: 0.6983 — val_f1: 0.3158\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 294ms/step - accuracy: 0.5567 - loss: 0.6984 - val_accuracy: 0.4800 - val_loss: 0.8675 - val_f1: 0.3158\nEpoch 17/200\n\u001b[1m21/37\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 314ms/step - accuracy: 0.5482 - loss: 0.7054","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom sklearn.metrics import f1_score, classification_report\nfrom mne.decoding import CSP\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, LearningRateScheduler\n\n# Configuration\ndata_dir = './preprocessed'\noutput_dir = './models'\nos.makedirs(output_dir, exist_ok=True)\n\n# === 1) Load preprocessed MI data ===\n# Using pre-saved .npz files to avoid reprocessing raw CSVs\ntrain_npz = np.load(os.path.join(data_dir, 'train_MI.npz'))\nval_npz   = np.load(os.path.join(data_dir, 'validation_MI.npz'))\nX_train, y_train = train_npz['X'], train_npz['y']\nX_val,   y_val   = val_npz['X'],   val_npz['y']\n\n# Binary labels\ny_train_bin = (y_train == 'Right').astype(int)\ny_val_bin   = (y_val   == 'Right').astype(int)\n\n# === 2) CSP spatial filtering ===\ncsp = CSP(n_components=4, log=False, norm_trace=False)\ncsp.fit(X_train, y_train_bin)\nW = csp.filters_[:4]\n\ndef apply_csp(X):\n    # X: trials x channels x samples\n    return np.stack([W.dot(epoch) for epoch in X], axis=0)\n\nXtr = apply_csp(X_train).transpose(0,2,1).astype('float32')  # trials x timesteps x components\nXvl = apply_csp(X_val)  .transpose(0,2,1).astype('float32')\n\nytr_oh = keras.utils.to_categorical(y_train_bin, 2)\nyvl_oh = keras.utils.to_categorical(y_val_bin,   2)\n\n# === 3) Data augmentation generator ===\ndef aug_generator(X, y, batch_size=32):\n    n = X.shape[0]\n    while True:\n        idx = np.random.randint(0, n, batch_size)\n        batch_x = X[idx].copy()\n        batch_y = y[idx]\n        # small Gaussian noise\n        batch_x += np.random.normal(0, 0.005, batch_x.shape)\n        yield batch_x, batch_y\n\ntrain_gen = aug_generator(Xtr, ytr_oh, batch_size=64)\nsteps_per_epoch = len(Xtr) // 64\n\n# === 4) Learning rate schedule ===\ndef cosine_lr(epoch, lr_max=5e-5, epochs=200):\n    return lr_max * (1 + np.cos(np.pi * epoch / epochs)) / 2\n\n# === 5) Callbacks ===\ncallbacks_A = [\n    EarlyStopping(monitor='val_f1', mode='max', patience=20, restore_best_weights=True),\n    ModelCheckpoint(os.path.join(output_dir, 'bestA.h5'), monitor='val_f1', mode='max', save_best_only=True),\n    CSVLogger(os.path.join(output_dir, 'logA.csv')),\n    LearningRateScheduler(cosine_lr)\n]\n\ncallbacks_B = [\n    EarlyStopping(monitor='val_f1', mode='max', patience=20, restore_best_weights=True),\n    ModelCheckpoint(os.path.join(output_dir, 'bestB.h5'), monitor='val_f1', mode='max', save_best_only=True),\n    CSVLogger(os.path.join(output_dir, 'logB.csv')),\n    LearningRateScheduler(cosine_lr)\n]\n\n# Custom F1 callback to compute val_f1 at each epoch\nclass F1Callback(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        preds = np.argmax(self.model.predict(Xvl, verbose=0), axis=1)\n        f1 = f1_score(y_val_bin, preds)\n        logs['val_f1'] = f1\n        print(f\" — val_f1: {f1:.4f}\")\n\n# insert F1Callback at front of callback lists\ndef with_f1(cbs): return [F1Callback()] + cbs\ncallbacks_A = with_f1(callbacks_A)\ncallbacks_B = with_f1(callbacks_B)\n\n# === 6) Model definitions ===\n\ndef build_modelA(input_shape):\n    model = keras.Sequential([\n        layers.Input(input_shape),\n        layers.Conv1D(32, 5, activation='relu', padding='same'),\n        layers.BatchNormalization(), layers.MaxPool1D(2),\n        layers.Conv1D(64, 5, activation='relu', padding='same'),\n        layers.BatchNormalization(), layers.MaxPool1D(2),\n        layers.Conv1D(128,5, activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.GlobalAveragePooling1D(),\n        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n        layers.Dropout(0.7),\n        layers.Dense(2, activation='softmax'),\n    ])\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n\ndef build_modelB(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for f in [16,32,64,128,256]:\n        x = layers.Conv1D(f,3,activation='relu',padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.MaxPool1D(2)(x)\n    x = layers.Flatten()(x)\n    for u in [128,64,32]:\n        x = layers.Dense(u, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n        x = layers.Dropout(0.5)(x)\n    out = layers.Dense(2, activation='softmax')(x)\n    model = keras.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# input shape\ninput_shape = Xtr.shape[1:]\n\n# === 7) Train Model A ===\nmodelA = build_modelA(input_shape)\nhistoryA = modelA.fit(\n    train_gen,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=(Xvl, yvl_oh),\n    epochs=200,\n    callbacks=callbacks_A,\n    verbose=2\n)\n\n# === 8) Train Model B ===\nmodelB = build_modelB(input_shape)\nhistoryB = modelB.fit(\n    train_gen,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=(Xvl, yvl_oh),\n    epochs=200,\n    callbacks=callbacks_B,\n    verbose=2\n)\n\n# === 9) Evaluate & select best ===\nbestA = keras.models.load_model(os.path.join(output_dir, 'bestA.h5'))\nbestB = keras.models.load_model(os.path.join(output_dir, 'bestB.h5'))\npredA = np.argmax(bestA.predict(Xvl), axis=1)\npredB = np.argmax(bestB.predict(Xvl), axis=1)\nf1A = f1_score(y_val_bin, predA)\nf1B = f1_score(y_val_bin, predB)\nprint(f\"Model A val_F1 = {f1A:.4f}\")\nprint(f\"Model B val_F1 = {f1B:.4f}\\n\")\n\n# choose and save final\nif f1B > f1A:\n    chosen, name, preds = bestB, 'Model B', predB\nelse:\n    chosen, name, preds = bestA, 'Model A', predA\nchosen.save(os.path.join(output_dir, 'best_chosen_model.h5'))\nprint(f\"=== Selected {name} ===\")\nprint(classification_report(y_val_bin, preds, target_names=['Left','Right']))\n\n# Also save final model state\nmodelA.save(os.path.join(output_dir, 'final_modelA.h5'))\nmodelB.save(os.path.join(output_dir, 'final_modelB.h5'))\nprint('All done.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:59:11.522211Z","iopub.execute_input":"2025-06-29T10:59:11.523103Z","iopub.status.idle":"2025-06-29T11:00:41.148552Z","shell.execute_reply.started":"2025-06-29T10:59:11.523076Z","shell.execute_reply":"2025-06-29T11:00:41.147760Z"}},"outputs":[{"name":"stdout","text":"Computing rank from data with rank=None\n    Using tolerance 7.4e+03 (2.2e-16 eps * 8 dim * 4.2e+18  max singular value)\n    Estimated rank (data): 8\n    data: rank 8 computed from 8 data channels with 0 projectors\nReducing data rank from 8 -> 8\nEstimating class=0 covariance using EMPIRICAL\nDone.\nEstimating class=1 covariance using EMPIRICAL\nDone.\nEpoch 1/200\n — val_f1: 0.3500\n37/37 - 10s - 278ms/step - accuracy: 0.5101 - loss: 0.8769 - val_accuracy: 0.4800 - val_loss: 0.7244 - val_f1: 0.3500 - learning_rate: 0.0010\nEpoch 2/200\n — val_f1: 0.3030\n37/37 - 1s - 29ms/step - accuracy: 0.5051 - loss: 0.7874 - val_accuracy: 0.5400 - val_loss: 0.7109 - val_f1: 0.3030 - learning_rate: 9.9994e-04\nEpoch 3/200\n — val_f1: 0.0000\n37/37 - 1s - 29ms/step - accuracy: 0.5080 - loss: 0.7303 - val_accuracy: 0.5200 - val_loss: 0.6927 - val_f1: 0.0000e+00 - learning_rate: 9.9969e-04\nEpoch 4/200\n — val_f1: 0.4400\n37/37 - 1s - 30ms/step - accuracy: 0.5106 - loss: 0.7324 - val_accuracy: 0.4400 - val_loss: 0.7008 - val_f1: 0.4400 - learning_rate: 9.9914e-04\nEpoch 5/200\n — val_f1: 0.5098\n37/37 - 1s - 30ms/step - accuracy: 0.5270 - loss: 0.7249 - val_accuracy: 0.5000 - val_loss: 0.7377 - val_f1: 0.5098 - learning_rate: 9.9815e-04\nEpoch 6/200\n — val_f1: 0.5806\n37/37 - 1s - 31ms/step - accuracy: 0.5013 - loss: 0.7233 - val_accuracy: 0.4800 - val_loss: 0.7172 - val_f1: 0.5806 - learning_rate: 9.9661e-04\nEpoch 7/200\n — val_f1: 0.3810\n37/37 - 1s - 29ms/step - accuracy: 0.5025 - loss: 0.7092 - val_accuracy: 0.4800 - val_loss: 0.7296 - val_f1: 0.3810 - learning_rate: 9.9440e-04\nEpoch 8/200\n — val_f1: 0.3404\n37/37 - 1s - 29ms/step - accuracy: 0.5093 - loss: 0.7062 - val_accuracy: 0.3800 - val_loss: 0.7215 - val_f1: 0.3404 - learning_rate: 9.9140e-04\nEpoch 9/200\n — val_f1: 0.2162\n37/37 - 1s - 29ms/step - accuracy: 0.5296 - loss: 0.7030 - val_accuracy: 0.4200 - val_loss: 0.7229 - val_f1: 0.2162 - learning_rate: 9.8749e-04\nEpoch 10/200\n — val_f1: 0.6207\n37/37 - 1s - 31ms/step - accuracy: 0.5106 - loss: 0.7041 - val_accuracy: 0.5600 - val_loss: 0.7154 - val_f1: 0.6207 - learning_rate: 9.8256e-04\nEpoch 11/200\n — val_f1: 0.6154\n37/37 - 1s - 30ms/step - accuracy: 0.5169 - loss: 0.7046 - val_accuracy: 0.5000 - val_loss: 0.7551 - val_f1: 0.6154 - learning_rate: 9.7652e-04\nEpoch 12/200\n — val_f1: 0.4889\n37/37 - 1s - 29ms/step - accuracy: 0.5262 - loss: 0.7016 - val_accuracy: 0.5400 - val_loss: 0.7216 - val_f1: 0.4889 - learning_rate: 9.6924e-04\nEpoch 13/200\n — val_f1: 0.3590\n37/37 - 1s - 29ms/step - accuracy: 0.5152 - loss: 0.7045 - val_accuracy: 0.5000 - val_loss: 0.7496 - val_f1: 0.3590 - learning_rate: 9.6066e-04\nEpoch 14/200\n — val_f1: 0.4348\n37/37 - 1s - 29ms/step - accuracy: 0.5249 - loss: 0.7032 - val_accuracy: 0.4800 - val_loss: 0.7413 - val_f1: 0.4348 - learning_rate: 9.5068e-04\nEpoch 15/200\n — val_f1: 0.3500\n37/37 - 1s - 30ms/step - accuracy: 0.5283 - loss: 0.7021 - val_accuracy: 0.4800 - val_loss: 0.7195 - val_f1: 0.3500 - learning_rate: 9.3923e-04\nEpoch 16/200\n — val_f1: 0.2500\n37/37 - 1s - 29ms/step - accuracy: 0.5359 - loss: 0.6984 - val_accuracy: 0.5200 - val_loss: 0.7575 - val_f1: 0.2500 - learning_rate: 9.2626e-04\nEpoch 17/200\n — val_f1: 0.5000\n37/37 - 1s - 29ms/step - accuracy: 0.5308 - loss: 0.6952 - val_accuracy: 0.4400 - val_loss: 0.7398 - val_f1: 0.5000 - learning_rate: 9.1171e-04\nEpoch 18/200\n — val_f1: 0.4167\n37/37 - 1s - 29ms/step - accuracy: 0.5287 - loss: 0.6993 - val_accuracy: 0.4400 - val_loss: 0.7066 - val_f1: 0.4167 - learning_rate: 8.9555e-04\nEpoch 19/200\n — val_f1: 0.3556\n37/37 - 1s - 29ms/step - accuracy: 0.5469 - loss: 0.6944 - val_accuracy: 0.4200 - val_loss: 0.7365 - val_f1: 0.3556 - learning_rate: 8.7777e-04\nEpoch 20/200\n — val_f1: 0.2439\n37/37 - 1s - 29ms/step - accuracy: 0.5528 - loss: 0.6919 - val_accuracy: 0.3800 - val_loss: 0.7281 - val_f1: 0.2439 - learning_rate: 8.5837e-04\nEpoch 21/200\n — val_f1: 0.2927\n37/37 - 1s - 29ms/step - accuracy: 0.5519 - loss: 0.6959 - val_accuracy: 0.4200 - val_loss: 0.7201 - val_f1: 0.2927 - learning_rate: 8.3736e-04\nEpoch 22/200\n — val_f1: 0.3415\n37/37 - 1s - 29ms/step - accuracy: 0.5279 - loss: 0.6985 - val_accuracy: 0.4600 - val_loss: 0.6994 - val_f1: 0.3415 - learning_rate: 8.1479e-04\nEpoch 23/200\n — val_f1: 0.5098\n37/37 - 1s - 29ms/step - accuracy: 0.5443 - loss: 0.6936 - val_accuracy: 0.5000 - val_loss: 0.7479 - val_f1: 0.5098 - learning_rate: 7.9071e-04\nEpoch 24/200\n — val_f1: 0.4783\n37/37 - 1s - 29ms/step - accuracy: 0.5232 - loss: 0.6953 - val_accuracy: 0.5200 - val_loss: 0.7321 - val_f1: 0.4783 - learning_rate: 7.6518e-04\nEpoch 25/200\n — val_f1: 0.3902\n37/37 - 1s - 29ms/step - accuracy: 0.5367 - loss: 0.6945 - val_accuracy: 0.5000 - val_loss: 0.7189 - val_f1: 0.3902 - learning_rate: 7.3832e-04\nEpoch 26/200\n — val_f1: 0.2286\n37/37 - 1s - 29ms/step - accuracy: 0.5380 - loss: 0.6923 - val_accuracy: 0.4600 - val_loss: 0.7102 - val_f1: 0.2286 - learning_rate: 7.1022e-04\nEpoch 27/200\n — val_f1: 0.3158\n37/37 - 1s - 29ms/step - accuracy: 0.5541 - loss: 0.6916 - val_accuracy: 0.4800 - val_loss: 0.7197 - val_f1: 0.3158 - learning_rate: 6.8101e-04\nEpoch 28/200\n — val_f1: 0.3913\n37/37 - 1s - 29ms/step - accuracy: 0.5194 - loss: 0.6982 - val_accuracy: 0.4400 - val_loss: 0.7151 - val_f1: 0.3913 - learning_rate: 6.5084e-04\nEpoch 29/200\n — val_f1: 0.5091\n37/37 - 1s - 29ms/step - accuracy: 0.5469 - loss: 0.6955 - val_accuracy: 0.4600 - val_loss: 0.7033 - val_f1: 0.5091 - learning_rate: 6.1987e-04\nEpoch 30/200\n — val_f1: 0.4390\n37/37 - 1s - 29ms/step - accuracy: 0.5515 - loss: 0.6947 - val_accuracy: 0.5400 - val_loss: 0.7167 - val_f1: 0.4390 - learning_rate: 5.8827e-04\nEpoch 1/200\n — val_f1: 0.4500\n37/37 - 15s - 404ms/step - accuracy: 0.5004 - loss: 2.4781 - val_accuracy: 0.5600 - val_loss: 0.7283 - val_f1: 0.4500 - learning_rate: 0.0010\nEpoch 2/200\n — val_f1: 0.2857\n37/37 - 1s - 33ms/step - accuracy: 0.5131 - loss: 1.5226 - val_accuracy: 0.5000 - val_loss: 0.7491 - val_f1: 0.2857 - learning_rate: 9.9994e-04\nEpoch 3/200\n — val_f1: 0.3000\n37/37 - 1s - 33ms/step - accuracy: 0.5308 - loss: 1.1239 - val_accuracy: 0.4400 - val_loss: 0.7601 - val_f1: 0.3000 - learning_rate: 9.9969e-04\nEpoch 4/200\n — val_f1: 0.0000\n37/37 - 1s - 33ms/step - accuracy: 0.5000 - loss: 1.0303 - val_accuracy: 0.5000 - val_loss: 0.7458 - val_f1: 0.0000e+00 - learning_rate: 9.9914e-04\nEpoch 5/200\n — val_f1: 0.0000\n37/37 - 1s - 33ms/step - accuracy: 0.4987 - loss: 0.9212 - val_accuracy: 0.5600 - val_loss: 0.7430 - val_f1: 0.0000e+00 - learning_rate: 9.9815e-04\nEpoch 6/200\n — val_f1: 0.0000\n37/37 - 1s - 34ms/step - accuracy: 0.5131 - loss: 0.8427 - val_accuracy: 0.5600 - val_loss: 0.7436 - val_f1: 0.0000e+00 - learning_rate: 9.9661e-04\nEpoch 7/200\n — val_f1: 0.0833\n37/37 - 1s - 32ms/step - accuracy: 0.5072 - loss: 0.8263 - val_accuracy: 0.5600 - val_loss: 0.7422 - val_f1: 0.0833 - learning_rate: 9.9440e-04\nEpoch 8/200\n — val_f1: 0.0000\n37/37 - 1s - 33ms/step - accuracy: 0.5017 - loss: 0.8087 - val_accuracy: 0.5600 - val_loss: 0.7424 - val_f1: 0.0000e+00 - learning_rate: 9.9140e-04\nEpoch 9/200\n — val_f1: 0.0000\n37/37 - 1s - 33ms/step - accuracy: 0.5030 - loss: 0.7945 - val_accuracy: 0.5600 - val_loss: 0.7422 - val_f1: 0.0000e+00 - learning_rate: 9.8749e-04\nEpoch 10/200\n — val_f1: 0.0000\n37/37 - 1s - 33ms/step - accuracy: 0.4992 - loss: 0.7850 - val_accuracy: 0.5400 - val_loss: 0.7445 - val_f1: 0.0000e+00 - learning_rate: 9.8256e-04\nEpoch 11/200\n — val_f1: 0.3590\n37/37 - 1s - 33ms/step - accuracy: 0.4911 - loss: 0.7693 - val_accuracy: 0.5000 - val_loss: 0.7489 - val_f1: 0.3590 - learning_rate: 9.7652e-04\nEpoch 12/200\n — val_f1: 0.0000\n37/37 - 1s - 32ms/step - accuracy: 0.4954 - loss: 0.7772 - val_accuracy: 0.5600 - val_loss: 0.7436 - val_f1: 0.0000e+00 - learning_rate: 9.6924e-04\nEpoch 13/200\n — val_f1: 0.0000\n37/37 - 1s - 32ms/step - accuracy: 0.4975 - loss: 0.7659 - val_accuracy: 0.5600 - val_loss: 0.7431 - val_f1: 0.0000e+00 - learning_rate: 9.6066e-04\nEpoch 14/200\n — val_f1: 0.3404\n37/37 - 1s - 33ms/step - accuracy: 0.5021 - loss: 0.7580 - val_accuracy: 0.3800 - val_loss: 0.7501 - val_f1: 0.3404 - learning_rate: 9.5068e-04\nEpoch 15/200\n — val_f1: 0.1250\n37/37 - 1s - 32ms/step - accuracy: 0.5076 - loss: 0.7598 - val_accuracy: 0.4400 - val_loss: 0.7465 - val_f1: 0.1250 - learning_rate: 9.3923e-04\nEpoch 16/200\n — val_f1: 0.2857\n37/37 - 1s - 32ms/step - accuracy: 0.4932 - loss: 0.7554 - val_accuracy: 0.6000 - val_loss: 0.7419 - val_f1: 0.2857 - learning_rate: 9.2626e-04\nEpoch 17/200\n — val_f1: 0.0000\n37/37 - 1s - 33ms/step - accuracy: 0.5055 - loss: 0.7589 - val_accuracy: 0.5600 - val_loss: 0.7447 - val_f1: 0.0000e+00 - learning_rate: 9.1171e-04\nEpoch 18/200\n — val_f1: 0.1935\n37/37 - 1s - 32ms/step - accuracy: 0.5080 - loss: 0.7591 - val_accuracy: 0.5000 - val_loss: 0.7436 - val_f1: 0.1935 - learning_rate: 8.9555e-04\nEpoch 19/200\n — val_f1: 0.0833\n37/37 - 1s - 32ms/step - accuracy: 0.5046 - loss: 0.7548 - val_accuracy: 0.5600 - val_loss: 0.7433 - val_f1: 0.0833 - learning_rate: 8.7777e-04\nEpoch 20/200\n — val_f1: 0.0000\n37/37 - 1s - 33ms/step - accuracy: 0.4983 - loss: 0.7545 - val_accuracy: 0.5600 - val_loss: 0.7428 - val_f1: 0.0000e+00 - learning_rate: 8.5837e-04\nEpoch 21/200\n — val_f1: 0.0000\n37/37 - 1s - 33ms/step - accuracy: 0.4920 - loss: 0.7556 - val_accuracy: 0.5600 - val_loss: 0.7427 - val_f1: 0.0000e+00 - learning_rate: 8.3736e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 399ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 638ms/step\nModel A val_F1 = 0.6207\nModel B val_F1 = 0.4500\n\n=== Selected Model A ===\n              precision    recall  f1-score   support\n\n        Left       0.71      0.36      0.48        28\n       Right       0.50      0.82      0.62        22\n\n    accuracy                           0.56        50\n   macro avg       0.61      0.59      0.55        50\nweighted avg       0.62      0.56      0.54        50\n\nAll done.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\n\nfrom sklearn.metrics import f1_score, classification_report\nfrom mne.decoding import CSP\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, regularizers\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping,\n    ModelCheckpoint,\n    CSVLogger,\n    LearningRateScheduler,\n)\n\n# --- 0) (Optional) Reproducibility ---\n# os.environ['PYTHONHASHSEED'] = '0'\n# np.random.seed(0)\n# tf.random.set_seed(0)\n# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n# os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n\n# --- 1) Config ---\ndata_dir   = './preprocessed'\noutput_dir = './models'\nos.makedirs(output_dir, exist_ok=True)\n\n# --- 2) Load & binarize labels ---\ntrain_npz = np.load(os.path.join(data_dir, 'train_MI.npz'))\nval_npz   = np.load(os.path.join(data_dir, 'validation_MI.npz'))\nX_train, y_train = train_npz['X'], train_npz['y']\nX_val,   y_val   = val_npz['X'],   val_npz['y']\n\ny_train_bin = (y_train == 'Right').astype(int)\ny_val_bin   = (y_val   == 'Right').astype(int)\n\n# --- 3) CSP (4 components) ---\ncsp = CSP(n_components=4, log=False, norm_trace=False)\ncsp.fit(X_train, y_train_bin)\nW = csp.filters_[:4]\ndef apply_csp(X): return np.stack([W.dot(ep) for ep in X], axis=0)\n\nXtr = apply_csp(X_train).transpose(0,2,1).astype('float32')  # (n, T, 4)\nXvl = apply_csp(X_val)  .transpose(0,2,1).astype('float32')\n\n# --- 4) For Model3 only: add channel axis → (n, T, F=4, 1) ---\nXtr_spec = Xtr[..., np.newaxis]\nXvl_spec = Xvl[..., np.newaxis]\n\n# --- 5) One‑hot labels ---\nytr_oh = keras.utils.to_categorical(y_train_bin, 2)\nyvl_oh = keras.utils.to_categorical(y_val_bin,   2)\n\n# --- 6) Data‑augmentation gens ---\ndef aug_gen(X, y, seed=0, batch_size=32):\n    n   = X.shape[0]\n    rng = np.random.RandomState(seed)\n    while True:\n        idx = rng.randint(0, n, batch_size)\n        bx, by = X[idx].copy(), y[idx]\n        bx += rng.normal(0, 0.005, bx.shape)\n        yield bx, by\n\ntrain_gen_1d = aug_gen(Xtr,   ytr_oh, seed=0, batch_size=64)\ntrain_gen_2d = aug_gen(Xtr_spec, ytr_oh, seed=1, batch_size=64)\n\nsteps_1d = len(Xtr)      // 64\nsteps_2d = len(Xtr_spec) // 64\n\n# --- 7) LR schedule ---\ndef cosine_lr(epoch, lr_max=5e-5, epochs=200):\n    return lr_max * (1 + np.cos(np.pi * epoch / epochs)) / 2\n\n# --- 8) F1Score metric ---\nclass F1Score(tf.keras.metrics.Metric):\n    def __init__(self, name=\"f1_score\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        # create the state variables up front\n        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        preds  = tf.argmax(y_pred, axis=1)\n        labels = tf.argmax(y_true, axis=1)\n        preds  = tf.cast(preds, tf.int32)\n        labels = tf.cast(labels, tf.int32)\n\n        tp = tf.reduce_sum(\n            tf.cast(tf.logical_and(preds == 1, labels == 1), tf.float32)\n        )\n        fp = tf.reduce_sum(\n            tf.cast(tf.logical_and(preds == 1, labels == 0), tf.float32)\n        )\n        fn = tf.reduce_sum(\n            tf.cast(tf.logical_and(preds == 0, labels == 1), tf.float32)\n        )\n\n        self.tp.assign_add(tp)\n        self.fp.assign_add(fp)\n        self.fn.assign_add(fn)\n\n    def result(self):\n        precision = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n        recall    = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n        return 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n\n    def reset_states(self):\n        self.tp.assign(0.0)\n        self.fp.assign(0.0)\n        self.fn.assign(0.0)\n\n# --- 9) Callbacks factory ---\ndef get_callbacks(name):\n    return [\n        EarlyStopping(\"val_f1_score\", mode=\"max\", patience=20, restore_best_weights=True),\n        ModelCheckpoint(\n            os.path.join(output_dir, f\"best_{name}.h5\"),\n            \"val_f1_score\", mode=\"max\", save_best_only=True\n        ),\n        CSVLogger(os.path.join(output_dir, f\"log_{name}.csv\")),\n        LearningRateScheduler(cosine_lr)\n    ]\n\n# --- 10) Model builders (all output 2 classes) ---\n\ndef build_modelA(input_shape):\n    m = keras.Sequential([\n        layers.Input(input_shape),\n        layers.Conv1D(32, 5, activation=\"relu\", padding=\"same\"),\n        layers.BatchNormalization(), layers.MaxPool1D(2),\n        layers.Conv1D(64, 5, activation=\"relu\", padding=\"same\"),\n        layers.BatchNormalization(), layers.MaxPool1D(2),\n        layers.Conv1D(128,5,activation=\"relu\",padding=\"same\"),\n        layers.BatchNormalization(),\n        layers.GlobalAveragePooling1D(),\n        layers.Dense(64, activation=\"relu\", \n                     kernel_regularizer=regularizers.l2(1e-4)),\n        layers.Dropout(0.7),\n        layers.Dense(2, activation=\"softmax\"),\n    ])\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_modelB(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for f in [16,32,64,128,256]:\n        x = layers.Conv1D(f,3,activation=\"relu\",padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.MaxPool1D(2)(x)\n    x = layers.Flatten()(x)\n    for u in [128,64,32]:\n        x = layers.Dense(u, activation=\"relu\",\n                         kernel_regularizer=regularizers.l2(1e-4))(x)\n        x = layers.Dropout(0.5)(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp, out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model1(input_shape):\n    inp = layers.Input(input_shape+(1,))\n    x = layers.Concatenate()([inp, inp, inp])\n    x = layers.Resizing(32,32)(x)\n    base = keras.applications.ResNet50(\n        include_top=False, weights=\"imagenet\",\n        input_shape=(32,32,3), pooling=\"avg\"\n    )\n    base.trainable = False\n    x = base(x)\n    x = layers.Reshape((1, x.shape[-1]))(x)\n    for _ in range(7):\n        x = layers.Conv1D(64,3,activation=\"relu\",padding=\"same\")(x)\n    x = layers.MultiHeadAttention(num_heads=4,key_dim=32)(x,x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(64,activation=\"relu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model2(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for _ in range(3):\n        x = layers.Conv1D(32,3,activation=\"elu\",padding=\"same\")(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(64,activation=\"elu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model3(input_shape):\n    inp = layers.Input(input_shape)  # (T, F, 1)\n    x = inp\n    for _ in range(5):\n        x = layers.Conv2D(32,(3,3),activation=\"relu\",padding=\"same\")(x)\n    x = layers.Flatten()(x)\n    for u in [128,64,32]:\n        x = layers.Dense(u, activation=\"relu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model4(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for _ in range(3):\n        x = layers.Conv1D(64,3,activation=\"relu\",padding=\"same\")(x)\n    x = layers.LSTM(128)(x)\n    for _ in range(4):\n        x = layers.Dense(64, activation=\"relu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model5(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for _ in range(7):\n        x = layers.Conv1D(64,3,activation=\"elu\",padding=\"same\")(x)\n    x = layers.Flatten()(x)\n    for _ in range(3):\n        x = layers.Dense(64, activation=\"elu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model6(input_shape):\n    C3, C4 = 0,2\n    eeg_in = layers.Input(input_shape)\n    c3 = layers.Lambda(lambda x: x[:,:,C3:C3+1])(eeg_in)\n    c4 = layers.Lambda(lambda x: x[:,:,C4:C4+1])(eeg_in)\n    def branch():\n        return models.Sequential([\n            layers.Conv1D(16,250,activation=\"relu\",padding=\"same\"),\n            layers.MaxPool1D(3),\n            layers.Conv1D(32,50,activation=\"relu\",padding=\"same\"),\n            layers.GlobalAveragePooling1D()\n        ])\n    b3, b4 = branch()(c3), branch()(c4)\n    x = layers.Concatenate()([b3,b4])\n    for _ in range(4):\n        x = layers.Dense(64,activation=\"relu\")(x)\n    out = layers.Dense(2,activation=\"softmax\")(x)\n    m = models.Model(eeg_in,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\n# --- 11) Train & eval loop ---\nbuilders = {\n    'oldA': build_modelA,\n    'oldB': build_modelB,\n    'model1': build_model1,\n    'model2': build_model2,\n    'model3': build_model3,\n    'model4': build_model4,\n    'model5': build_model5,\n    'model6': build_model6,\n}\n\nresults = {}\nshape_1d = Xtr.shape[1:]      # (T, 4)\nshape_2d = Xtr_spec.shape[1:] # (T, 4, 1)\n\nfor name, build_fn in builders.items():\n    print(f\"\\n>>> Training {name}\")\n    if name == 'model3':\n        model = build_fn(shape_2d)\n        gen   = train_gen_2d\n        steps = steps_2d\n        val_x = Xvl_spec\n    else:\n        model = build_fn(shape_1d)\n        gen   = train_gen_1d\n        steps = steps_1d\n        val_x = Xvl\n\n    history = model.fit(\n        gen,\n        steps_per_epoch=steps,\n        validation_data=(val_x, yvl_oh),\n        epochs=200,\n        callbacks=get_callbacks(name),\n        verbose=2\n    )\n\n    # model now has best weights\n    preds = np.argmax(model.predict(val_x), axis=1)\n    f1    = f1_score(y_val_bin, preds)\n    print(f\"{name} →  val F1 = {f1:.4f}\")\n    print(classification_report(y_val_bin, preds, target_names=['Left','Right']))\n    results[name] = (f1, model)\n\n# --- 12) Pick & save final ---\nbest_name, (best_f1, best_model) = max(results.items(), key=lambda kv: kv[1][0])\nprint(f\"\\n=== Final best: {best_name} (F1={best_f1:.4f}) ===\")\nbest_model.save(os.path.join(output_dir, 'best_final.h5'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T11:12:55.561118Z","iopub.execute_input":"2025-06-29T11:12:55.561386Z","iopub.status.idle":"2025-06-29T11:21:43.095468Z","shell.execute_reply.started":"2025-06-29T11:12:55.561366Z","shell.execute_reply":"2025-06-29T11:21:43.094802Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Computing rank from data with rank=None\n    Using tolerance 7.4e+03 (2.2e-16 eps * 8 dim * 4.2e+18  max singular value)\n    Estimated rank (data): 8\n    data: rank 8 computed from 8 data channels with 0 projectors\nReducing data rank from 8 -> 8\nEstimating class=0 covariance using EMPIRICAL\nDone.\nEstimating class=1 covariance using EMPIRICAL\nDone.\n\n>>> Training oldA\nEpoch 1/200\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1751195588.614101      71 service.cc:148] XLA service 0x7dbafc024d00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1751195588.615269      71 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1751195589.063482      71 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1751195592.703276      71 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"37/37 - 11s - 290ms/step - accuracy: 0.5059 - f1_score: 0.4913 - loss: 0.9140 - val_accuracy: 0.5200 - val_f1_score: 0.0769 - val_loss: 0.6908 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 29ms/step - accuracy: 0.5101 - f1_score: 0.5222 - loss: 0.7715 - val_accuracy: 0.6400 - val_f1_score: 0.5263 - val_loss: 0.6854 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 27ms/step - accuracy: 0.5055 - f1_score: 0.4989 - loss: 0.7593 - val_accuracy: 0.4800 - val_f1_score: 0.2353 - val_loss: 0.7078 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 27ms/step - accuracy: 0.5068 - f1_score: 0.5309 - loss: 0.7301 - val_accuracy: 0.5600 - val_f1_score: 0.3889 - val_loss: 0.6908 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 27ms/step - accuracy: 0.5114 - f1_score: 0.5287 - loss: 0.7093 - val_accuracy: 0.5800 - val_f1_score: 0.4878 - val_loss: 0.7236 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 27ms/step - accuracy: 0.5232 - f1_score: 0.5007 - loss: 0.7109 - val_accuracy: 0.5600 - val_f1_score: 0.5000 - val_loss: 0.7163 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 27ms/step - accuracy: 0.5296 - f1_score: 0.4857 - loss: 0.7018 - val_accuracy: 0.6000 - val_f1_score: 0.5000 - val_loss: 0.7127 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 28ms/step - accuracy: 0.5393 - f1_score: 0.5523 - loss: 0.7002 - val_accuracy: 0.5400 - val_f1_score: 0.5818 - val_loss: 0.6873 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 27ms/step - accuracy: 0.5270 - f1_score: 0.5201 - loss: 0.7037 - val_accuracy: 0.6400 - val_f1_score: 0.5000 - val_loss: 0.7218 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 27ms/step - accuracy: 0.5232 - f1_score: 0.5738 - loss: 0.7059 - val_accuracy: 0.5400 - val_f1_score: 0.5306 - val_loss: 0.7092 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 27ms/step - accuracy: 0.5101 - f1_score: 0.5254 - loss: 0.7025 - val_accuracy: 0.6000 - val_f1_score: 0.4737 - val_loss: 0.7000 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 27ms/step - accuracy: 0.5325 - f1_score: 0.5243 - loss: 0.6976 - val_accuracy: 0.4600 - val_f1_score: 0.4000 - val_loss: 0.7307 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 27ms/step - accuracy: 0.5351 - f1_score: 0.5862 - loss: 0.6981 - val_accuracy: 0.6200 - val_f1_score: 0.3871 - val_loss: 0.6906 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 27ms/step - accuracy: 0.5367 - f1_score: 0.6078 - loss: 0.6976 - val_accuracy: 0.4600 - val_f1_score: 0.4255 - val_loss: 0.6929 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 27ms/step - accuracy: 0.5405 - f1_score: 0.5988 - loss: 0.6968 - val_accuracy: 0.4600 - val_f1_score: 0.4706 - val_loss: 0.6974 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 27ms/step - accuracy: 0.5270 - f1_score: 0.5587 - loss: 0.7018 - val_accuracy: 0.6000 - val_f1_score: 0.4444 - val_loss: 0.7101 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 28ms/step - accuracy: 0.5384 - f1_score: 0.5283 - loss: 0.6978 - val_accuracy: 0.5600 - val_f1_score: 0.4762 - val_loss: 0.7569 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 27ms/step - accuracy: 0.5304 - f1_score: 0.5045 - loss: 0.6950 - val_accuracy: 0.5400 - val_f1_score: 0.2581 - val_loss: 0.7378 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 27ms/step - accuracy: 0.5524 - f1_score: 0.5767 - loss: 0.6909 - val_accuracy: 0.5400 - val_f1_score: 0.5106 - val_loss: 0.7172 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 27ms/step - accuracy: 0.5418 - f1_score: 0.5786 - loss: 0.6963 - val_accuracy: 0.4600 - val_f1_score: 0.4490 - val_loss: 0.7632 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 27ms/step - accuracy: 0.5211 - f1_score: 0.5711 - loss: 0.7001 - val_accuracy: 0.5400 - val_f1_score: 0.2069 - val_loss: 0.7351 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 27ms/step - accuracy: 0.5595 - f1_score: 0.5439 - loss: 0.6927 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.7307 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 27ms/step - accuracy: 0.5511 - f1_score: 0.5787 - loss: 0.6927 - val_accuracy: 0.4200 - val_f1_score: 0.3830 - val_loss: 0.7541 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 27ms/step - accuracy: 0.5215 - f1_score: 0.5777 - loss: 0.6969 - val_accuracy: 0.5000 - val_f1_score: 0.1935 - val_loss: 0.7642 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 27ms/step - accuracy: 0.5579 - f1_score: 0.6191 - loss: 0.6919 - val_accuracy: 0.5000 - val_f1_score: 0.3243 - val_loss: 0.7719 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 27ms/step - accuracy: 0.5401 - f1_score: 0.5836 - loss: 0.6933 - val_accuracy: 0.5000 - val_f1_score: 0.2424 - val_loss: 0.7465 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 27ms/step - accuracy: 0.5519 - f1_score: 0.5771 - loss: 0.6930 - val_accuracy: 0.4200 - val_f1_score: 0.3556 - val_loss: 0.7869 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 27ms/step - accuracy: 0.5469 - f1_score: 0.5839 - loss: 0.6933 - val_accuracy: 0.5200 - val_f1_score: 0.2000 - val_loss: 0.7753 - learning_rate: 6.5084e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 439ms/step\noldA →  val F1 = 0.5818\n              precision    recall  f1-score   support\n\n        Left       0.65      0.39      0.49        28\n       Right       0.48      0.73      0.58        22\n\n    accuracy                           0.54        50\n   macro avg       0.57      0.56      0.54        50\nweighted avg       0.58      0.54      0.53        50\n\n\n>>> Training oldB\nEpoch 1/200\n37/37 - 16s - 422ms/step - accuracy: 0.4899 - f1_score: 0.5168 - loss: 3.0388 - val_accuracy: 0.6200 - val_f1_score: 0.5128 - val_loss: 0.7382 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 32ms/step - accuracy: 0.5046 - f1_score: 0.5143 - loss: 2.0299 - val_accuracy: 0.5000 - val_f1_score: 0.1379 - val_loss: 0.7548 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 32ms/step - accuracy: 0.5025 - f1_score: 0.5092 - loss: 1.4819 - val_accuracy: 0.5800 - val_f1_score: 0.3636 - val_loss: 0.7592 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 32ms/step - accuracy: 0.4937 - f1_score: 0.5088 - loss: 1.2491 - val_accuracy: 0.4200 - val_f1_score: 0.3830 - val_loss: 0.7537 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 32ms/step - accuracy: 0.4886 - f1_score: 0.5055 - loss: 1.0252 - val_accuracy: 0.5200 - val_f1_score: 0.0769 - val_loss: 0.7544 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 32ms/step - accuracy: 0.5106 - f1_score: 0.5062 - loss: 0.9264 - val_accuracy: 0.5000 - val_f1_score: 0.0000e+00 - val_loss: 0.7446 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 32ms/step - accuracy: 0.4818 - f1_score: 0.4390 - loss: 0.8855 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.7422 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 2s - 41ms/step - accuracy: 0.4835 - f1_score: 0.4469 - loss: 0.8320 - val_accuracy: 0.5400 - val_f1_score: 0.4390 - val_loss: 0.7449 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 32ms/step - accuracy: 0.5038 - f1_score: 0.4391 - loss: 0.8201 - val_accuracy: 0.5800 - val_f1_score: 0.2759 - val_loss: 0.7437 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 32ms/step - accuracy: 0.4949 - f1_score: 0.4200 - loss: 0.7923 - val_accuracy: 0.5000 - val_f1_score: 0.2424 - val_loss: 0.7452 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 32ms/step - accuracy: 0.5110 - f1_score: 0.4563 - loss: 0.7895 - val_accuracy: 0.5400 - val_f1_score: 0.2581 - val_loss: 0.7441 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 32ms/step - accuracy: 0.5042 - f1_score: 0.4301 - loss: 0.7797 - val_accuracy: 0.5800 - val_f1_score: 0.0870 - val_loss: 0.7434 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 32ms/step - accuracy: 0.4890 - f1_score: 0.4086 - loss: 0.7870 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.7442 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 32ms/step - accuracy: 0.5013 - f1_score: 0.3794 - loss: 0.7753 - val_accuracy: 0.5800 - val_f1_score: 0.0870 - val_loss: 0.7437 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 32ms/step - accuracy: 0.4793 - f1_score: 0.3346 - loss: 0.7762 - val_accuracy: 0.5800 - val_f1_score: 0.1600 - val_loss: 0.7445 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 32ms/step - accuracy: 0.5089 - f1_score: 0.4228 - loss: 0.7754 - val_accuracy: 0.5400 - val_f1_score: 0.2069 - val_loss: 0.7477 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 36ms/step - accuracy: 0.4928 - f1_score: 0.4922 - loss: 0.7575 - val_accuracy: 0.4600 - val_f1_score: 0.5970 - val_loss: 0.7510 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 32ms/step - accuracy: 0.5038 - f1_score: 0.5048 - loss: 0.7576 - val_accuracy: 0.4400 - val_f1_score: 0.3333 - val_loss: 0.7505 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 32ms/step - accuracy: 0.5110 - f1_score: 0.4639 - loss: 0.7593 - val_accuracy: 0.5000 - val_f1_score: 0.2424 - val_loss: 0.7465 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 32ms/step - accuracy: 0.5076 - f1_score: 0.4832 - loss: 0.7631 - val_accuracy: 0.5000 - val_f1_score: 0.4898 - val_loss: 0.7589 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 32ms/step - accuracy: 0.5266 - f1_score: 0.5370 - loss: 0.7610 - val_accuracy: 0.5000 - val_f1_score: 0.5283 - val_loss: 0.7465 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 32ms/step - accuracy: 0.5127 - f1_score: 0.5034 - loss: 0.7614 - val_accuracy: 0.4800 - val_f1_score: 0.5000 - val_loss: 0.7465 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 32ms/step - accuracy: 0.4814 - f1_score: 0.4552 - loss: 0.7599 - val_accuracy: 0.5600 - val_f1_score: 0.4211 - val_loss: 0.7458 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 32ms/step - accuracy: 0.5034 - f1_score: 0.3650 - loss: 0.7552 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.7439 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 32ms/step - accuracy: 0.5152 - f1_score: 0.3310 - loss: 0.7511 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.7447 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 32ms/step - accuracy: 0.5232 - f1_score: 0.3654 - loss: 0.7484 - val_accuracy: 0.5600 - val_f1_score: 0.0833 - val_loss: 0.7445 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 32ms/step - accuracy: 0.5194 - f1_score: 0.3835 - loss: 0.7537 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.7435 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 32ms/step - accuracy: 0.4949 - f1_score: 0.2525 - loss: 0.7522 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.7425 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 1s - 32ms/step - accuracy: 0.5110 - f1_score: 0.2186 - loss: 0.7446 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.7419 - learning_rate: 6.1987e-04\nEpoch 30/200\n37/37 - 1s - 32ms/step - accuracy: 0.4987 - f1_score: 0.2972 - loss: 0.7504 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.7442 - learning_rate: 5.8827e-04\nEpoch 31/200\n37/37 - 1s - 32ms/step - accuracy: 0.5076 - f1_score: 0.4075 - loss: 0.7522 - val_accuracy: 0.5200 - val_f1_score: 0.0769 - val_loss: 0.7442 - learning_rate: 5.5621e-04\nEpoch 32/200\n37/37 - 1s - 32ms/step - accuracy: 0.5055 - f1_score: 0.4212 - loss: 0.7489 - val_accuracy: 0.5200 - val_f1_score: 0.3684 - val_loss: 0.7448 - learning_rate: 5.2388e-04\nEpoch 33/200\n37/37 - 1s - 32ms/step - accuracy: 0.4983 - f1_score: 0.4194 - loss: 0.7512 - val_accuracy: 0.5200 - val_f1_score: 0.2500 - val_loss: 0.7447 - learning_rate: 4.9148e-04\nEpoch 34/200\n37/37 - 1s - 32ms/step - accuracy: 0.5262 - f1_score: 0.3974 - loss: 0.7459 - val_accuracy: 0.5400 - val_f1_score: 0.2581 - val_loss: 0.7443 - learning_rate: 4.5920e-04\nEpoch 35/200\n37/37 - 1s - 32ms/step - accuracy: 0.5110 - f1_score: 0.3937 - loss: 0.7442 - val_accuracy: 0.5600 - val_f1_score: 0.3125 - val_loss: 0.7446 - learning_rate: 4.2723e-04\nEpoch 36/200\n37/37 - 1s - 32ms/step - accuracy: 0.5160 - f1_score: 0.4224 - loss: 0.7414 - val_accuracy: 0.5800 - val_f1_score: 0.4000 - val_loss: 0.7445 - learning_rate: 3.9575e-04\nEpoch 37/200\n37/37 - 1s - 32ms/step - accuracy: 0.5051 - f1_score: 0.4288 - loss: 0.7440 - val_accuracy: 0.5800 - val_f1_score: 0.3226 - val_loss: 0.7439 - learning_rate: 3.6494e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 673ms/step\noldB →  val F1 = 0.5970\n              precision    recall  f1-score   support\n\n        Left       0.60      0.11      0.18        28\n       Right       0.44      0.91      0.60        22\n\n    accuracy                           0.46        50\n   macro avg       0.52      0.51      0.39        50\nweighted avg       0.53      0.46      0.36        50\n\n\n>>> Training model1\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\nEpoch 1/200\n37/37 - 30s - 805ms/step - accuracy: 0.5110 - f1_score: 0.0446 - loss: 0.6934 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6918 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 23ms/step - accuracy: 0.5008 - f1_score: 0.0000e+00 - loss: 0.6933 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6931 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 22ms/step - accuracy: 0.4932 - f1_score: 0.0798 - loss: 0.6932 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6925 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 40ms/step - accuracy: 0.4894 - f1_score: 0.5095 - loss: 0.6934 - val_accuracy: 0.4400 - val_f1_score: 0.5758 - val_loss: 0.6938 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 40ms/step - accuracy: 0.5008 - f1_score: 0.5485 - loss: 0.6932 - val_accuracy: 0.4200 - val_f1_score: 0.5915 - val_loss: 0.6969 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 39ms/step - accuracy: 0.5160 - f1_score: 0.6801 - loss: 0.6931 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6983 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 15ms/step - accuracy: 0.5165 - f1_score: 0.6811 - loss: 0.6927 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6981 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 22ms/step - accuracy: 0.4916 - f1_score: 0.6091 - loss: 0.6938 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6928 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 21ms/step - accuracy: 0.4958 - f1_score: 0.3768 - loss: 0.6932 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6938 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 22ms/step - accuracy: 0.5236 - f1_score: 0.6874 - loss: 0.6927 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6962 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 22ms/step - accuracy: 0.5025 - f1_score: 0.6689 - loss: 0.6935 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6945 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 22ms/step - accuracy: 0.5215 - f1_score: 0.6855 - loss: 0.6926 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6967 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 23ms/step - accuracy: 0.5008 - f1_score: 0.6674 - loss: 0.6934 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6950 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 22ms/step - accuracy: 0.4814 - f1_score: 0.4208 - loss: 0.6935 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6927 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 22ms/step - accuracy: 0.4966 - f1_score: 0.6019 - loss: 0.6933 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6943 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 22ms/step - accuracy: 0.4827 - f1_score: 0.5800 - loss: 0.6933 - val_accuracy: 0.4400 - val_f1_score: 0.6000 - val_loss: 0.6938 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 22ms/step - accuracy: 0.5063 - f1_score: 0.6607 - loss: 0.6931 - val_accuracy: 0.4200 - val_f1_score: 0.5538 - val_loss: 0.6988 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 22ms/step - accuracy: 0.5131 - f1_score: 0.6771 - loss: 0.6924 - val_accuracy: 0.4200 - val_f1_score: 0.5915 - val_loss: 0.6998 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 22ms/step - accuracy: 0.5220 - f1_score: 0.6840 - loss: 0.6924 - val_accuracy: 0.4000 - val_f1_score: 0.5455 - val_loss: 0.7232 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 22ms/step - accuracy: 0.5106 - f1_score: 0.6751 - loss: 0.6920 - val_accuracy: 0.4200 - val_f1_score: 0.5672 - val_loss: 0.9665 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 22ms/step - accuracy: 0.4970 - f1_score: 0.6629 - loss: 0.6923 - val_accuracy: 0.4400 - val_f1_score: 0.5625 - val_loss: 1.5243 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 24ms/step - accuracy: 0.5076 - f1_score: 0.6723 - loss: 0.6960 - val_accuracy: 0.4200 - val_f1_score: 0.5538 - val_loss: 1.1272 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 25ms/step - accuracy: 0.5169 - f1_score: 0.6804 - loss: 0.6924 - val_accuracy: 0.4400 - val_f1_score: 0.6000 - val_loss: 0.6992 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 23ms/step - accuracy: 0.5253 - f1_score: 0.6883 - loss: 0.6921 - val_accuracy: 0.4400 - val_f1_score: 0.5882 - val_loss: 0.7140 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 22ms/step - accuracy: 0.5034 - f1_score: 0.6559 - loss: 0.6920 - val_accuracy: 0.5400 - val_f1_score: 0.3429 - val_loss: 1.1313 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 22ms/step - accuracy: 0.5190 - f1_score: 0.6772 - loss: 0.6915 - val_accuracy: 0.4000 - val_f1_score: 0.5312 - val_loss: 0.9748 - learning_rate: 7.1022e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6s/step\nmodel1 →  val F1 = 0.6111\n              precision    recall  f1-score   support\n\n        Left       0.00      0.00      0.00        28\n       Right       0.44      1.00      0.61        22\n\n    accuracy                           0.44        50\n   macro avg       0.22      0.50      0.31        50\nweighted avg       0.19      0.44      0.27        50\n\n\n>>> Training model2\nEpoch 1/200\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"37/37 - 5s - 137ms/step - accuracy: 0.5177 - f1_score: 0.4307 - loss: 0.6954 - val_accuracy: 0.4000 - val_f1_score: 0.4643 - val_loss: 0.7069 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 21ms/step - accuracy: 0.5220 - f1_score: 0.5811 - loss: 0.6946 - val_accuracy: 0.4400 - val_f1_score: 0.4615 - val_loss: 0.7082 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 22ms/step - accuracy: 0.4924 - f1_score: 0.6276 - loss: 0.6944 - val_accuracy: 0.4200 - val_f1_score: 0.5085 - val_loss: 0.7002 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 21ms/step - accuracy: 0.5110 - f1_score: 0.4678 - loss: 0.6942 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.7013 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 20ms/step - accuracy: 0.5004 - f1_score: 0.5845 - loss: 0.6927 - val_accuracy: 0.4800 - val_f1_score: 0.5000 - val_loss: 0.7040 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 21ms/step - accuracy: 0.5114 - f1_score: 0.3983 - loss: 0.6936 - val_accuracy: 0.5000 - val_f1_score: 0.2424 - val_loss: 0.7075 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 21ms/step - accuracy: 0.5203 - f1_score: 0.5872 - loss: 0.6901 - val_accuracy: 0.5400 - val_f1_score: 0.5106 - val_loss: 0.7027 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 22ms/step - accuracy: 0.5190 - f1_score: 0.5661 - loss: 0.6911 - val_accuracy: 0.4000 - val_f1_score: 0.5455 - val_loss: 0.7336 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 20ms/step - accuracy: 0.5220 - f1_score: 0.6058 - loss: 0.6925 - val_accuracy: 0.5000 - val_f1_score: 0.3590 - val_loss: 0.7176 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 21ms/step - accuracy: 0.5139 - f1_score: 0.4383 - loss: 0.6918 - val_accuracy: 0.5200 - val_f1_score: 0.2500 - val_loss: 0.7214 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 21ms/step - accuracy: 0.5363 - f1_score: 0.3845 - loss: 0.6883 - val_accuracy: 0.4800 - val_f1_score: 0.3500 - val_loss: 0.7367 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 21ms/step - accuracy: 0.5063 - f1_score: 0.4793 - loss: 0.6907 - val_accuracy: 0.5000 - val_f1_score: 0.2424 - val_loss: 0.7546 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 21ms/step - accuracy: 0.5194 - f1_score: 0.4555 - loss: 0.6913 - val_accuracy: 0.4600 - val_f1_score: 0.5091 - val_loss: 0.7363 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 21ms/step - accuracy: 0.5177 - f1_score: 0.4593 - loss: 0.6916 - val_accuracy: 0.5400 - val_f1_score: 0.4103 - val_loss: 0.7470 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 22ms/step - accuracy: 0.5439 - f1_score: 0.3735 - loss: 0.6879 - val_accuracy: 0.4800 - val_f1_score: 0.1875 - val_loss: 0.7223 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 21ms/step - accuracy: 0.5220 - f1_score: 0.4061 - loss: 0.6905 - val_accuracy: 0.4000 - val_f1_score: 0.3182 - val_loss: 0.7326 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 21ms/step - accuracy: 0.5165 - f1_score: 0.4607 - loss: 0.6923 - val_accuracy: 0.4200 - val_f1_score: 0.2564 - val_loss: 0.7203 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 24ms/step - accuracy: 0.5080 - f1_score: 0.5440 - loss: 0.6913 - val_accuracy: 0.3400 - val_f1_score: 0.3265 - val_loss: 0.7220 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 21ms/step - accuracy: 0.5118 - f1_score: 0.5039 - loss: 0.6876 - val_accuracy: 0.5400 - val_f1_score: 0.4390 - val_loss: 0.7345 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 22ms/step - accuracy: 0.5279 - f1_score: 0.4909 - loss: 0.6905 - val_accuracy: 0.4800 - val_f1_score: 0.5517 - val_loss: 0.7221 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 20ms/step - accuracy: 0.5177 - f1_score: 0.6097 - loss: 0.6905 - val_accuracy: 0.4200 - val_f1_score: 0.4528 - val_loss: 0.7168 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 21ms/step - accuracy: 0.5262 - f1_score: 0.4339 - loss: 0.6888 - val_accuracy: 0.5400 - val_f1_score: 0.4651 - val_loss: 0.7350 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 21ms/step - accuracy: 0.5334 - f1_score: 0.5906 - loss: 0.6885 - val_accuracy: 0.4400 - val_f1_score: 0.3913 - val_loss: 0.7234 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 20ms/step - accuracy: 0.5207 - f1_score: 0.4753 - loss: 0.6888 - val_accuracy: 0.5400 - val_f1_score: 0.5490 - val_loss: 0.7435 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 22ms/step - accuracy: 0.5312 - f1_score: 0.5674 - loss: 0.6859 - val_accuracy: 0.5200 - val_f1_score: 0.6129 - val_loss: 0.7722 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 20ms/step - accuracy: 0.5211 - f1_score: 0.5984 - loss: 0.6868 - val_accuracy: 0.5800 - val_f1_score: 0.5714 - val_loss: 0.7565 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 21ms/step - accuracy: 0.5334 - f1_score: 0.4815 - loss: 0.6882 - val_accuracy: 0.5800 - val_f1_score: 0.5116 - val_loss: 0.7500 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 21ms/step - accuracy: 0.5236 - f1_score: 0.5171 - loss: 0.6874 - val_accuracy: 0.5400 - val_f1_score: 0.5490 - val_loss: 0.7462 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 1s - 21ms/step - accuracy: 0.5211 - f1_score: 0.4919 - loss: 0.6892 - val_accuracy: 0.4600 - val_f1_score: 0.5263 - val_loss: 0.7478 - learning_rate: 6.1987e-04\nEpoch 30/200\n37/37 - 1s - 21ms/step - accuracy: 0.5304 - f1_score: 0.5804 - loss: 0.6846 - val_accuracy: 0.5400 - val_f1_score: 0.5965 - val_loss: 0.7516 - learning_rate: 5.8827e-04\nEpoch 31/200\n37/37 - 1s - 21ms/step - accuracy: 0.5405 - f1_score: 0.6444 - loss: 0.6865 - val_accuracy: 0.4600 - val_f1_score: 0.5846 - val_loss: 0.7449 - learning_rate: 5.5621e-04\nEpoch 32/200\n37/37 - 1s - 21ms/step - accuracy: 0.5139 - f1_score: 0.5917 - loss: 0.6883 - val_accuracy: 0.5600 - val_f1_score: 0.5600 - val_loss: 0.7473 - learning_rate: 5.2388e-04\nEpoch 33/200\n37/37 - 1s - 21ms/step - accuracy: 0.5186 - f1_score: 0.5447 - loss: 0.6883 - val_accuracy: 0.4400 - val_f1_score: 0.5172 - val_loss: 0.7538 - learning_rate: 4.9148e-04\nEpoch 34/200\n37/37 - 1s - 21ms/step - accuracy: 0.5253 - f1_score: 0.5176 - loss: 0.6874 - val_accuracy: 0.4200 - val_f1_score: 0.4912 - val_loss: 0.7408 - learning_rate: 4.5920e-04\nEpoch 35/200\n37/37 - 1s - 21ms/step - accuracy: 0.5236 - f1_score: 0.5838 - loss: 0.6872 - val_accuracy: 0.5400 - val_f1_score: 0.5306 - val_loss: 0.7366 - learning_rate: 4.2723e-04\nEpoch 36/200\n37/37 - 1s - 21ms/step - accuracy: 0.5351 - f1_score: 0.5100 - loss: 0.6873 - val_accuracy: 0.5800 - val_f1_score: 0.5532 - val_loss: 0.7199 - learning_rate: 3.9575e-04\nEpoch 37/200\n37/37 - 1s - 21ms/step - accuracy: 0.5439 - f1_score: 0.4540 - loss: 0.6858 - val_accuracy: 0.6400 - val_f1_score: 0.5500 - val_loss: 0.7263 - learning_rate: 3.6494e-04\nEpoch 38/200\n37/37 - 1s - 21ms/step - accuracy: 0.5346 - f1_score: 0.4325 - loss: 0.6867 - val_accuracy: 0.6400 - val_f1_score: 0.6087 - val_loss: 0.7527 - learning_rate: 3.3498e-04\nEpoch 39/200\n37/37 - 1s - 21ms/step - accuracy: 0.5215 - f1_score: 0.5076 - loss: 0.6853 - val_accuracy: 0.5800 - val_f1_score: 0.5333 - val_loss: 0.7577 - learning_rate: 3.0602e-04\nEpoch 40/200\n37/37 - 1s - 21ms/step - accuracy: 0.5342 - f1_score: 0.4989 - loss: 0.6872 - val_accuracy: 0.5800 - val_f1_score: 0.5116 - val_loss: 0.7867 - learning_rate: 2.7820e-04\nEpoch 41/200\n37/37 - 1s - 22ms/step - accuracy: 0.5279 - f1_score: 0.5084 - loss: 0.6857 - val_accuracy: 0.5800 - val_f1_score: 0.4000 - val_loss: 0.7533 - learning_rate: 2.5163e-04\nEpoch 42/200\n37/37 - 1s - 21ms/step - accuracy: 0.5186 - f1_score: 0.4761 - loss: 0.6887 - val_accuracy: 0.6000 - val_f1_score: 0.5652 - val_loss: 0.7464 - learning_rate: 2.2643e-04\nEpoch 43/200\n37/37 - 1s - 22ms/step - accuracy: 0.5367 - f1_score: 0.4818 - loss: 0.6847 - val_accuracy: 0.6800 - val_f1_score: 0.6190 - val_loss: 0.7477 - learning_rate: 2.0267e-04\nEpoch 44/200\n37/37 - 1s - 20ms/step - accuracy: 0.5507 - f1_score: 0.4316 - loss: 0.6851 - val_accuracy: 0.6600 - val_f1_score: 0.5854 - val_loss: 0.7593 - learning_rate: 1.8042e-04\nEpoch 45/200\n37/37 - 1s - 21ms/step - accuracy: 0.5334 - f1_score: 0.4107 - loss: 0.6851 - val_accuracy: 0.6800 - val_f1_score: 0.5556 - val_loss: 0.7481 - learning_rate: 1.5972e-04\nEpoch 46/200\n37/37 - 1s - 21ms/step - accuracy: 0.5431 - f1_score: 0.4468 - loss: 0.6865 - val_accuracy: 0.6200 - val_f1_score: 0.5581 - val_loss: 0.7429 - learning_rate: 1.4058e-04\nEpoch 47/200\n37/37 - 1s - 21ms/step - accuracy: 0.5465 - f1_score: 0.4797 - loss: 0.6884 - val_accuracy: 0.6000 - val_f1_score: 0.5652 - val_loss: 0.7403 - learning_rate: 1.2302e-04\nEpoch 48/200\n37/37 - 1s - 21ms/step - accuracy: 0.5325 - f1_score: 0.5082 - loss: 0.6841 - val_accuracy: 0.5400 - val_f1_score: 0.5490 - val_loss: 0.7476 - learning_rate: 1.0700e-04\nEpoch 49/200\n37/37 - 1s - 21ms/step - accuracy: 0.5342 - f1_score: 0.5156 - loss: 0.6843 - val_accuracy: 0.5400 - val_f1_score: 0.5490 - val_loss: 0.7490 - learning_rate: 9.2503e-05\nEpoch 50/200\n37/37 - 1s - 21ms/step - accuracy: 0.5393 - f1_score: 0.4984 - loss: 0.6863 - val_accuracy: 0.6000 - val_f1_score: 0.5833 - val_loss: 0.7442 - learning_rate: 7.9466e-05\nEpoch 51/200\n37/37 - 1s - 22ms/step - accuracy: 0.5435 - f1_score: 0.5120 - loss: 0.6858 - val_accuracy: 0.6800 - val_f1_score: 0.6364 - val_loss: 0.7439 - learning_rate: 6.7829e-05\nEpoch 52/200\n37/37 - 1s - 20ms/step - accuracy: 0.5380 - f1_score: 0.5333 - loss: 0.6845 - val_accuracy: 0.6000 - val_f1_score: 0.5833 - val_loss: 0.7474 - learning_rate: 5.7516e-05\nEpoch 53/200\n37/37 - 1s - 22ms/step - accuracy: 0.5579 - f1_score: 0.5386 - loss: 0.6839 - val_accuracy: 0.6200 - val_f1_score: 0.5957 - val_loss: 0.7460 - learning_rate: 4.8444e-05\nEpoch 54/200\n37/37 - 1s - 21ms/step - accuracy: 0.5249 - f1_score: 0.5046 - loss: 0.6847 - val_accuracy: 0.6200 - val_f1_score: 0.5957 - val_loss: 0.7495 - learning_rate: 4.0524e-05\nEpoch 55/200\n37/37 - 1s - 21ms/step - accuracy: 0.5236 - f1_score: 0.5104 - loss: 0.6853 - val_accuracy: 0.6600 - val_f1_score: 0.6222 - val_loss: 0.7498 - learning_rate: 3.3661e-05\nEpoch 56/200\n37/37 - 1s - 21ms/step - accuracy: 0.5359 - f1_score: 0.4993 - loss: 0.6852 - val_accuracy: 0.6600 - val_f1_score: 0.6047 - val_loss: 0.7501 - learning_rate: 2.7761e-05\nEpoch 57/200\n37/37 - 1s - 21ms/step - accuracy: 0.5274 - f1_score: 0.4975 - loss: 0.6857 - val_accuracy: 0.6400 - val_f1_score: 0.5909 - val_loss: 0.7513 - learning_rate: 2.2728e-05\nEpoch 58/200\n37/37 - 1s - 23ms/step - accuracy: 0.5312 - f1_score: 0.5186 - loss: 0.6857 - val_accuracy: 0.6200 - val_f1_score: 0.5778 - val_loss: 0.7510 - learning_rate: 1.8470e-05\nEpoch 59/200\n37/37 - 1s - 22ms/step - accuracy: 0.5536 - f1_score: 0.5426 - loss: 0.6810 - val_accuracy: 0.6400 - val_f1_score: 0.6087 - val_loss: 0.7523 - learning_rate: 1.4895e-05\nEpoch 60/200\n37/37 - 1s - 20ms/step - accuracy: 0.5355 - f1_score: 0.5184 - loss: 0.6857 - val_accuracy: 0.6400 - val_f1_score: 0.6087 - val_loss: 0.7533 - learning_rate: 1.1919e-05\nEpoch 61/200\n37/37 - 1s - 21ms/step - accuracy: 0.5435 - f1_score: 0.5193 - loss: 0.6845 - val_accuracy: 0.6600 - val_f1_score: 0.6222 - val_loss: 0.7538 - learning_rate: 9.4625e-06\nEpoch 62/200\n37/37 - 1s - 20ms/step - accuracy: 0.5427 - f1_score: 0.5310 - loss: 0.6830 - val_accuracy: 0.6600 - val_f1_score: 0.6222 - val_loss: 0.7541 - learning_rate: 7.4517e-06\nEpoch 63/200\n37/37 - 1s - 21ms/step - accuracy: 0.5236 - f1_score: 0.5074 - loss: 0.6861 - val_accuracy: 0.6600 - val_f1_score: 0.6222 - val_loss: 0.7537 - learning_rate: 5.8201e-06\nEpoch 64/200\n37/37 - 1s - 20ms/step - accuracy: 0.5498 - f1_score: 0.5237 - loss: 0.6820 - val_accuracy: 0.6600 - val_f1_score: 0.6222 - val_loss: 0.7535 - learning_rate: 4.5077e-06\nEpoch 65/200\n37/37 - 1s - 20ms/step - accuracy: 0.5452 - f1_score: 0.5258 - loss: 0.6834 - val_accuracy: 0.6600 - val_f1_score: 0.6222 - val_loss: 0.7535 - learning_rate: 3.4615e-06\nEpoch 66/200\n37/37 - 1s - 21ms/step - accuracy: 0.5342 - f1_score: 0.5181 - loss: 0.6846 - val_accuracy: 0.6600 - val_f1_score: 0.6222 - val_loss: 0.7538 - learning_rate: 2.6351e-06\nEpoch 67/200\n37/37 - 1s - 21ms/step - accuracy: 0.5080 - f1_score: 0.4755 - loss: 0.6900 - val_accuracy: 0.6600 - val_f1_score: 0.6222 - val_loss: 0.7540 - learning_rate: 1.9882e-06\nEpoch 68/200\n37/37 - 1s - 20ms/step - accuracy: 0.5376 - f1_score: 0.5282 - loss: 0.6835 - val_accuracy: 0.6600 - val_f1_score: 0.6222 - val_loss: 0.7539 - learning_rate: 1.4867e-06\nEpoch 69/200\n37/37 - 1s - 21ms/step - accuracy: 0.5296 - f1_score: 0.4986 - loss: 0.6861 - val_accuracy: 0.6600 - val_f1_score: 0.6222 - val_loss: 0.7539 - learning_rate: 1.1014e-06\nEpoch 70/200\n37/37 - 1s - 21ms/step - accuracy: 0.5367 - f1_score: 0.5191 - loss: 0.6844 - val_accuracy: 0.6600 - val_f1_score: 0.6222 - val_loss: 0.7539 - learning_rate: 8.0841e-07\nEpoch 71/200\n37/37 - 1s - 21ms/step - accuracy: 0.5448 - f1_score: 0.5192 - loss: 0.6849 - val_accuracy: 0.6600 - val_f1_score: 0.6222 - val_loss: 0.7538 - learning_rate: 5.8771e-07\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 328ms/step\nmodel2 →  val F1 = 0.6364\n              precision    recall  f1-score   support\n\n        Left       0.71      0.71      0.71        28\n       Right       0.64      0.64      0.64        22\n\n    accuracy                           0.68        50\n   macro avg       0.68      0.68      0.68        50\nweighted avg       0.68      0.68      0.68        50\n\n\n>>> Training model3\nEpoch 1/200\n37/37 - 13s - 353ms/step - accuracy: 0.5076 - f1_score: 0.4748 - loss: 0.7023 - val_accuracy: 0.5000 - val_f1_score: 0.4444 - val_loss: 0.7168 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 3s - 87ms/step - accuracy: 0.5152 - f1_score: 0.6116 - loss: 0.6897 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7244 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 39ms/step - accuracy: 0.5203 - f1_score: 0.6407 - loss: 0.6852 - val_accuracy: 0.4400 - val_f1_score: 0.6000 - val_loss: 0.7049 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 39ms/step - accuracy: 0.5739 - f1_score: 0.5784 - loss: 0.6736 - val_accuracy: 0.5600 - val_f1_score: 0.5600 - val_loss: 0.7109 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 39ms/step - accuracy: 0.6102 - f1_score: 0.6816 - loss: 0.6472 - val_accuracy: 0.5400 - val_f1_score: 0.5490 - val_loss: 0.9304 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 39ms/step - accuracy: 0.6736 - f1_score: 0.7112 - loss: 0.5811 - val_accuracy: 0.6000 - val_f1_score: 0.4737 - val_loss: 1.6508 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 39ms/step - accuracy: 0.7829 - f1_score: 0.7775 - loss: 0.4595 - val_accuracy: 0.5800 - val_f1_score: 0.5882 - val_loss: 1.2516 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 39ms/step - accuracy: 0.8742 - f1_score: 0.8728 - loss: 0.2967 - val_accuracy: 0.4800 - val_f1_score: 0.5667 - val_loss: 2.3308 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 38ms/step - accuracy: 0.9358 - f1_score: 0.9379 - loss: 0.1829 - val_accuracy: 0.5000 - val_f1_score: 0.5098 - val_loss: 2.1388 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 38ms/step - accuracy: 0.9620 - f1_score: 0.9627 - loss: 0.1086 - val_accuracy: 0.5200 - val_f1_score: 0.4783 - val_loss: 2.8841 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 39ms/step - accuracy: 0.9861 - f1_score: 0.9857 - loss: 0.0506 - val_accuracy: 0.5400 - val_f1_score: 0.5106 - val_loss: 3.1573 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 39ms/step - accuracy: 0.9873 - f1_score: 0.9874 - loss: 0.0606 - val_accuracy: 0.5800 - val_f1_score: 0.5532 - val_loss: 1.3880 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 39ms/step - accuracy: 0.9894 - f1_score: 0.9896 - loss: 0.0346 - val_accuracy: 0.4600 - val_f1_score: 0.4255 - val_loss: 5.0198 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 38ms/step - accuracy: 0.9886 - f1_score: 0.9884 - loss: 0.0279 - val_accuracy: 0.5400 - val_f1_score: 0.5490 - val_loss: 9.3168 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 39ms/step - accuracy: 0.9958 - f1_score: 0.9959 - loss: 0.0207 - val_accuracy: 0.4800 - val_f1_score: 0.4091 - val_loss: 5.8814 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 39ms/step - accuracy: 0.9962 - f1_score: 0.9963 - loss: 0.0088 - val_accuracy: 0.4600 - val_f1_score: 0.4906 - val_loss: 4.8603 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 39ms/step - accuracy: 0.9992 - f1_score: 0.9992 - loss: 0.0033 - val_accuracy: 0.4400 - val_f1_score: 0.3913 - val_loss: 6.4385 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 39ms/step - accuracy: 0.9992 - f1_score: 0.9992 - loss: 0.0036 - val_accuracy: 0.5000 - val_f1_score: 0.4681 - val_loss: 5.6095 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 38ms/step - accuracy: 0.9992 - f1_score: 0.9992 - loss: 0.0044 - val_accuracy: 0.4800 - val_f1_score: 0.5357 - val_loss: 3.2417 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 39ms/step - accuracy: 0.9958 - f1_score: 0.9959 - loss: 0.0187 - val_accuracy: 0.5000 - val_f1_score: 0.5098 - val_loss: 7.5912 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 38ms/step - accuracy: 0.9962 - f1_score: 0.9963 - loss: 0.0153 - val_accuracy: 0.4400 - val_f1_score: 0.4167 - val_loss: 4.7984 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 39ms/step - accuracy: 0.9987 - f1_score: 0.9988 - loss: 0.0075 - val_accuracy: 0.4800 - val_f1_score: 0.4091 - val_loss: 3.9206 - learning_rate: 8.1479e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340ms/step\nmodel3 →  val F1 = 0.6111\n              precision    recall  f1-score   support\n\n        Left       0.00      0.00      0.00        28\n       Right       0.44      1.00      0.61        22\n\n    accuracy                           0.44        50\n   macro avg       0.22      0.50      0.31        50\nweighted avg       0.19      0.44      0.27        50\n\n\n>>> Training model4\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200\n37/37 - 10s - 275ms/step - accuracy: 0.5156 - f1_score: 0.5624 - loss: 0.6918 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.7194 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 4s - 118ms/step - accuracy: 0.5241 - f1_score: 0.4756 - loss: 0.6920 - val_accuracy: 0.5000 - val_f1_score: 0.5455 - val_loss: 0.7134 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 4s - 117ms/step - accuracy: 0.5405 - f1_score: 0.4619 - loss: 0.6882 - val_accuracy: 0.5000 - val_f1_score: 0.4681 - val_loss: 0.7089 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 4s - 116ms/step - accuracy: 0.5376 - f1_score: 0.4778 - loss: 0.6911 - val_accuracy: 0.5400 - val_f1_score: 0.5306 - val_loss: 0.6992 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 4s - 116ms/step - accuracy: 0.5220 - f1_score: 0.4374 - loss: 0.6923 - val_accuracy: 0.5000 - val_f1_score: 0.5098 - val_loss: 0.7047 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 4s - 116ms/step - accuracy: 0.5545 - f1_score: 0.5054 - loss: 0.6858 - val_accuracy: 0.5000 - val_f1_score: 0.4444 - val_loss: 0.7718 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 4s - 117ms/step - accuracy: 0.5439 - f1_score: 0.5317 - loss: 0.6890 - val_accuracy: 0.5200 - val_f1_score: 0.5862 - val_loss: 0.7132 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 4s - 115ms/step - accuracy: 0.5304 - f1_score: 0.5320 - loss: 0.6872 - val_accuracy: 0.5000 - val_f1_score: 0.4681 - val_loss: 0.7044 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 4s - 116ms/step - accuracy: 0.5494 - f1_score: 0.4575 - loss: 0.6865 - val_accuracy: 0.5600 - val_f1_score: 0.4762 - val_loss: 0.6937 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 4s - 116ms/step - accuracy: 0.5486 - f1_score: 0.4726 - loss: 0.6864 - val_accuracy: 0.5800 - val_f1_score: 0.5116 - val_loss: 0.7044 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 4s - 116ms/step - accuracy: 0.5574 - f1_score: 0.5024 - loss: 0.6856 - val_accuracy: 0.4600 - val_f1_score: 0.5091 - val_loss: 0.7112 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 4s - 116ms/step - accuracy: 0.5460 - f1_score: 0.5563 - loss: 0.6845 - val_accuracy: 0.4800 - val_f1_score: 0.4348 - val_loss: 0.7227 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 4s - 115ms/step - accuracy: 0.5532 - f1_score: 0.5517 - loss: 0.6834 - val_accuracy: 0.5400 - val_f1_score: 0.4889 - val_loss: 0.6998 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 4s - 117ms/step - accuracy: 0.5452 - f1_score: 0.5053 - loss: 0.6900 - val_accuracy: 0.5600 - val_f1_score: 0.5926 - val_loss: 0.7001 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 4s - 116ms/step - accuracy: 0.5587 - f1_score: 0.6061 - loss: 0.6825 - val_accuracy: 0.5400 - val_f1_score: 0.5106 - val_loss: 0.7152 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 4s - 116ms/step - accuracy: 0.5591 - f1_score: 0.4816 - loss: 0.6840 - val_accuracy: 0.5200 - val_f1_score: 0.5000 - val_loss: 0.7177 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 4s - 116ms/step - accuracy: 0.5566 - f1_score: 0.4771 - loss: 0.6815 - val_accuracy: 0.4600 - val_f1_score: 0.4706 - val_loss: 0.7219 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 4s - 115ms/step - accuracy: 0.5739 - f1_score: 0.5776 - loss: 0.6780 - val_accuracy: 0.4800 - val_f1_score: 0.5357 - val_loss: 0.7709 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 4s - 115ms/step - accuracy: 0.5532 - f1_score: 0.5111 - loss: 0.6807 - val_accuracy: 0.4400 - val_f1_score: 0.4615 - val_loss: 0.7525 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 4s - 115ms/step - accuracy: 0.5735 - f1_score: 0.5844 - loss: 0.6769 - val_accuracy: 0.5400 - val_f1_score: 0.4651 - val_loss: 0.7117 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 4s - 116ms/step - accuracy: 0.5781 - f1_score: 0.5747 - loss: 0.6732 - val_accuracy: 0.4800 - val_f1_score: 0.1875 - val_loss: 0.7836 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 4s - 116ms/step - accuracy: 0.5756 - f1_score: 0.5793 - loss: 0.6753 - val_accuracy: 0.4200 - val_f1_score: 0.5085 - val_loss: 0.7740 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 4s - 115ms/step - accuracy: 0.5612 - f1_score: 0.5712 - loss: 0.6742 - val_accuracy: 0.4600 - val_f1_score: 0.5091 - val_loss: 0.7382 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 4s - 116ms/step - accuracy: 0.5688 - f1_score: 0.5382 - loss: 0.6755 - val_accuracy: 0.4800 - val_f1_score: 0.5185 - val_loss: 0.7372 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 4s - 116ms/step - accuracy: 0.5747 - f1_score: 0.5691 - loss: 0.6751 - val_accuracy: 0.4800 - val_f1_score: 0.5667 - val_loss: 0.7479 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 4s - 116ms/step - accuracy: 0.5815 - f1_score: 0.5859 - loss: 0.6755 - val_accuracy: 0.4600 - val_f1_score: 0.5263 - val_loss: 0.7284 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 4s - 115ms/step - accuracy: 0.5925 - f1_score: 0.5864 - loss: 0.6630 - val_accuracy: 0.4600 - val_f1_score: 0.4490 - val_loss: 0.7906 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 4s - 115ms/step - accuracy: 0.6018 - f1_score: 0.5934 - loss: 0.6609 - val_accuracy: 0.5400 - val_f1_score: 0.5106 - val_loss: 0.7837 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 4s - 115ms/step - accuracy: 0.5735 - f1_score: 0.5102 - loss: 0.6750 - val_accuracy: 0.4000 - val_f1_score: 0.4444 - val_loss: 0.7389 - learning_rate: 6.1987e-04\nEpoch 30/200\n37/37 - 4s - 115ms/step - accuracy: 0.5988 - f1_score: 0.5732 - loss: 0.6656 - val_accuracy: 0.4400 - val_f1_score: 0.3636 - val_loss: 0.8029 - learning_rate: 5.8827e-04\nEpoch 31/200\n37/37 - 4s - 116ms/step - accuracy: 0.6039 - f1_score: 0.5805 - loss: 0.6581 - val_accuracy: 0.5400 - val_f1_score: 0.4390 - val_loss: 0.8152 - learning_rate: 5.5621e-04\nEpoch 32/200\n37/37 - 4s - 116ms/step - accuracy: 0.6106 - f1_score: 0.6012 - loss: 0.6644 - val_accuracy: 0.4400 - val_f1_score: 0.4400 - val_loss: 0.7790 - learning_rate: 5.2388e-04\nEpoch 33/200\n37/37 - 4s - 116ms/step - accuracy: 0.6043 - f1_score: 0.6126 - loss: 0.6586 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.7988 - learning_rate: 4.9148e-04\nEpoch 34/200\n37/37 - 4s - 116ms/step - accuracy: 0.6056 - f1_score: 0.6200 - loss: 0.6561 - val_accuracy: 0.4200 - val_f1_score: 0.4727 - val_loss: 0.8191 - learning_rate: 4.5920e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259ms/step\nmodel4 →  val F1 = 0.5926\n              precision    recall  f1-score   support\n\n        Left       0.67      0.43      0.52        28\n       Right       0.50      0.73      0.59        22\n\n    accuracy                           0.56        50\n   macro avg       0.58      0.58      0.56        50\nweighted avg       0.59      0.56      0.55        50\n\n\n>>> Training model5\nEpoch 1/200\n37/37 - 10s - 262ms/step - accuracy: 0.5186 - f1_score: 0.5270 - loss: 1.0615 - val_accuracy: 0.4800 - val_f1_score: 0.4800 - val_loss: 0.7653 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 23ms/step - accuracy: 0.5169 - f1_score: 0.5342 - loss: 0.7445 - val_accuracy: 0.4600 - val_f1_score: 0.4706 - val_loss: 0.8123 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 22ms/step - accuracy: 0.5456 - f1_score: 0.5358 - loss: 0.7081 - val_accuracy: 0.5800 - val_f1_score: 0.3226 - val_loss: 0.7000 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 35ms/step - accuracy: 0.5486 - f1_score: 0.5595 - loss: 0.6923 - val_accuracy: 0.5400 - val_f1_score: 0.5106 - val_loss: 0.7172 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 24ms/step - accuracy: 0.6655 - f1_score: 0.6700 - loss: 0.6005 - val_accuracy: 0.5800 - val_f1_score: 0.4324 - val_loss: 0.7528 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 38ms/step - accuracy: 0.7251 - f1_score: 0.7304 - loss: 0.5297 - val_accuracy: 0.6200 - val_f1_score: 0.5366 - val_loss: 1.2202 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 21ms/step - accuracy: 0.7538 - f1_score: 0.7568 - loss: 0.5196 - val_accuracy: 0.5600 - val_f1_score: 0.5000 - val_loss: 1.5708 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 22ms/step - accuracy: 0.7694 - f1_score: 0.7791 - loss: 0.4847 - val_accuracy: 0.5200 - val_f1_score: 0.5000 - val_loss: 1.5018 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 21ms/step - accuracy: 0.8285 - f1_score: 0.8308 - loss: 0.3650 - val_accuracy: 0.5400 - val_f1_score: 0.4390 - val_loss: 1.6859 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 22ms/step - accuracy: 0.8932 - f1_score: 0.8968 - loss: 0.2495 - val_accuracy: 0.4400 - val_f1_score: 0.3913 - val_loss: 2.6255 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 24ms/step - accuracy: 0.8978 - f1_score: 0.8977 - loss: 0.2523 - val_accuracy: 0.5200 - val_f1_score: 0.4000 - val_loss: 2.2269 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 21ms/step - accuracy: 0.9215 - f1_score: 0.9222 - loss: 0.2031 - val_accuracy: 0.5200 - val_f1_score: 0.4545 - val_loss: 2.5383 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 22ms/step - accuracy: 0.9527 - f1_score: 0.9530 - loss: 0.1263 - val_accuracy: 0.5600 - val_f1_score: 0.4500 - val_loss: 3.0833 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 22ms/step - accuracy: 0.9742 - f1_score: 0.9738 - loss: 0.0785 - val_accuracy: 0.4800 - val_f1_score: 0.3810 - val_loss: 3.2365 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 22ms/step - accuracy: 0.9810 - f1_score: 0.9813 - loss: 0.0650 - val_accuracy: 0.4800 - val_f1_score: 0.3810 - val_loss: 3.0490 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 21ms/step - accuracy: 0.9873 - f1_score: 0.9878 - loss: 0.0453 - val_accuracy: 0.5600 - val_f1_score: 0.5217 - val_loss: 3.4308 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 34ms/step - accuracy: 0.9861 - f1_score: 0.9865 - loss: 0.0359 - val_accuracy: 0.5800 - val_f1_score: 0.5532 - val_loss: 3.3567 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 21ms/step - accuracy: 0.9945 - f1_score: 0.9947 - loss: 0.0228 - val_accuracy: 0.4800 - val_f1_score: 0.3500 - val_loss: 5.0371 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 21ms/step - accuracy: 0.9924 - f1_score: 0.9923 - loss: 0.0276 - val_accuracy: 0.5000 - val_f1_score: 0.3902 - val_loss: 4.2277 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 22ms/step - accuracy: 0.9941 - f1_score: 0.9942 - loss: 0.0161 - val_accuracy: 0.4600 - val_f1_score: 0.3721 - val_loss: 4.1624 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 21ms/step - accuracy: 0.9987 - f1_score: 0.9988 - loss: 0.0055 - val_accuracy: 0.5400 - val_f1_score: 0.4103 - val_loss: 4.8086 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 21ms/step - accuracy: 0.9992 - f1_score: 0.9992 - loss: 0.0033 - val_accuracy: 0.4600 - val_f1_score: 0.3077 - val_loss: 5.5246 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 21ms/step - accuracy: 0.9992 - f1_score: 0.9992 - loss: 0.0030 - val_accuracy: 0.5000 - val_f1_score: 0.4186 - val_loss: 5.3321 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 6.4643e-04 - val_accuracy: 0.5200 - val_f1_score: 0.4286 - val_loss: 5.3737 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 23ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 5.0590e-04 - val_accuracy: 0.5200 - val_f1_score: 0.4286 - val_loss: 5.3941 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 4.2942e-04 - val_accuracy: 0.5200 - val_f1_score: 0.4286 - val_loss: 5.4345 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 2.7668e-04 - val_accuracy: 0.5200 - val_f1_score: 0.4286 - val_loss: 5.4983 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 2.9761e-04 - val_accuracy: 0.5200 - val_f1_score: 0.4286 - val_loss: 5.5530 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 2.2683e-04 - val_accuracy: 0.5200 - val_f1_score: 0.4286 - val_loss: 5.6171 - learning_rate: 6.1987e-04\nEpoch 30/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 1.9412e-04 - val_accuracy: 0.5200 - val_f1_score: 0.4286 - val_loss: 5.6533 - learning_rate: 5.8827e-04\nEpoch 31/200\n37/37 - 1s - 21ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 1.9003e-04 - val_accuracy: 0.5200 - val_f1_score: 0.4286 - val_loss: 5.6807 - learning_rate: 5.5621e-04\nEpoch 32/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 1.8254e-04 - val_accuracy: 0.5200 - val_f1_score: 0.4286 - val_loss: 5.7081 - learning_rate: 5.2388e-04\nEpoch 33/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 1.8578e-04 - val_accuracy: 0.5200 - val_f1_score: 0.4286 - val_loss: 5.7335 - learning_rate: 4.9148e-04\nEpoch 34/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 1.5633e-04 - val_accuracy: 0.5200 - val_f1_score: 0.4286 - val_loss: 5.7442 - learning_rate: 4.5920e-04\nEpoch 35/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 1.9785e-04 - val_accuracy: 0.5000 - val_f1_score: 0.3902 - val_loss: 5.7699 - learning_rate: 4.2723e-04\nEpoch 36/200\n37/37 - 1s - 23ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 1.5023e-04 - val_accuracy: 0.5000 - val_f1_score: 0.3902 - val_loss: 5.7872 - learning_rate: 3.9575e-04\nEpoch 37/200\n37/37 - 1s - 21ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 1.4390e-04 - val_accuracy: 0.5000 - val_f1_score: 0.3902 - val_loss: 5.8073 - learning_rate: 3.6494e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 422ms/step\nmodel5 →  val F1 = 0.5532\n              precision    recall  f1-score   support\n\n        Left       0.64      0.57      0.60        28\n       Right       0.52      0.59      0.55        22\n\n    accuracy                           0.58        50\n   macro avg       0.58      0.58      0.58        50\nweighted avg       0.59      0.58      0.58        50\n\n\n>>> Training model6\nEpoch 1/200\n37/37 - 8s - 219ms/step - accuracy: 0.4996 - f1_score: 0.5796 - loss: 0.6953 - val_accuracy: 0.6000 - val_f1_score: 0.2308 - val_loss: 0.6915 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 39ms/step - accuracy: 0.5055 - f1_score: 0.5642 - loss: 0.6938 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7161 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 37ms/step - accuracy: 0.5160 - f1_score: 0.5990 - loss: 0.6933 - val_accuracy: 0.4800 - val_f1_score: 0.4583 - val_loss: 0.6990 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 38ms/step - accuracy: 0.5030 - f1_score: 0.6250 - loss: 0.6940 - val_accuracy: 0.4400 - val_f1_score: 0.6000 - val_loss: 0.6947 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 37ms/step - accuracy: 0.5072 - f1_score: 0.6730 - loss: 0.6929 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6993 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 37ms/step - accuracy: 0.4920 - f1_score: 0.5385 - loss: 0.6929 - val_accuracy: 0.4400 - val_f1_score: 0.5484 - val_loss: 0.6962 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 37ms/step - accuracy: 0.4802 - f1_score: 0.5630 - loss: 0.6939 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6936 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 37ms/step - accuracy: 0.5055 - f1_score: 0.6583 - loss: 0.6930 - val_accuracy: 0.4200 - val_f1_score: 0.4727 - val_loss: 0.6941 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 39ms/step - accuracy: 0.5131 - f1_score: 0.6764 - loss: 0.6925 - val_accuracy: 0.5200 - val_f1_score: 0.6364 - val_loss: 0.7010 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 37ms/step - accuracy: 0.4878 - f1_score: 0.6213 - loss: 0.6930 - val_accuracy: 0.4600 - val_f1_score: 0.3721 - val_loss: 0.6914 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 37ms/step - accuracy: 0.5156 - f1_score: 0.4408 - loss: 0.6926 - val_accuracy: 0.6000 - val_f1_score: 0.1667 - val_loss: 0.6895 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 37ms/step - accuracy: 0.5055 - f1_score: 0.2649 - loss: 0.6925 - val_accuracy: 0.5600 - val_f1_score: 0.1538 - val_loss: 0.6915 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 37ms/step - accuracy: 0.5004 - f1_score: 0.2963 - loss: 0.6926 - val_accuracy: 0.6000 - val_f1_score: 0.5000 - val_loss: 0.6914 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 37ms/step - accuracy: 0.4996 - f1_score: 0.4796 - loss: 0.6917 - val_accuracy: 0.4400 - val_f1_score: 0.4615 - val_loss: 0.6977 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 37ms/step - accuracy: 0.4996 - f1_score: 0.4800 - loss: 0.6918 - val_accuracy: 0.5800 - val_f1_score: 0.4000 - val_loss: 0.6965 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 37ms/step - accuracy: 0.5144 - f1_score: 0.3329 - loss: 0.6919 - val_accuracy: 0.5400 - val_f1_score: 0.5106 - val_loss: 0.6987 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 37ms/step - accuracy: 0.4962 - f1_score: 0.5176 - loss: 0.6922 - val_accuracy: 0.5000 - val_f1_score: 0.5283 - val_loss: 0.6921 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 37ms/step - accuracy: 0.5017 - f1_score: 0.2840 - loss: 0.6934 - val_accuracy: 0.5800 - val_f1_score: 0.0870 - val_loss: 0.6894 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 37ms/step - accuracy: 0.5068 - f1_score: 0.2673 - loss: 0.6918 - val_accuracy: 0.5400 - val_f1_score: 0.2581 - val_loss: 0.7013 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 37ms/step - accuracy: 0.5253 - f1_score: 0.3911 - loss: 0.6899 - val_accuracy: 0.4600 - val_f1_score: 0.2286 - val_loss: 0.7148 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 38ms/step - accuracy: 0.5089 - f1_score: 0.5103 - loss: 0.6919 - val_accuracy: 0.4600 - val_f1_score: 0.5714 - val_loss: 0.6944 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 37ms/step - accuracy: 0.5089 - f1_score: 0.6440 - loss: 0.6927 - val_accuracy: 0.4800 - val_f1_score: 0.5667 - val_loss: 0.7002 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 37ms/step - accuracy: 0.4970 - f1_score: 0.5133 - loss: 0.6919 - val_accuracy: 0.5200 - val_f1_score: 0.2941 - val_loss: 0.7023 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 37ms/step - accuracy: 0.5190 - f1_score: 0.4015 - loss: 0.6893 - val_accuracy: 0.5000 - val_f1_score: 0.2857 - val_loss: 0.7155 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 37ms/step - accuracy: 0.5228 - f1_score: 0.4235 - loss: 0.6918 - val_accuracy: 0.5200 - val_f1_score: 0.4783 - val_loss: 0.7140 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 37ms/step - accuracy: 0.5376 - f1_score: 0.6146 - loss: 0.6900 - val_accuracy: 0.5000 - val_f1_score: 0.4186 - val_loss: 0.7020 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 38ms/step - accuracy: 0.5144 - f1_score: 0.4961 - loss: 0.6891 - val_accuracy: 0.4800 - val_f1_score: 0.4583 - val_loss: 0.7077 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 37ms/step - accuracy: 0.5211 - f1_score: 0.5446 - loss: 0.6884 - val_accuracy: 0.4400 - val_f1_score: 0.3913 - val_loss: 0.7209 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 1s - 37ms/step - accuracy: 0.5241 - f1_score: 0.5596 - loss: 0.6903 - val_accuracy: 0.4400 - val_f1_score: 0.4615 - val_loss: 0.7372 - learning_rate: 6.1987e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 447ms/step\nmodel6 →  val F1 = 0.6364\n              precision    recall  f1-score   support\n\n        Left       0.83      0.18      0.29        28\n       Right       0.48      0.95      0.64        22\n\n    accuracy                           0.52        50\n   macro avg       0.66      0.57      0.47        50\nweighted avg       0.68      0.52      0.44        50\n\n\n=== Final best: model6 (F1=0.6364) ===\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\n\nfrom sklearn.metrics import f1_score, classification_report\nfrom mne.decoding import CSP\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, LearningRateScheduler\n\n# --- 1) Config ---\ndata_dir = './preprocessed'\noutput_dir = './models2'\nos.makedirs(output_dir, exist_ok=True)\n\n# --- 2) Load data ---\ntrain_npz = np.load(os.path.join(data_dir, 'train_MI.npz'))\nval_npz   = np.load(os.path.join(data_dir, 'validation_MI.npz'))\nX_train, y_train = train_npz['X'], train_npz['y']\nX_val,   y_val   = val_npz['X'],   val_npz['y']\n\ny_train_bin = (y_train == 'Right').astype(int)\ny_val_bin   = (y_val   == 'Right').astype(int)\n\n# --- 3) Select MI-related EEG channels: C3, CZ, C4, PZ (indices 2,3,4,5) ---\nmi_indices = [1, 2, 3, 4]\nX_train_mi = X_train[:, mi_indices, :]\nX_val_mi   = X_val[:,   mi_indices, :]\n\n# --- 4) CSP (n_components=4) ---\ncsp = CSP(n_components=4, log=False, norm_trace=False)\ncsp.fit(X_train_mi, y_train_bin)\nW = csp.filters_[:4]\ndef apply_csp(X): return np.stack([W.dot(ep) for ep in X], axis=0)\n\nXtr = apply_csp(X_train_mi).transpose(0, 2, 1).astype('float32')  # (n, T, 4)\nXvl = apply_csp(X_val_mi).transpose(0, 2, 1).astype('float32')\nXtr_spec = Xtr[..., np.newaxis]\nXvl_spec = Xvl[..., np.newaxis]\n\n# --- 5) One-hot encode labels ---\nytr_oh = keras.utils.to_categorical(y_train_bin, 2)\nyvl_oh = keras.utils.to_categorical(y_val_bin,   2)\n\n# --- 6) Data augmentation ---\ndef aug_gen(X, y, seed=0, batch_size=32):\n    n   = X.shape[0]\n    rng = np.random.RandomState(seed)\n    while True:\n        idx = rng.randint(0, n, batch_size)\n        bx, by = X[idx].copy(), y[idx]\n        bx += rng.normal(0, 0.005, bx.shape)\n        yield bx, by\n\ntrain_gen_1d = aug_gen(Xtr, ytr_oh, seed=0, batch_size=64)\ntrain_gen_2d = aug_gen(Xtr_spec, ytr_oh, seed=1, batch_size=64)\n\nsteps_1d = len(Xtr)      // 64\nsteps_2d = len(Xtr_spec) // 64\n\n# --- 7) Learning rate schedule ---\ndef cosine_lr(epoch, lr_max=5e-5, epochs=200):\n    return lr_max * (1 + np.cos(np.pi * epoch / epochs)) / 2\n\n# --- 8) F1 metric ---\nclass F1Score(tf.keras.metrics.Metric):\n    def __init__(self, name=\"f1_score\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        preds  = tf.argmax(y_pred, axis=1)\n        labels = tf.argmax(y_true, axis=1)\n        preds  = tf.cast(preds, tf.int32)\n        labels = tf.cast(labels, tf.int32)\n        tp = tf.reduce_sum(tf.cast(tf.logical_and(preds==1, labels==1), tf.float32))\n        fp = tf.reduce_sum(tf.cast(tf.logical_and(preds==1, labels==0), tf.float32))\n        fn = tf.reduce_sum(tf.cast(tf.logical_and(preds==0, labels==1), tf.float32))\n        self.tp.assign_add(tp)\n        self.fp.assign_add(fp)\n        self.fn.assign_add(fn)\n\n    def result(self):\n        precision = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n        recall    = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n        return 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n\n    def reset_states(self):\n        self.tp.assign(0.)\n        self.fp.assign(0.)\n        self.fn.assign(0.)\n\n# --- 9) Callbacks ---\ndef get_callbacks(name):\n    return [\n        EarlyStopping(\"val_f1_score\", mode=\"max\", patience=20, restore_best_weights=True),\n        ModelCheckpoint(f\"{output_dir}/best_{name}.h5\", \"val_f1_score\", mode=\"max\", save_best_only=True),\n        CSVLogger(f\"{output_dir}/log_{name}.csv\"),\n        LearningRateScheduler(cosine_lr)\n    ]\n\n# --- 10) Model builders ---\n\ndef build_modelA(input_shape):\n    m = keras.Sequential([\n        layers.Input(input_shape),\n        layers.Conv1D(32, 5, activation=\"relu\", padding=\"same\"),\n        layers.BatchNormalization(), layers.MaxPool1D(2),\n        layers.Conv1D(64, 5, activation=\"relu\", padding=\"same\"),\n        layers.BatchNormalization(), layers.MaxPool1D(2),\n        layers.Conv1D(128,5,activation=\"relu\",padding=\"same\"),\n        layers.BatchNormalization(),\n        layers.GlobalAveragePooling1D(),\n        layers.Dense(64, activation=\"relu\", \n                     kernel_regularizer=regularizers.l2(1e-4)),\n        layers.Dropout(0.7),\n        layers.Dense(2, activation=\"softmax\"),\n    ])\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_modelB(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for f in [16,32,64,128,256]:\n        x = layers.Conv1D(f,3,activation=\"relu\",padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.MaxPool1D(2)(x)\n    x = layers.Flatten()(x)\n    for u in [128,64,32]:\n        x = layers.Dense(u, activation=\"relu\",\n                         kernel_regularizer=regularizers.l2(1e-4))(x)\n        x = layers.Dropout(0.5)(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp, out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model1(input_shape):\n    inp = layers.Input(input_shape+(1,))\n    x = layers.Concatenate()([inp, inp, inp])\n    x = layers.Resizing(32,32)(x)\n    base = keras.applications.ResNet50(\n        include_top=False, weights=\"imagenet\",\n        input_shape=(32,32,3), pooling=\"avg\"\n    )\n    base.trainable = False\n    x = base(x)\n    x = layers.Reshape((1, x.shape[-1]))(x)\n    for _ in range(7):\n        x = layers.Conv1D(64,3,activation=\"relu\",padding=\"same\")(x)\n    x = layers.MultiHeadAttention(num_heads=4,key_dim=32)(x,x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(64,activation=\"relu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model2(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for _ in range(3):\n        x = layers.Conv1D(32,3,activation=\"elu\",padding=\"same\")(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(64,activation=\"elu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model3(input_shape):\n    inp = layers.Input(input_shape)  # (T, F, 1)\n    x = inp\n    for _ in range(5):\n        x = layers.Conv2D(32,(3,3),activation=\"relu\",padding=\"same\")(x)\n    x = layers.Flatten()(x)\n    for u in [128,64,32]:\n        x = layers.Dense(u, activation=\"relu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model4(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for _ in range(3):\n        x = layers.Conv1D(64,3,activation=\"relu\",padding=\"same\")(x)\n    x = layers.LSTM(128)(x)\n    for _ in range(4):\n        x = layers.Dense(64, activation=\"relu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model5(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for _ in range(7):\n        x = layers.Conv1D(64,3,activation=\"elu\",padding=\"same\")(x)\n    x = layers.Flatten()(x)\n    for _ in range(3):\n        x = layers.Dense(64, activation=\"elu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model6(input_shape):\n    C3, C4 = 0,2\n    eeg_in = layers.Input(input_shape)\n    c3 = layers.Lambda(lambda x: x[:,:,C3:C3+1])(eeg_in)\n    c4 = layers.Lambda(lambda x: x[:,:,C4:C4+1])(eeg_in)\n    def branch():\n        return models.Sequential([\n            layers.Conv1D(16,250,activation=\"relu\",padding=\"same\"),\n            layers.MaxPool1D(3),\n            layers.Conv1D(32,50,activation=\"relu\",padding=\"same\"),\n            layers.GlobalAveragePooling1D()\n        ])\n    b3, b4 = branch()(c3), branch()(c4)\n    x = layers.Concatenate()([b3,b4])\n    for _ in range(4):\n        x = layers.Dense(64,activation=\"relu\")(x)\n    out = layers.Dense(2,activation=\"softmax\")(x)\n    m = models.Model(eeg_in,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n# --- 11) Train + Evaluate ---\nbuilders = {\n    'oldA': build_modelA,\n    'oldB': build_modelB,\n    'model1': build_model1,\n    'model2': build_model2,\n    'model3': build_model3,\n    'model4': build_model4,\n    'model5': build_model5,\n    'model6': build_model6,\n}\n\nresults = {}\nshape_1d = Xtr.shape[1:]      # (T, 4)\nshape_2d = Xtr_spec.shape[1:] # (T, 4, 1)\n\nfor name, build_fn in builders.items():\n    print(f\"\\n>>> Training {name}\")\n    if name == 'model3':\n        model = build_fn(shape_2d)\n        gen   = train_gen_2d\n        steps = steps_2d\n        val_x = Xvl_spec\n    else:\n        model = build_fn(shape_1d)\n        gen   = train_gen_1d\n        steps = steps_1d\n        val_x = Xvl\n\n    history = model.fit(\n        gen, steps_per_epoch=steps,\n        validation_data=(val_x, yvl_oh),\n        epochs=200,\n        callbacks=get_callbacks(name),\n        verbose=2\n    )\n\n    preds = np.argmax(model.predict(val_x), axis=1)\n    f1 = f1_score(y_val_bin, preds)\n    print(f\"{name} → F1 = {f1:.4f}\")\n    print(classification_report(y_val_bin, preds, target_names=['Left','Right']))\n    results[name] = (f1, model)\n\n# --- 12) Save best ---\nbest_name, (best_f1, best_model) = max(results.items(), key=lambda kv: kv[1][0])\nprint(f\"\\n=== Final best: {best_name} (F1={best_f1:.4f}) ===\")\nbest_model.save(os.path.join(output_dir, 'best_final.h5'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T11:21:43.096572Z","iopub.execute_input":"2025-06-29T11:21:43.097124Z","iopub.status.idle":"2025-06-29T11:29:24.340530Z","shell.execute_reply.started":"2025-06-29T11:21:43.097103Z","shell.execute_reply":"2025-06-29T11:29:24.339919Z"}},"outputs":[{"name":"stdout","text":"Computing rank from data with rank=None\n    Using tolerance 3.2e+03 (2.2e-16 eps * 4 dim * 3.6e+18  max singular value)\n    Estimated rank (data): 4\n    data: rank 4 computed from 4 data channels with 0 projectors\nReducing data rank from 4 -> 4\nEstimating class=0 covariance using EMPIRICAL\nDone.\nEstimating class=1 covariance using EMPIRICAL\nDone.\n\n>>> Training oldA\nEpoch 1/200\n37/37 - 9s - 249ms/step - accuracy: 0.4907 - f1_score: 0.4911 - loss: 0.9158 - val_accuracy: 0.5800 - val_f1_score: 0.0870 - val_loss: 0.6839 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 28ms/step - accuracy: 0.5144 - f1_score: 0.5106 - loss: 0.7812 - val_accuracy: 0.5400 - val_f1_score: 0.4390 - val_loss: 0.7140 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 27ms/step - accuracy: 0.4958 - f1_score: 0.4684 - loss: 0.7661 - val_accuracy: 0.6200 - val_f1_score: 0.3448 - val_loss: 0.6996 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 27ms/step - accuracy: 0.5106 - f1_score: 0.5225 - loss: 0.7422 - val_accuracy: 0.5400 - val_f1_score: 0.2069 - val_loss: 0.7000 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 27ms/step - accuracy: 0.5169 - f1_score: 0.5442 - loss: 0.7213 - val_accuracy: 0.5400 - val_f1_score: 0.0000e+00 - val_loss: 0.6842 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 27ms/step - accuracy: 0.5046 - f1_score: 0.4780 - loss: 0.7172 - val_accuracy: 0.5600 - val_f1_score: 0.3125 - val_loss: 0.6889 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 27ms/step - accuracy: 0.5397 - f1_score: 0.4972 - loss: 0.7030 - val_accuracy: 0.5000 - val_f1_score: 0.3902 - val_loss: 0.7021 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 27ms/step - accuracy: 0.5000 - f1_score: 0.4966 - loss: 0.7101 - val_accuracy: 0.5400 - val_f1_score: 0.1481 - val_loss: 0.7030 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 27ms/step - accuracy: 0.5072 - f1_score: 0.4998 - loss: 0.7109 - val_accuracy: 0.5600 - val_f1_score: 0.3889 - val_loss: 0.7002 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 28ms/step - accuracy: 0.5144 - f1_score: 0.5378 - loss: 0.7074 - val_accuracy: 0.5000 - val_f1_score: 0.4681 - val_loss: 0.6896 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 27ms/step - accuracy: 0.5203 - f1_score: 0.4869 - loss: 0.7033 - val_accuracy: 0.5600 - val_f1_score: 0.4500 - val_loss: 0.7148 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 27ms/step - accuracy: 0.5338 - f1_score: 0.5098 - loss: 0.6994 - val_accuracy: 0.4400 - val_f1_score: 0.3636 - val_loss: 0.7007 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 27ms/step - accuracy: 0.5173 - f1_score: 0.5441 - loss: 0.7002 - val_accuracy: 0.5800 - val_f1_score: 0.3636 - val_loss: 0.7025 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 28ms/step - accuracy: 0.5224 - f1_score: 0.5734 - loss: 0.6997 - val_accuracy: 0.5600 - val_f1_score: 0.6207 - val_loss: 0.7029 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 27ms/step - accuracy: 0.5068 - f1_score: 0.4877 - loss: 0.7018 - val_accuracy: 0.6200 - val_f1_score: 0.4865 - val_loss: 0.7080 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 27ms/step - accuracy: 0.5452 - f1_score: 0.5566 - loss: 0.6981 - val_accuracy: 0.5600 - val_f1_score: 0.5000 - val_loss: 0.7248 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 27ms/step - accuracy: 0.5198 - f1_score: 0.4624 - loss: 0.7020 - val_accuracy: 0.4600 - val_f1_score: 0.3721 - val_loss: 0.7049 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 27ms/step - accuracy: 0.5338 - f1_score: 0.4327 - loss: 0.6977 - val_accuracy: 0.6000 - val_f1_score: 0.2857 - val_loss: 0.6971 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 27ms/step - accuracy: 0.5059 - f1_score: 0.4662 - loss: 0.7010 - val_accuracy: 0.4600 - val_f1_score: 0.4706 - val_loss: 0.7159 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 27ms/step - accuracy: 0.5456 - f1_score: 0.5342 - loss: 0.6951 - val_accuracy: 0.4800 - val_f1_score: 0.4091 - val_loss: 0.7234 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 27ms/step - accuracy: 0.5317 - f1_score: 0.5259 - loss: 0.7013 - val_accuracy: 0.5600 - val_f1_score: 0.0833 - val_loss: 0.7132 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 27ms/step - accuracy: 0.5460 - f1_score: 0.4584 - loss: 0.6973 - val_accuracy: 0.5600 - val_f1_score: 0.2667 - val_loss: 0.7004 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 27ms/step - accuracy: 0.5245 - f1_score: 0.5159 - loss: 0.7000 - val_accuracy: 0.5600 - val_f1_score: 0.5600 - val_loss: 0.7093 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 27ms/step - accuracy: 0.5245 - f1_score: 0.5147 - loss: 0.7009 - val_accuracy: 0.5600 - val_f1_score: 0.4211 - val_loss: 0.7066 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 27ms/step - accuracy: 0.5236 - f1_score: 0.5614 - loss: 0.6960 - val_accuracy: 0.4600 - val_f1_score: 0.4490 - val_loss: 0.7146 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 27ms/step - accuracy: 0.5274 - f1_score: 0.5900 - loss: 0.6940 - val_accuracy: 0.4800 - val_f1_score: 0.4583 - val_loss: 0.7002 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 27ms/step - accuracy: 0.5473 - f1_score: 0.5930 - loss: 0.6959 - val_accuracy: 0.5600 - val_f1_score: 0.4500 - val_loss: 0.7165 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 27ms/step - accuracy: 0.5380 - f1_score: 0.5909 - loss: 0.6983 - val_accuracy: 0.4800 - val_f1_score: 0.4091 - val_loss: 0.7113 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 1s - 27ms/step - accuracy: 0.5486 - f1_score: 0.5995 - loss: 0.6928 - val_accuracy: 0.5600 - val_f1_score: 0.5600 - val_loss: 0.7050 - learning_rate: 6.1987e-04\nEpoch 30/200\n37/37 - 1s - 27ms/step - accuracy: 0.5359 - f1_score: 0.5943 - loss: 0.6923 - val_accuracy: 0.5000 - val_f1_score: 0.4444 - val_loss: 0.7232 - learning_rate: 5.8827e-04\nEpoch 31/200\n37/37 - 1s - 27ms/step - accuracy: 0.5321 - f1_score: 0.5532 - loss: 0.6958 - val_accuracy: 0.5600 - val_f1_score: 0.4500 - val_loss: 0.7106 - learning_rate: 5.5621e-04\nEpoch 32/200\n37/37 - 1s - 27ms/step - accuracy: 0.5528 - f1_score: 0.6014 - loss: 0.6937 - val_accuracy: 0.4800 - val_f1_score: 0.4348 - val_loss: 0.7359 - learning_rate: 5.2388e-04\nEpoch 33/200\n37/37 - 1s - 27ms/step - accuracy: 0.5583 - f1_score: 0.6348 - loss: 0.6927 - val_accuracy: 0.5000 - val_f1_score: 0.5614 - val_loss: 0.7261 - learning_rate: 4.9148e-04\nEpoch 34/200\n37/37 - 1s - 27ms/step - accuracy: 0.5389 - f1_score: 0.5925 - loss: 0.6911 - val_accuracy: 0.5600 - val_f1_score: 0.5769 - val_loss: 0.7440 - learning_rate: 4.5920e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380ms/step\noldA → F1 = 0.6207\n              precision    recall  f1-score   support\n\n        Left       0.71      0.36      0.48        28\n       Right       0.50      0.82      0.62        22\n\n    accuracy                           0.56        50\n   macro avg       0.61      0.59      0.55        50\nweighted avg       0.62      0.56      0.54        50\n\n\n>>> Training oldB\nEpoch 1/200\n37/37 - 14s - 367ms/step - accuracy: 0.5034 - f1_score: 0.4630 - loss: 2.5522 - val_accuracy: 0.4800 - val_f1_score: 0.4800 - val_loss: 0.7516 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 35ms/step - accuracy: 0.4903 - f1_score: 0.4914 - loss: 1.6504 - val_accuracy: 0.4800 - val_f1_score: 0.6061 - val_loss: 0.7587 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 31ms/step - accuracy: 0.5228 - f1_score: 0.5280 - loss: 1.2404 - val_accuracy: 0.4000 - val_f1_score: 0.5714 - val_loss: 0.7552 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 35ms/step - accuracy: 0.4920 - f1_score: 0.5171 - loss: 1.0459 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7506 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 36ms/step - accuracy: 0.5253 - f1_score: 0.5557 - loss: 0.9339 - val_accuracy: 0.4600 - val_f1_score: 0.6197 - val_loss: 0.7480 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 31ms/step - accuracy: 0.5097 - f1_score: 0.5624 - loss: 0.8601 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7534 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 31ms/step - accuracy: 0.4941 - f1_score: 0.5479 - loss: 0.8220 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7525 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 31ms/step - accuracy: 0.4894 - f1_score: 0.5436 - loss: 0.8439 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7518 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 31ms/step - accuracy: 0.5220 - f1_score: 0.5983 - loss: 0.7970 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7508 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 31ms/step - accuracy: 0.4945 - f1_score: 0.5705 - loss: 0.7927 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7531 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 31ms/step - accuracy: 0.5063 - f1_score: 0.6123 - loss: 0.7953 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7514 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 31ms/step - accuracy: 0.4958 - f1_score: 0.6062 - loss: 0.7758 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7557 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 31ms/step - accuracy: 0.5013 - f1_score: 0.5923 - loss: 0.7797 - val_accuracy: 0.4400 - val_f1_score: 0.5172 - val_loss: 0.7466 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 31ms/step - accuracy: 0.5156 - f1_score: 0.6097 - loss: 0.7660 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7516 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 31ms/step - accuracy: 0.5106 - f1_score: 0.5868 - loss: 0.7691 - val_accuracy: 0.4600 - val_f1_score: 0.4706 - val_loss: 0.7504 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 31ms/step - accuracy: 0.5046 - f1_score: 0.5809 - loss: 0.7698 - val_accuracy: 0.4000 - val_f1_score: 0.3750 - val_loss: 0.7483 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 31ms/step - accuracy: 0.5084 - f1_score: 0.5864 - loss: 0.7647 - val_accuracy: 0.5000 - val_f1_score: 0.6032 - val_loss: 0.7484 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 31ms/step - accuracy: 0.4827 - f1_score: 0.5340 - loss: 0.7790 - val_accuracy: 0.4400 - val_f1_score: 0.5484 - val_loss: 0.7517 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 31ms/step - accuracy: 0.5042 - f1_score: 0.5816 - loss: 0.7632 - val_accuracy: 0.5200 - val_f1_score: 0.3684 - val_loss: 0.7502 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 32ms/step - accuracy: 0.5224 - f1_score: 0.5611 - loss: 0.7580 - val_accuracy: 0.4800 - val_f1_score: 0.4583 - val_loss: 0.7486 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 31ms/step - accuracy: 0.4962 - f1_score: 0.5203 - loss: 0.7577 - val_accuracy: 0.5400 - val_f1_score: 0.0000e+00 - val_loss: 0.7465 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 31ms/step - accuracy: 0.5034 - f1_score: 0.5721 - loss: 0.7522 - val_accuracy: 0.4800 - val_f1_score: 0.4800 - val_loss: 0.7452 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 31ms/step - accuracy: 0.5080 - f1_score: 0.5595 - loss: 0.7520 - val_accuracy: 0.6000 - val_f1_score: 0.3750 - val_loss: 0.7416 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 31ms/step - accuracy: 0.5017 - f1_score: 0.5561 - loss: 0.7540 - val_accuracy: 0.4600 - val_f1_score: 0.6197 - val_loss: 0.7460 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 31ms/step - accuracy: 0.5114 - f1_score: 0.5962 - loss: 0.7489 - val_accuracy: 0.5400 - val_f1_score: 0.0000e+00 - val_loss: 0.7453 - learning_rate: 7.3832e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607ms/step\noldB → F1 = 0.6197\n              precision    recall  f1-score   support\n\n        Left       1.00      0.04      0.07        28\n       Right       0.45      1.00      0.62        22\n\n    accuracy                           0.46        50\n   macro avg       0.72      0.52      0.34        50\nweighted avg       0.76      0.46      0.31        50\n\n\n>>> Training model1\nEpoch 1/200\n37/37 - 26s - 698ms/step - accuracy: 0.4966 - f1_score: 0.5542 - loss: 0.6937 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6948 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 23ms/step - accuracy: 0.4768 - f1_score: 0.5735 - loss: 0.6936 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6933 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 21ms/step - accuracy: 0.5038 - f1_score: 0.3890 - loss: 0.6932 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6913 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 21ms/step - accuracy: 0.4916 - f1_score: 0.5037 - loss: 0.6934 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6953 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 22ms/step - accuracy: 0.4996 - f1_score: 0.6075 - loss: 0.6936 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6923 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 22ms/step - accuracy: 0.4987 - f1_score: 0.2175 - loss: 0.6935 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6939 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 21ms/step - accuracy: 0.5055 - f1_score: 0.2167 - loss: 0.6930 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6939 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 22ms/step - accuracy: 0.4894 - f1_score: 0.2859 - loss: 0.6933 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6934 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 22ms/step - accuracy: 0.4932 - f1_score: 0.1643 - loss: 0.6932 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6925 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 21ms/step - accuracy: 0.4920 - f1_score: 0.5258 - loss: 0.6936 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6946 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 21ms/step - accuracy: 0.4954 - f1_score: 0.5334 - loss: 0.6932 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6947 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 22ms/step - accuracy: 0.5152 - f1_score: 0.6800 - loss: 0.6928 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6974 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 22ms/step - accuracy: 0.5165 - f1_score: 0.6811 - loss: 0.6927 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6981 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 21ms/step - accuracy: 0.4899 - f1_score: 0.6525 - loss: 0.6939 - val_accuracy: 0.4600 - val_f1_score: 0.2703 - val_loss: 0.6931 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 21ms/step - accuracy: 0.5190 - f1_score: 0.3906 - loss: 0.6931 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6942 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 21ms/step - accuracy: 0.5236 - f1_score: 0.6874 - loss: 0.6926 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6970 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 21ms/step - accuracy: 0.5025 - f1_score: 0.6689 - loss: 0.6934 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6945 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 21ms/step - accuracy: 0.5215 - f1_score: 0.6855 - loss: 0.6944 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6984 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 21ms/step - accuracy: 0.5008 - f1_score: 0.6674 - loss: 0.6899 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.8011 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 21ms/step - accuracy: 0.5063 - f1_score: 0.3247 - loss: 0.6900 - val_accuracy: 0.5800 - val_f1_score: 0.2222 - val_loss: 0.7185 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 21ms/step - accuracy: 0.4937 - f1_score: 0.4869 - loss: 0.6889 - val_accuracy: 0.5000 - val_f1_score: 0.3902 - val_loss: 0.7652 - learning_rate: 8.3736e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5s/step\nmodel1 → F1 = 0.6111\n              precision    recall  f1-score   support\n\n        Left       0.00      0.00      0.00        28\n       Right       0.44      1.00      0.61        22\n\n    accuracy                           0.44        50\n   macro avg       0.22      0.50      0.31        50\nweighted avg       0.19      0.44      0.27        50\n\n\n>>> Training model2\nEpoch 1/200\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"37/37 - 4s - 118ms/step - accuracy: 0.5236 - f1_score: 0.4910 - loss: 0.6943 - val_accuracy: 0.4400 - val_f1_score: 0.5625 - val_loss: 0.7118 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 21ms/step - accuracy: 0.4928 - f1_score: 0.5043 - loss: 0.6960 - val_accuracy: 0.4400 - val_f1_score: 0.5625 - val_loss: 0.7075 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 21ms/step - accuracy: 0.5038 - f1_score: 0.6379 - loss: 0.6949 - val_accuracy: 0.5200 - val_f1_score: 0.5714 - val_loss: 0.7034 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 20ms/step - accuracy: 0.5194 - f1_score: 0.6535 - loss: 0.6915 - val_accuracy: 0.5000 - val_f1_score: 0.5614 - val_loss: 0.6995 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 21ms/step - accuracy: 0.5089 - f1_score: 0.5583 - loss: 0.6950 - val_accuracy: 0.5000 - val_f1_score: 0.5098 - val_loss: 0.6922 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 21ms/step - accuracy: 0.5152 - f1_score: 0.5341 - loss: 0.6929 - val_accuracy: 0.5600 - val_f1_score: 0.5000 - val_loss: 0.6925 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 20ms/step - accuracy: 0.5013 - f1_score: 0.3916 - loss: 0.6934 - val_accuracy: 0.5200 - val_f1_score: 0.5714 - val_loss: 0.7095 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 21ms/step - accuracy: 0.5021 - f1_score: 0.5950 - loss: 0.6957 - val_accuracy: 0.5200 - val_f1_score: 0.5714 - val_loss: 0.6997 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 21ms/step - accuracy: 0.5304 - f1_score: 0.6527 - loss: 0.6917 - val_accuracy: 0.4600 - val_f1_score: 0.5970 - val_loss: 0.7004 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 20ms/step - accuracy: 0.5055 - f1_score: 0.5725 - loss: 0.6918 - val_accuracy: 0.5200 - val_f1_score: 0.5000 - val_loss: 0.6966 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 20ms/step - accuracy: 0.5051 - f1_score: 0.5614 - loss: 0.6901 - val_accuracy: 0.4200 - val_f1_score: 0.5246 - val_loss: 0.7109 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 21ms/step - accuracy: 0.5030 - f1_score: 0.4124 - loss: 0.6953 - val_accuracy: 0.6200 - val_f1_score: 0.4242 - val_loss: 0.6884 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 20ms/step - accuracy: 0.5021 - f1_score: 0.4989 - loss: 0.6944 - val_accuracy: 0.4200 - val_f1_score: 0.5915 - val_loss: 0.6966 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 20ms/step - accuracy: 0.5089 - f1_score: 0.6628 - loss: 0.6927 - val_accuracy: 0.4400 - val_f1_score: 0.5758 - val_loss: 0.6974 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 21ms/step - accuracy: 0.5190 - f1_score: 0.5570 - loss: 0.6921 - val_accuracy: 0.5800 - val_f1_score: 0.4615 - val_loss: 0.6921 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 21ms/step - accuracy: 0.5063 - f1_score: 0.5103 - loss: 0.6918 - val_accuracy: 0.4600 - val_f1_score: 0.5263 - val_loss: 0.6957 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 21ms/step - accuracy: 0.4987 - f1_score: 0.4912 - loss: 0.6931 - val_accuracy: 0.5400 - val_f1_score: 0.3030 - val_loss: 0.6985 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 21ms/step - accuracy: 0.5186 - f1_score: 0.5501 - loss: 0.6906 - val_accuracy: 0.5000 - val_f1_score: 0.3902 - val_loss: 0.7084 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 20ms/step - accuracy: 0.5169 - f1_score: 0.4987 - loss: 0.6890 - val_accuracy: 0.4000 - val_f1_score: 0.5000 - val_loss: 0.7311 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 21ms/step - accuracy: 0.5076 - f1_score: 0.6454 - loss: 0.6942 - val_accuracy: 0.4200 - val_f1_score: 0.4912 - val_loss: 0.7003 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 20ms/step - accuracy: 0.5030 - f1_score: 0.4724 - loss: 0.6933 - val_accuracy: 0.5000 - val_f1_score: 0.3590 - val_loss: 0.7047 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 20ms/step - accuracy: 0.5266 - f1_score: 0.3323 - loss: 0.6911 - val_accuracy: 0.5600 - val_f1_score: 0.4500 - val_loss: 0.6926 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 20ms/step - accuracy: 0.5084 - f1_score: 0.4055 - loss: 0.6902 - val_accuracy: 0.5600 - val_f1_score: 0.4211 - val_loss: 0.7009 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 20ms/step - accuracy: 0.5241 - f1_score: 0.3571 - loss: 0.6915 - val_accuracy: 0.5400 - val_f1_score: 0.3429 - val_loss: 0.7022 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 21ms/step - accuracy: 0.5329 - f1_score: 0.4252 - loss: 0.6898 - val_accuracy: 0.5600 - val_f1_score: 0.4211 - val_loss: 0.7008 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 20ms/step - accuracy: 0.5165 - f1_score: 0.4214 - loss: 0.6919 - val_accuracy: 0.5800 - val_f1_score: 0.4324 - val_loss: 0.7016 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 20ms/step - accuracy: 0.5232 - f1_score: 0.3162 - loss: 0.6891 - val_accuracy: 0.5400 - val_f1_score: 0.4103 - val_loss: 0.7030 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 20ms/step - accuracy: 0.5270 - f1_score: 0.4621 - loss: 0.6893 - val_accuracy: 0.5600 - val_f1_score: 0.2667 - val_loss: 0.7178 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 1s - 20ms/step - accuracy: 0.5051 - f1_score: 0.5496 - loss: 0.6939 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 0.7100 - learning_rate: 6.1987e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 256ms/step\nmodel2 → F1 = 0.5970\n              precision    recall  f1-score   support\n\n        Left       0.60      0.11      0.18        28\n       Right       0.44      0.91      0.60        22\n\n    accuracy                           0.46        50\n   macro avg       0.52      0.51      0.39        50\nweighted avg       0.53      0.46      0.36        50\n\n\n>>> Training model3\nEpoch 1/200\n37/37 - 9s - 252ms/step - accuracy: 0.5097 - f1_score: 0.3126 - loss: 0.7010 - val_accuracy: 0.4200 - val_f1_score: 0.5538 - val_loss: 0.7117 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 38ms/step - accuracy: 0.5224 - f1_score: 0.6331 - loss: 0.6889 - val_accuracy: 0.3600 - val_f1_score: 0.4667 - val_loss: 0.7162 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 38ms/step - accuracy: 0.5469 - f1_score: 0.6687 - loss: 0.6841 - val_accuracy: 0.5200 - val_f1_score: 0.5000 - val_loss: 0.7743 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 3s - 86ms/step - accuracy: 0.6144 - f1_score: 0.6025 - loss: 0.6421 - val_accuracy: 0.5600 - val_f1_score: 0.5600 - val_loss: 1.6650 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 3s - 87ms/step - accuracy: 0.7031 - f1_score: 0.7227 - loss: 0.5511 - val_accuracy: 0.4800 - val_f1_score: 0.5667 - val_loss: 1.2056 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 38ms/step - accuracy: 0.8024 - f1_score: 0.8058 - loss: 0.4367 - val_accuracy: 0.5400 - val_f1_score: 0.5660 - val_loss: 1.1332 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 3s - 85ms/step - accuracy: 0.8881 - f1_score: 0.8887 - loss: 0.2701 - val_accuracy: 0.6000 - val_f1_score: 0.6154 - val_loss: 1.7584 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 38ms/step - accuracy: 0.9333 - f1_score: 0.9338 - loss: 0.1656 - val_accuracy: 0.5400 - val_f1_score: 0.5965 - val_loss: 2.2584 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 38ms/step - accuracy: 0.9493 - f1_score: 0.9499 - loss: 0.1373 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 1.8233 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 38ms/step - accuracy: 0.9658 - f1_score: 0.9657 - loss: 0.0956 - val_accuracy: 0.5400 - val_f1_score: 0.5660 - val_loss: 2.6158 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 38ms/step - accuracy: 0.9789 - f1_score: 0.9780 - loss: 0.0574 - val_accuracy: 0.5400 - val_f1_score: 0.5306 - val_loss: 3.0980 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 38ms/step - accuracy: 0.9721 - f1_score: 0.9719 - loss: 0.0891 - val_accuracy: 0.5000 - val_f1_score: 0.5614 - val_loss: 4.5463 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 38ms/step - accuracy: 0.9831 - f1_score: 0.9832 - loss: 0.0427 - val_accuracy: 0.5000 - val_f1_score: 0.5098 - val_loss: 3.7835 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 38ms/step - accuracy: 0.9916 - f1_score: 0.9914 - loss: 0.0287 - val_accuracy: 0.4800 - val_f1_score: 0.5517 - val_loss: 6.4542 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 38ms/step - accuracy: 0.9873 - f1_score: 0.9877 - loss: 0.0301 - val_accuracy: 0.5400 - val_f1_score: 0.5660 - val_loss: 5.4236 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 38ms/step - accuracy: 0.9780 - f1_score: 0.9783 - loss: 0.0382 - val_accuracy: 0.5400 - val_f1_score: 0.5660 - val_loss: 6.6448 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 38ms/step - accuracy: 0.9818 - f1_score: 0.9819 - loss: 0.0281 - val_accuracy: 0.5800 - val_f1_score: 0.6038 - val_loss: 8.7691 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 38ms/step - accuracy: 0.9882 - f1_score: 0.9885 - loss: 0.0247 - val_accuracy: 0.5800 - val_f1_score: 0.5532 - val_loss: 7.0957 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 3s - 87ms/step - accuracy: 0.9806 - f1_score: 0.9808 - loss: 0.0593 - val_accuracy: 0.6000 - val_f1_score: 0.6296 - val_loss: 8.0351 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 38ms/step - accuracy: 0.9848 - f1_score: 0.9850 - loss: 0.0317 - val_accuracy: 0.5000 - val_f1_score: 0.5283 - val_loss: 7.6793 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 38ms/step - accuracy: 0.9937 - f1_score: 0.9938 - loss: 0.0171 - val_accuracy: 0.6000 - val_f1_score: 0.5833 - val_loss: 6.3953 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 38ms/step - accuracy: 0.9945 - f1_score: 0.9946 - loss: 0.0237 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 4.5413 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 38ms/step - accuracy: 0.9962 - f1_score: 0.9962 - loss: 0.0113 - val_accuracy: 0.4800 - val_f1_score: 0.5357 - val_loss: 6.1851 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 38ms/step - accuracy: 0.9979 - f1_score: 0.9979 - loss: 0.0094 - val_accuracy: 0.6000 - val_f1_score: 0.5833 - val_loss: 4.7372 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 38ms/step - accuracy: 0.9996 - f1_score: 0.9996 - loss: 0.0024 - val_accuracy: 0.5400 - val_f1_score: 0.5490 - val_loss: 5.1136 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 6.1361e-04 - val_accuracy: 0.5400 - val_f1_score: 0.5490 - val_loss: 5.4390 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 4.1549e-04 - val_accuracy: 0.5800 - val_f1_score: 0.5714 - val_loss: 5.4660 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 3.4635e-04 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 5.5885 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 2.9000e-04 - val_accuracy: 0.5600 - val_f1_score: 0.5600 - val_loss: 5.6972 - learning_rate: 6.1987e-04\nEpoch 30/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 1.7523e-04 - val_accuracy: 0.5400 - val_f1_score: 0.5306 - val_loss: 5.8917 - learning_rate: 5.8827e-04\nEpoch 31/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 1.2972e-04 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 5.8585 - learning_rate: 5.5621e-04\nEpoch 32/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 8.3752e-05 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 6.0147 - learning_rate: 5.2388e-04\nEpoch 33/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 5.7103e-05 - val_accuracy: 0.5400 - val_f1_score: 0.5306 - val_loss: 6.1492 - learning_rate: 4.9148e-04\nEpoch 34/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 4.0818e-05 - val_accuracy: 0.5400 - val_f1_score: 0.5306 - val_loss: 6.1717 - learning_rate: 4.5920e-04\nEpoch 35/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 5.5035e-05 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 6.1935 - learning_rate: 4.2723e-04\nEpoch 36/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 4.7336e-05 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 6.2162 - learning_rate: 3.9575e-04\nEpoch 37/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 3.6460e-05 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 6.3273 - learning_rate: 3.6494e-04\nEpoch 38/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 3.3696e-05 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 6.3756 - learning_rate: 3.3498e-04\nEpoch 39/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 2.7423e-05 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 6.3683 - learning_rate: 3.0602e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300ms/step\nmodel3 → F1 = 0.6296\n              precision    recall  f1-score   support\n\n        Left       0.72      0.46      0.57        28\n       Right       0.53      0.77      0.63        22\n\n    accuracy                           0.60        50\n   macro avg       0.63      0.62      0.60        50\nweighted avg       0.64      0.60      0.59        50\n\n\n>>> Training model4\nEpoch 1/200\n37/37 - 9s - 237ms/step - accuracy: 0.5106 - f1_score: 0.5731 - loss: 0.6921 - val_accuracy: 0.5600 - val_f1_score: 0.2143 - val_loss: 0.6948 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 4s - 119ms/step - accuracy: 0.5042 - f1_score: 0.3337 - loss: 0.6921 - val_accuracy: 0.5400 - val_f1_score: 0.5490 - val_loss: 0.7105 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 4s - 117ms/step - accuracy: 0.5249 - f1_score: 0.5760 - loss: 0.6910 - val_accuracy: 0.5200 - val_f1_score: 0.5200 - val_loss: 0.6994 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 4s - 117ms/step - accuracy: 0.5334 - f1_score: 0.4236 - loss: 0.6901 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.7262 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 4s - 116ms/step - accuracy: 0.5296 - f1_score: 0.4918 - loss: 0.6912 - val_accuracy: 0.5000 - val_f1_score: 0.5098 - val_loss: 0.7138 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 4s - 117ms/step - accuracy: 0.5291 - f1_score: 0.3767 - loss: 0.6903 - val_accuracy: 0.5000 - val_f1_score: 0.5283 - val_loss: 0.7554 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 4s - 116ms/step - accuracy: 0.5253 - f1_score: 0.4891 - loss: 0.6893 - val_accuracy: 0.4800 - val_f1_score: 0.5357 - val_loss: 0.7469 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 4s - 117ms/step - accuracy: 0.5486 - f1_score: 0.5178 - loss: 0.6859 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.6960 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 4s - 119ms/step - accuracy: 0.5456 - f1_score: 0.4807 - loss: 0.6882 - val_accuracy: 0.5400 - val_f1_score: 0.5660 - val_loss: 0.7118 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 4s - 117ms/step - accuracy: 0.5498 - f1_score: 0.5028 - loss: 0.6822 - val_accuracy: 0.5600 - val_f1_score: 0.5600 - val_loss: 0.7019 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 4s - 118ms/step - accuracy: 0.5363 - f1_score: 0.4141 - loss: 0.6885 - val_accuracy: 0.5200 - val_f1_score: 0.6364 - val_loss: 0.7338 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 4s - 117ms/step - accuracy: 0.5460 - f1_score: 0.5372 - loss: 0.6878 - val_accuracy: 0.5600 - val_f1_score: 0.5600 - val_loss: 0.7114 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 4s - 117ms/step - accuracy: 0.5655 - f1_score: 0.5645 - loss: 0.6761 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.7718 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 4s - 117ms/step - accuracy: 0.5591 - f1_score: 0.5094 - loss: 0.6782 - val_accuracy: 0.5600 - val_f1_score: 0.5600 - val_loss: 0.7782 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 4s - 116ms/step - accuracy: 0.5410 - f1_score: 0.4826 - loss: 0.6844 - val_accuracy: 0.5000 - val_f1_score: 0.4898 - val_loss: 0.7302 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 4s - 117ms/step - accuracy: 0.5481 - f1_score: 0.4760 - loss: 0.6824 - val_accuracy: 0.4800 - val_f1_score: 0.5185 - val_loss: 0.7499 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 4s - 116ms/step - accuracy: 0.5756 - f1_score: 0.5818 - loss: 0.6776 - val_accuracy: 0.5400 - val_f1_score: 0.5306 - val_loss: 0.7217 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 4s - 116ms/step - accuracy: 0.5600 - f1_score: 0.5062 - loss: 0.6793 - val_accuracy: 0.5600 - val_f1_score: 0.5217 - val_loss: 0.7164 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 4s - 117ms/step - accuracy: 0.5680 - f1_score: 0.4783 - loss: 0.6808 - val_accuracy: 0.5400 - val_f1_score: 0.4889 - val_loss: 0.6958 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 4s - 116ms/step - accuracy: 0.5511 - f1_score: 0.4437 - loss: 0.6876 - val_accuracy: 0.5600 - val_f1_score: 0.3889 - val_loss: 0.6950 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 4s - 117ms/step - accuracy: 0.5515 - f1_score: 0.5560 - loss: 0.6816 - val_accuracy: 0.5200 - val_f1_score: 0.4286 - val_loss: 0.7166 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 4s - 115ms/step - accuracy: 0.5549 - f1_score: 0.4782 - loss: 0.6813 - val_accuracy: 0.5800 - val_f1_score: 0.5333 - val_loss: 0.7127 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 4s - 117ms/step - accuracy: 0.5836 - f1_score: 0.5530 - loss: 0.6696 - val_accuracy: 0.5200 - val_f1_score: 0.3333 - val_loss: 0.7145 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 4s - 117ms/step - accuracy: 0.5671 - f1_score: 0.4730 - loss: 0.6803 - val_accuracy: 0.4800 - val_f1_score: 0.2778 - val_loss: 0.7152 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 4s - 117ms/step - accuracy: 0.6106 - f1_score: 0.5371 - loss: 0.6653 - val_accuracy: 0.5000 - val_f1_score: 0.2424 - val_loss: 0.7609 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 4s - 117ms/step - accuracy: 0.5849 - f1_score: 0.5207 - loss: 0.6726 - val_accuracy: 0.5600 - val_f1_score: 0.4500 - val_loss: 0.7050 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 4s - 117ms/step - accuracy: 0.5739 - f1_score: 0.5369 - loss: 0.6782 - val_accuracy: 0.5200 - val_f1_score: 0.4545 - val_loss: 0.6965 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 4s - 117ms/step - accuracy: 0.5722 - f1_score: 0.6078 - loss: 0.6737 - val_accuracy: 0.5000 - val_f1_score: 0.4898 - val_loss: 0.7321 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 4s - 116ms/step - accuracy: 0.5655 - f1_score: 0.5501 - loss: 0.6684 - val_accuracy: 0.5200 - val_f1_score: 0.5000 - val_loss: 0.7880 - learning_rate: 6.1987e-04\nEpoch 30/200\n37/37 - 4s - 117ms/step - accuracy: 0.5726 - f1_score: 0.5780 - loss: 0.6648 - val_accuracy: 0.5000 - val_f1_score: 0.3902 - val_loss: 0.7164 - learning_rate: 5.8827e-04\nEpoch 31/200\n37/37 - 4s - 117ms/step - accuracy: 0.5735 - f1_score: 0.5562 - loss: 0.6669 - val_accuracy: 0.4800 - val_f1_score: 0.3158 - val_loss: 0.7050 - learning_rate: 5.5621e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257ms/step\nmodel4 → F1 = 0.6364\n              precision    recall  f1-score   support\n\n        Left       0.83      0.18      0.29        28\n       Right       0.48      0.95      0.64        22\n\n    accuracy                           0.52        50\n   macro avg       0.66      0.57      0.47        50\nweighted avg       0.68      0.52      0.44        50\n\n\n>>> Training model5\nEpoch 1/200\n37/37 - 7s - 197ms/step - accuracy: 0.5008 - f1_score: 0.4979 - loss: 1.1883 - val_accuracy: 0.6200 - val_f1_score: 0.5366 - val_loss: 1.1408 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 37ms/step - accuracy: 0.5177 - f1_score: 0.5320 - loss: 0.8407 - val_accuracy: 0.5800 - val_f1_score: 0.6182 - val_loss: 0.7098 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 22ms/step - accuracy: 0.5203 - f1_score: 0.5321 - loss: 0.7363 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 1.0088 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 23ms/step - accuracy: 0.5317 - f1_score: 0.5056 - loss: 0.7219 - val_accuracy: 0.4600 - val_f1_score: 0.5263 - val_loss: 0.6980 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 23ms/step - accuracy: 0.5448 - f1_score: 0.5917 - loss: 0.7019 - val_accuracy: 0.4800 - val_f1_score: 0.5806 - val_loss: 0.7760 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 22ms/step - accuracy: 0.5828 - f1_score: 0.6000 - loss: 0.6785 - val_accuracy: 0.5600 - val_f1_score: 0.3889 - val_loss: 0.9580 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 22ms/step - accuracy: 0.6630 - f1_score: 0.6178 - loss: 0.6105 - val_accuracy: 0.4400 - val_f1_score: 0.2632 - val_loss: 0.9595 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 23ms/step - accuracy: 0.7120 - f1_score: 0.7080 - loss: 0.5458 - val_accuracy: 0.5200 - val_f1_score: 0.5556 - val_loss: 0.9435 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 22ms/step - accuracy: 0.7846 - f1_score: 0.7986 - loss: 0.4497 - val_accuracy: 0.5400 - val_f1_score: 0.3429 - val_loss: 1.4514 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 23ms/step - accuracy: 0.8454 - f1_score: 0.8471 - loss: 0.3491 - val_accuracy: 0.4200 - val_f1_score: 0.3256 - val_loss: 1.7543 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 24ms/step - accuracy: 0.8809 - f1_score: 0.8822 - loss: 0.2761 - val_accuracy: 0.4600 - val_f1_score: 0.4706 - val_loss: 2.0038 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 22ms/step - accuracy: 0.9003 - f1_score: 0.8968 - loss: 0.2422 - val_accuracy: 0.5000 - val_f1_score: 0.3902 - val_loss: 1.8915 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 22ms/step - accuracy: 0.9227 - f1_score: 0.9238 - loss: 0.1985 - val_accuracy: 0.5400 - val_f1_score: 0.4889 - val_loss: 2.5080 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 22ms/step - accuracy: 0.9383 - f1_score: 0.9404 - loss: 0.1569 - val_accuracy: 0.5600 - val_f1_score: 0.4500 - val_loss: 2.7547 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 21ms/step - accuracy: 0.9641 - f1_score: 0.9634 - loss: 0.0993 - val_accuracy: 0.5000 - val_f1_score: 0.4898 - val_loss: 3.2686 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 22ms/step - accuracy: 0.9548 - f1_score: 0.9562 - loss: 0.1336 - val_accuracy: 0.5400 - val_f1_score: 0.5106 - val_loss: 2.3906 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 25ms/step - accuracy: 0.9586 - f1_score: 0.9598 - loss: 0.1038 - val_accuracy: 0.4400 - val_f1_score: 0.5000 - val_loss: 4.4040 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 24ms/step - accuracy: 0.9535 - f1_score: 0.9529 - loss: 0.1305 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 4.0034 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 22ms/step - accuracy: 0.9624 - f1_score: 0.9642 - loss: 0.1134 - val_accuracy: 0.5400 - val_f1_score: 0.4390 - val_loss: 3.1955 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 21ms/step - accuracy: 0.9633 - f1_score: 0.9618 - loss: 0.0923 - val_accuracy: 0.4200 - val_f1_score: 0.4528 - val_loss: 5.6185 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 22ms/step - accuracy: 0.9726 - f1_score: 0.9724 - loss: 0.0739 - val_accuracy: 0.4200 - val_f1_score: 0.4082 - val_loss: 5.0863 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 22ms/step - accuracy: 0.9840 - f1_score: 0.9837 - loss: 0.0403 - val_accuracy: 0.4800 - val_f1_score: 0.4583 - val_loss: 5.4017 - learning_rate: 8.1479e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step  \nmodel5 → F1 = 0.6182\n              precision    recall  f1-score   support\n\n        Left       0.71      0.43      0.53        28\n       Right       0.52      0.77      0.62        22\n\n    accuracy                           0.58        50\n   macro avg       0.61      0.60      0.58        50\nweighted avg       0.62      0.58      0.57        50\n\n\n>>> Training model6\nEpoch 1/200\n37/37 - 7s - 189ms/step - accuracy: 0.5072 - f1_score: 0.4866 - loss: 0.6946 - val_accuracy: 0.5000 - val_f1_score: 0.3243 - val_loss: 0.6965 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 39ms/step - accuracy: 0.5046 - f1_score: 0.4959 - loss: 0.6935 - val_accuracy: 0.4000 - val_f1_score: 0.4231 - val_loss: 0.7562 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 38ms/step - accuracy: 0.5110 - f1_score: 0.3693 - loss: 0.6946 - val_accuracy: 0.5400 - val_f1_score: 0.3030 - val_loss: 0.7058 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 37ms/step - accuracy: 0.5046 - f1_score: 0.3406 - loss: 0.6934 - val_accuracy: 0.5400 - val_f1_score: 0.4103 - val_loss: 0.7057 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 37ms/step - accuracy: 0.5051 - f1_score: 0.4979 - loss: 0.6942 - val_accuracy: 0.4800 - val_f1_score: 0.3500 - val_loss: 0.7129 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 38ms/step - accuracy: 0.5131 - f1_score: 0.5426 - loss: 0.6913 - val_accuracy: 0.4800 - val_f1_score: 0.4091 - val_loss: 0.7278 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 39ms/step - accuracy: 0.5148 - f1_score: 0.4387 - loss: 0.6934 - val_accuracy: 0.5200 - val_f1_score: 0.5714 - val_loss: 0.7284 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 37ms/step - accuracy: 0.5236 - f1_score: 0.6762 - loss: 0.6922 - val_accuracy: 0.4000 - val_f1_score: 0.5161 - val_loss: 0.7059 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 38ms/step - accuracy: 0.5084 - f1_score: 0.5724 - loss: 0.6921 - val_accuracy: 0.5200 - val_f1_score: 0.4000 - val_loss: 0.7076 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 38ms/step - accuracy: 0.5228 - f1_score: 0.5183 - loss: 0.6917 - val_accuracy: 0.5800 - val_f1_score: 0.0870 - val_loss: 0.6853 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 38ms/step - accuracy: 0.5144 - f1_score: 0.4758 - loss: 0.6916 - val_accuracy: 0.5200 - val_f1_score: 0.4000 - val_loss: 0.7484 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 37ms/step - accuracy: 0.4958 - f1_score: 0.5254 - loss: 0.6931 - val_accuracy: 0.5800 - val_f1_score: 0.1600 - val_loss: 0.6873 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 39ms/step - accuracy: 0.5152 - f1_score: 0.6019 - loss: 0.6925 - val_accuracy: 0.4400 - val_f1_score: 0.6000 - val_loss: 0.6938 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 37ms/step - accuracy: 0.5021 - f1_score: 0.4734 - loss: 0.6938 - val_accuracy: 0.4800 - val_f1_score: 0.5357 - val_loss: 0.7029 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 38ms/step - accuracy: 0.5080 - f1_score: 0.6342 - loss: 0.6926 - val_accuracy: 0.5600 - val_f1_score: 0.5217 - val_loss: 0.6800 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 38ms/step - accuracy: 0.4903 - f1_score: 0.4172 - loss: 0.6926 - val_accuracy: 0.5200 - val_f1_score: 0.2941 - val_loss: 0.6945 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 37ms/step - accuracy: 0.5160 - f1_score: 0.2625 - loss: 0.6922 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.7068 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 38ms/step - accuracy: 0.5296 - f1_score: 0.5408 - loss: 0.6898 - val_accuracy: 0.5200 - val_f1_score: 0.4783 - val_loss: 0.7395 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 38ms/step - accuracy: 0.5156 - f1_score: 0.5166 - loss: 0.6926 - val_accuracy: 0.5800 - val_f1_score: 0.5882 - val_loss: 0.7100 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 37ms/step - accuracy: 0.5004 - f1_score: 0.5894 - loss: 0.6913 - val_accuracy: 0.4400 - val_f1_score: 0.5333 - val_loss: 0.7333 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 38ms/step - accuracy: 0.5198 - f1_score: 0.6186 - loss: 0.6914 - val_accuracy: 0.5800 - val_f1_score: 0.4615 - val_loss: 0.6974 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 37ms/step - accuracy: 0.5173 - f1_score: 0.5419 - loss: 0.6932 - val_accuracy: 0.4200 - val_f1_score: 0.4528 - val_loss: 0.6951 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 38ms/step - accuracy: 0.5114 - f1_score: 0.6185 - loss: 0.6918 - val_accuracy: 0.4400 - val_f1_score: 0.5333 - val_loss: 0.7071 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 38ms/step - accuracy: 0.5203 - f1_score: 0.6014 - loss: 0.6900 - val_accuracy: 0.5000 - val_f1_score: 0.4444 - val_loss: 0.7072 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 37ms/step - accuracy: 0.5152 - f1_score: 0.6145 - loss: 0.6881 - val_accuracy: 0.5000 - val_f1_score: 0.5283 - val_loss: 0.7427 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 37ms/step - accuracy: 0.4970 - f1_score: 0.4917 - loss: 0.6950 - val_accuracy: 0.5000 - val_f1_score: 0.3590 - val_loss: 0.6946 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 38ms/step - accuracy: 0.5274 - f1_score: 0.5260 - loss: 0.6908 - val_accuracy: 0.5000 - val_f1_score: 0.3902 - val_loss: 0.7027 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 38ms/step - accuracy: 0.5300 - f1_score: 0.5159 - loss: 0.6917 - val_accuracy: 0.5200 - val_f1_score: 0.4000 - val_loss: 0.7040 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 1s - 38ms/step - accuracy: 0.5169 - f1_score: 0.4526 - loss: 0.6916 - val_accuracy: 0.5600 - val_f1_score: 0.4762 - val_loss: 0.7056 - learning_rate: 6.1987e-04\nEpoch 30/200\n37/37 - 1s - 38ms/step - accuracy: 0.5030 - f1_score: 0.5540 - loss: 0.6917 - val_accuracy: 0.5600 - val_f1_score: 0.4762 - val_loss: 0.6940 - learning_rate: 5.8827e-04\nEpoch 31/200\n37/37 - 1s - 38ms/step - accuracy: 0.5059 - f1_score: 0.4553 - loss: 0.6908 - val_accuracy: 0.5400 - val_f1_score: 0.4390 - val_loss: 0.6964 - learning_rate: 5.5621e-04\nEpoch 32/200\n37/37 - 1s - 38ms/step - accuracy: 0.5228 - f1_score: 0.5296 - loss: 0.6911 - val_accuracy: 0.5000 - val_f1_score: 0.4444 - val_loss: 0.7008 - learning_rate: 5.2388e-04\nEpoch 33/200\n37/37 - 1s - 38ms/step - accuracy: 0.5367 - f1_score: 0.4900 - loss: 0.6878 - val_accuracy: 0.5400 - val_f1_score: 0.4103 - val_loss: 0.7042 - learning_rate: 4.9148e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394ms/step\nmodel6 → F1 = 0.6000\n              precision    recall  f1-score   support\n\n        Left       0.50      0.04      0.07        28\n       Right       0.44      0.95      0.60        22\n\n    accuracy                           0.44        50\n   macro avg       0.47      0.50      0.33        50\nweighted avg       0.47      0.44      0.30        50\n\n\n=== Final best: model4 (F1=0.6364) ===\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\n\nfrom sklearn.metrics import f1_score, classification_report\nfrom mne.decoding import CSP\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, LearningRateScheduler\n\n# --- 0) Config ---\ndata_dir   = './preprocessed'\noutput_dir = './models3'\nos.makedirs(output_dir, exist_ok=True)\n\neeg_indices = [1, 2, 3, 4]  # C3, CZ, C4, PZ\n\n# --- 1) Load & filter data ---\ntrain_npz = np.load(os.path.join(data_dir, 'train_MI.npz'))\nval_npz   = np.load(os.path.join(data_dir, 'validation_MI.npz'))\n\nX_train_all, y_train = train_npz['X'], train_npz['y']\nX_val_all,   y_val   = val_npz['X'],   val_npz['y']\n\n# Filter for EEG channels only\nX_train_raw = X_train_all[:, eeg_indices, :].transpose(0, 2, 1).astype('float32')  # (n, T, 4)\nX_val_raw   = X_val_all[:,   eeg_indices, :].transpose(0, 2, 1).astype('float32')  # (n, T, 4)\n\n# --- 2) Binarize labels ---\ny_train_bin = (y_train == 'Right').astype(int)\ny_val_bin   = (y_val   == 'Right').astype(int)\n\n# --- 3) CSP for models 3, 4, 5 ---\ncsp = CSP(n_components=4, log=False, norm_trace=False)\nX_train_csp_input = X_train_raw.transpose(0, 2, 1).astype('float64').copy()  # (n, channels, time)\ncsp.fit(X_train_csp_input, y_train_bin)  # ← NO 'rank' argument\n\nW = csp.filters_[:4]\n\ndef apply_csp(X):  # X: (n, T, C)\n    return np.stack([W.dot(ep.T) for ep in X], axis=0)\n\n\nXtr_csp = apply_csp(X_train_raw).astype('float32')\nXvl_csp = apply_csp(X_val_raw).astype('float32')\n\nXtr_csp = Xtr_csp.transpose(0, 2, 1)  # (n, T, 4)\nXvl_csp = Xvl_csp.transpose(0, 2, 1)\n\n\n# For 2D models\nXtr_csp_2d = Xtr_csp[..., np.newaxis]\nXvl_csp_2d = Xvl_csp[..., np.newaxis]\n\n# --- 4) One-hot labels ---\nytr_oh = keras.utils.to_categorical(y_train_bin, 2)\nyvl_oh = keras.utils.to_categorical(y_val_bin,   2)\n\n# --- 5) Data augmentation ---\ndef aug_gen(X, y, seed=0, batch_size=32):\n    n = X.shape[0]\n    rng = np.random.RandomState(seed)\n    while True:\n        idx = rng.randint(0, n, batch_size)\n        bx, by = X[idx].copy(), y[idx]\n        bx += rng.normal(0, 0.005, bx.shape)\n        yield bx, by\n\ntrain_gen_raw   = aug_gen(X_train_raw,  ytr_oh, seed=0, batch_size=64)\ntrain_gen_csp1d = aug_gen(Xtr_csp,      ytr_oh, seed=1, batch_size=64)\ntrain_gen_csp2d = aug_gen(Xtr_csp_2d,   ytr_oh, seed=2, batch_size=64)\n\nsteps_raw   = len(X_train_raw)  // 64\nsteps_csp1d = len(Xtr_csp)      // 64\nsteps_csp2d = len(Xtr_csp_2d)   // 64\n\n# --- 6) Cosine LR schedule ---\ndef cosine_lr(epoch, lr_max=5e-5, epochs=200):\n    return lr_max * (1 + np.cos(np.pi * epoch / epochs)) / 2\n\n# --- 7) F1 Score Metric ---\nclass F1Score(tf.keras.metrics.Metric):\n    def __init__(self, name=\"f1_score\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        preds  = tf.argmax(y_pred, axis=1)\n        labels = tf.argmax(y_true, axis=1)\n        self.tp.assign_add(tf.reduce_sum(tf.cast(tf.logical_and(preds == 1, labels == 1), tf.float32)))\n        self.fp.assign_add(tf.reduce_sum(tf.cast(tf.logical_and(preds == 1, labels == 0), tf.float32)))\n        self.fn.assign_add(tf.reduce_sum(tf.cast(tf.logical_and(preds == 0, labels == 1), tf.float32)))\n\n    def result(self):\n        p = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n        r = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n        return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n\n    def reset_states(self):\n        self.tp.assign(0.0)\n        self.fp.assign(0.0)\n        self.fn.assign(0.0)\n\n# --- 8) Callback factory ---\ndef get_callbacks(name):\n    return [\n        EarlyStopping(\"val_f1_score\", mode=\"max\", patience=20, restore_best_weights=True),\n        ModelCheckpoint(os.path.join(output_dir, f\"best_{name}.h5\"),\n                        \"val_f1_score\", mode=\"max\", save_best_only=True),\n        CSVLogger(os.path.join(output_dir, f\"log_{name}.csv\")),\n        LearningRateScheduler(cosine_lr)\n    ]\n\n# --- 9) Model Builders ---\n# [All model builder functions remain unchanged — see previous message if needed.]\ndef build_modelA(input_shape):\n    m = keras.Sequential([\n        layers.Input(input_shape),\n        layers.Conv1D(32, 5, activation=\"relu\", padding=\"same\"),\n        layers.BatchNormalization(), layers.MaxPool1D(2),\n        layers.Conv1D(64, 5, activation=\"relu\", padding=\"same\"),\n        layers.BatchNormalization(), layers.MaxPool1D(2),\n        layers.Conv1D(128,5,activation=\"relu\",padding=\"same\"),\n        layers.BatchNormalization(),\n        layers.GlobalAveragePooling1D(),\n        layers.Dense(64, activation=\"relu\", \n                     kernel_regularizer=regularizers.l2(1e-4)),\n        layers.Dropout(0.7),\n        layers.Dense(2, activation=\"softmax\"),\n    ])\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_modelB(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for f in [16,32,64,128,256]:\n        x = layers.Conv1D(f,3,activation=\"relu\",padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.MaxPool1D(2)(x)\n    x = layers.Flatten()(x)\n    for u in [128,64,32]:\n        x = layers.Dense(u, activation=\"relu\",\n                         kernel_regularizer=regularizers.l2(1e-4))(x)\n        x = layers.Dropout(0.5)(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp, out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model1(input_shape):\n    inp = layers.Input(input_shape+(1,))\n    x = layers.Concatenate()([inp, inp, inp])\n    x = layers.Resizing(32,32)(x)\n    base = keras.applications.ResNet50(\n        include_top=False, weights=\"imagenet\",\n        input_shape=(32,32,3), pooling=\"avg\"\n    )\n    base.trainable = False\n    x = base(x)\n    x = layers.Reshape((1, x.shape[-1]))(x)\n    for _ in range(7):\n        x = layers.Conv1D(64,3,activation=\"relu\",padding=\"same\")(x)\n    x = layers.MultiHeadAttention(num_heads=4,key_dim=32)(x,x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(64,activation=\"relu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model2(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for _ in range(3):\n        x = layers.Conv1D(32,3,activation=\"elu\",padding=\"same\")(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(64,activation=\"elu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model3(input_shape):\n    inp = layers.Input(input_shape)  # (T, F, 1)\n    x = inp\n    for _ in range(5):\n        x = layers.Conv2D(32,(3,3),activation=\"relu\",padding=\"same\")(x)\n    x = layers.Flatten()(x)\n    for u in [128,64,32]:\n        x = layers.Dense(u, activation=\"relu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model4(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for _ in range(3):\n        x = layers.Conv1D(64,3,activation=\"relu\",padding=\"same\")(x)\n    x = layers.LSTM(128)(x)\n    for _ in range(4):\n        x = layers.Dense(64, activation=\"relu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model5(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for _ in range(7):\n        x = layers.Conv1D(64,3,activation=\"elu\",padding=\"same\")(x)\n    x = layers.Flatten()(x)\n    for _ in range(3):\n        x = layers.Dense(64, activation=\"elu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model6(input_shape):\n    C3, C4 = 0,2\n    eeg_in = layers.Input(input_shape)\n    c3 = layers.Lambda(lambda x: x[:,:,C3:C3+1])(eeg_in)\n    c4 = layers.Lambda(lambda x: x[:,:,C4:C4+1])(eeg_in)\n    def branch():\n        return models.Sequential([\n            layers.Conv1D(16,250,activation=\"relu\",padding=\"same\"),\n            layers.MaxPool1D(3),\n            layers.Conv1D(32,50,activation=\"relu\",padding=\"same\"),\n            layers.GlobalAveragePooling1D()\n        ])\n    b3, b4 = branch()(c3), branch()(c4)\n    x = layers.Concatenate()([b3,b4])\n    for _ in range(4):\n        x = layers.Dense(64,activation=\"relu\")(x)\n    out = layers.Dense(2,activation=\"softmax\")(x)\n    m = models.Model(eeg_in,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n# (You can paste model builders here unchanged if needed)\n\n# --- 10) Train & evaluate ---\nbuilders = {\n    'oldA': build_modelA,\n    'oldB': build_modelB,\n    'model1': build_model1,\n    'model2': build_model2,\n    'model3': build_model3,\n    'model4': build_model4,\n    'model5': build_model5,\n    'model6': build_model6,\n}\n\nresults = {}\nshape_raw   = X_train_raw.shape[1:]\nshape_csp1d = Xtr_csp.shape[1:]\nshape_csp2d = Xtr_csp_2d.shape[1:]\n\nfor name, build_fn in builders.items():\n    print(f\"\\n>>> Training {name}\")\n    if name in ['model3']:\n        model = build_fn(shape_csp2d)\n        gen, steps, val_x = train_gen_csp2d, steps_csp2d, Xvl_csp_2d\n    elif name in ['model4', 'model5']:\n        model = build_fn(shape_csp1d)\n        gen, steps, val_x = train_gen_csp1d, steps_csp1d, Xvl_csp\n    else:\n        model = build_fn(shape_raw)\n        gen, steps, val_x = train_gen_raw, steps_raw, X_val_raw\n\n    model.fit(\n        gen, steps_per_epoch=steps,\n        validation_data=(val_x, yvl_oh),\n        epochs=200, callbacks=get_callbacks(name), verbose=2\n    )\n\n    preds = np.argmax(model.predict(val_x), axis=1)\n    f1 = f1_score(y_val_bin, preds)\n    print(f\"{name} → val F1 = {f1:.4f}\")\n    print(classification_report(y_val_bin, preds, target_names=[\"Left\", \"Right\"]))\n    results[name] = (f1, model)\n\n# --- 11) Save best model ---\nbest_name, (best_f1, best_model) = max(results.items(), key=lambda kv: kv[1][0])\nprint(f\"\\n=== Final best: {best_name} (F1={best_f1:.4f}) ===\")\nbest_model.save(os.path.join(output_dir, 'best_final.h5'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T02:34:14.546778Z","iopub.execute_input":"2025-07-23T02:34:14.547042Z","iopub.status.idle":"2025-07-23T02:44:40.796199Z","shell.execute_reply.started":"2025-07-23T02:34:14.547021Z","shell.execute_reply":"2025-07-23T02:44:40.795358Z"}},"outputs":[{"name":"stderr","text":"2025-07-23 02:34:16.028611: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753238056.230080      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753238056.291003      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Computing rank from data with rank=None\n    Using tolerance 3.2e+03 (2.2e-16 eps * 4 dim * 3.6e+18  max singular value)\n    Estimated rank (data): 4\n    data: rank 4 computed from 4 data channels with 0 projectors\nReducing data rank from 4 -> 4\nEstimating class=0 covariance using EMPIRICAL\nDone.\nEstimating class=1 covariance using EMPIRICAL\nDone.\n\n>>> Training oldA\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1753238076.518329      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1753238081.800418     103 service.cc:148] XLA service 0x7e21dc027450 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1753238081.801340     103 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1753238082.250249     103 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1753238085.772575     103 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"37/37 - 10s - 278ms/step - accuracy: 0.5114 - f1_score: 0.5268 - loss: 0.8497 - val_accuracy: 0.5400 - val_f1_score: 0.3030 - val_loss: 1.3972 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 28ms/step - accuracy: 0.4937 - f1_score: 0.4856 - loss: 0.7747 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 1.4394 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 26ms/step - accuracy: 0.5224 - f1_score: 0.5144 - loss: 0.7270 - val_accuracy: 0.5400 - val_f1_score: 0.3429 - val_loss: 0.7312 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 27ms/step - accuracy: 0.5034 - f1_score: 0.5377 - loss: 0.7254 - val_accuracy: 0.4200 - val_f1_score: 0.4082 - val_loss: 0.7203 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 27ms/step - accuracy: 0.5101 - f1_score: 0.5199 - loss: 0.7101 - val_accuracy: 0.4800 - val_f1_score: 0.0714 - val_loss: 0.7197 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 27ms/step - accuracy: 0.4878 - f1_score: 0.4563 - loss: 0.7150 - val_accuracy: 0.6000 - val_f1_score: 0.3333 - val_loss: 0.7106 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 27ms/step - accuracy: 0.5169 - f1_score: 0.4767 - loss: 0.7059 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.8054 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 28ms/step - accuracy: 0.5076 - f1_score: 0.4666 - loss: 0.7070 - val_accuracy: 0.5000 - val_f1_score: 0.5763 - val_loss: 0.7147 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 26ms/step - accuracy: 0.5220 - f1_score: 0.5220 - loss: 0.7027 - val_accuracy: 0.5400 - val_f1_score: 0.0000e+00 - val_loss: 0.7667 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 26ms/step - accuracy: 0.5220 - f1_score: 0.5421 - loss: 0.7006 - val_accuracy: 0.4600 - val_f1_score: 0.5091 - val_loss: 0.7146 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 28ms/step - accuracy: 0.5300 - f1_score: 0.5171 - loss: 0.7006 - val_accuracy: 0.5400 - val_f1_score: 0.6102 - val_loss: 0.7294 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 27ms/step - accuracy: 0.5051 - f1_score: 0.4887 - loss: 0.7051 - val_accuracy: 0.5000 - val_f1_score: 0.4444 - val_loss: 0.7146 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 26ms/step - accuracy: 0.5118 - f1_score: 0.5677 - loss: 0.7012 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.7057 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 28ms/step - accuracy: 0.5291 - f1_score: 0.6133 - loss: 0.7007 - val_accuracy: 0.5000 - val_f1_score: 0.6269 - val_loss: 0.7106 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 27ms/step - accuracy: 0.5139 - f1_score: 0.5688 - loss: 0.7018 - val_accuracy: 0.5400 - val_f1_score: 0.5660 - val_loss: 0.7152 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 26ms/step - accuracy: 0.5249 - f1_score: 0.5353 - loss: 0.6982 - val_accuracy: 0.6600 - val_f1_score: 0.6047 - val_loss: 0.6960 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 27ms/step - accuracy: 0.5338 - f1_score: 0.4396 - loss: 0.7020 - val_accuracy: 0.6000 - val_f1_score: 0.2857 - val_loss: 0.6932 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 26ms/step - accuracy: 0.5312 - f1_score: 0.4494 - loss: 0.6959 - val_accuracy: 0.6200 - val_f1_score: 0.2963 - val_loss: 0.6883 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 26ms/step - accuracy: 0.5346 - f1_score: 0.4712 - loss: 0.6978 - val_accuracy: 0.4600 - val_f1_score: 0.6197 - val_loss: 0.7411 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 27ms/step - accuracy: 0.5351 - f1_score: 0.5236 - loss: 0.6989 - val_accuracy: 0.4400 - val_f1_score: 0.4167 - val_loss: 0.7291 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 27ms/step - accuracy: 0.5177 - f1_score: 0.4603 - loss: 0.7005 - val_accuracy: 0.6000 - val_f1_score: 0.2308 - val_loss: 0.7048 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 28ms/step - accuracy: 0.5507 - f1_score: 0.4544 - loss: 0.6941 - val_accuracy: 0.5800 - val_f1_score: 0.5714 - val_loss: 0.7287 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 27ms/step - accuracy: 0.5207 - f1_score: 0.4551 - loss: 0.7017 - val_accuracy: 0.5000 - val_f1_score: 0.4898 - val_loss: 0.7008 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 27ms/step - accuracy: 0.5186 - f1_score: 0.5026 - loss: 0.6985 - val_accuracy: 0.5800 - val_f1_score: 0.5333 - val_loss: 0.7002 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 27ms/step - accuracy: 0.5418 - f1_score: 0.5730 - loss: 0.6964 - val_accuracy: 0.4800 - val_f1_score: 0.5517 - val_loss: 0.7057 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 27ms/step - accuracy: 0.5372 - f1_score: 0.5671 - loss: 0.6951 - val_accuracy: 0.5600 - val_f1_score: 0.4211 - val_loss: 0.6946 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 26ms/step - accuracy: 0.5198 - f1_score: 0.5105 - loss: 0.6953 - val_accuracy: 0.6000 - val_f1_score: 0.4118 - val_loss: 0.6906 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 26ms/step - accuracy: 0.5359 - f1_score: 0.5588 - loss: 0.6970 - val_accuracy: 0.6000 - val_f1_score: 0.5833 - val_loss: 0.7041 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 1s - 26ms/step - accuracy: 0.5414 - f1_score: 0.5666 - loss: 0.6968 - val_accuracy: 0.5000 - val_f1_score: 0.5614 - val_loss: 0.7051 - learning_rate: 6.1987e-04\nEpoch 30/200\n37/37 - 1s - 27ms/step - accuracy: 0.5477 - f1_score: 0.6041 - loss: 0.6925 - val_accuracy: 0.5400 - val_f1_score: 0.4651 - val_loss: 0.6980 - learning_rate: 5.8827e-04\nEpoch 31/200\n37/37 - 1s - 27ms/step - accuracy: 0.5367 - f1_score: 0.5542 - loss: 0.6933 - val_accuracy: 0.6200 - val_f1_score: 0.5581 - val_loss: 0.6900 - learning_rate: 5.5621e-04\nEpoch 32/200\n37/37 - 1s - 26ms/step - accuracy: 0.5296 - f1_score: 0.5943 - loss: 0.6959 - val_accuracy: 0.4600 - val_f1_score: 0.4706 - val_loss: 0.7018 - learning_rate: 5.2388e-04\nEpoch 33/200\n37/37 - 1s - 26ms/step - accuracy: 0.5574 - f1_score: 0.6095 - loss: 0.6902 - val_accuracy: 0.5200 - val_f1_score: 0.4286 - val_loss: 0.7069 - learning_rate: 4.9148e-04\nEpoch 34/200\n37/37 - 1s - 27ms/step - accuracy: 0.5629 - f1_score: 0.5756 - loss: 0.6907 - val_accuracy: 0.5800 - val_f1_score: 0.3636 - val_loss: 0.6977 - learning_rate: 4.5920e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 430ms/step\noldA → val F1 = 0.6269\n              precision    recall  f1-score   support\n\n        Left       0.80      0.14      0.24        28\n       Right       0.47      0.95      0.63        22\n\n    accuracy                           0.50        50\n   macro avg       0.63      0.55      0.43        50\nweighted avg       0.65      0.50      0.41        50\n\n\n>>> Training oldB\nEpoch 1/200\n37/37 - 15s - 410ms/step - accuracy: 0.5017 - f1_score: 0.4718 - loss: 2.6014 - val_accuracy: 0.4400 - val_f1_score: 0.5172 - val_loss: 0.8137 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 31ms/step - accuracy: 0.5093 - f1_score: 0.5348 - loss: 1.6895 - val_accuracy: 0.4000 - val_f1_score: 0.4643 - val_loss: 0.8098 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 36ms/step - accuracy: 0.5114 - f1_score: 0.5095 - loss: 1.2398 - val_accuracy: 0.6200 - val_f1_score: 0.6275 - val_loss: 0.7546 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 32ms/step - accuracy: 0.5152 - f1_score: 0.5193 - loss: 1.0359 - val_accuracy: 0.5200 - val_f1_score: 0.2941 - val_loss: 0.7246 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 31ms/step - accuracy: 0.5021 - f1_score: 0.5106 - loss: 0.9683 - val_accuracy: 0.5000 - val_f1_score: 0.6154 - val_loss: 0.7588 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 31ms/step - accuracy: 0.5262 - f1_score: 0.5367 - loss: 0.8751 - val_accuracy: 0.4600 - val_f1_score: 0.4706 - val_loss: 0.7543 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 31ms/step - accuracy: 0.5008 - f1_score: 0.5215 - loss: 0.8538 - val_accuracy: 0.4200 - val_f1_score: 0.4314 - val_loss: 0.7605 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 31ms/step - accuracy: 0.4844 - f1_score: 0.5026 - loss: 0.8161 - val_accuracy: 0.5000 - val_f1_score: 0.3590 - val_loss: 0.7504 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 31ms/step - accuracy: 0.5093 - f1_score: 0.5457 - loss: 0.7890 - val_accuracy: 0.5000 - val_f1_score: 0.6154 - val_loss: 0.7500 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 31ms/step - accuracy: 0.4983 - f1_score: 0.5434 - loss: 0.7860 - val_accuracy: 0.4600 - val_f1_score: 0.5574 - val_loss: 0.7473 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 31ms/step - accuracy: 0.5114 - f1_score: 0.5782 - loss: 0.7762 - val_accuracy: 0.4400 - val_f1_score: 0.5484 - val_loss: 0.7534 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 36ms/step - accuracy: 0.4949 - f1_score: 0.5567 - loss: 0.7764 - val_accuracy: 0.6600 - val_f1_score: 0.6792 - val_loss: 0.7466 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 31ms/step - accuracy: 0.4983 - f1_score: 0.5537 - loss: 0.7716 - val_accuracy: 0.4200 - val_f1_score: 0.5246 - val_loss: 0.7488 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 31ms/step - accuracy: 0.5131 - f1_score: 0.5851 - loss: 0.7568 - val_accuracy: 0.4400 - val_f1_score: 0.6000 - val_loss: 0.7529 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 31ms/step - accuracy: 0.5283 - f1_score: 0.6082 - loss: 0.7635 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7525 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 31ms/step - accuracy: 0.5000 - f1_score: 0.5798 - loss: 0.7615 - val_accuracy: 0.4600 - val_f1_score: 0.6197 - val_loss: 0.7462 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 31ms/step - accuracy: 0.5198 - f1_score: 0.5981 - loss: 0.7613 - val_accuracy: 0.5000 - val_f1_score: 0.5902 - val_loss: 0.7524 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 31ms/step - accuracy: 0.4949 - f1_score: 0.5853 - loss: 0.7552 - val_accuracy: 0.4800 - val_f1_score: 0.5357 - val_loss: 0.7573 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 31ms/step - accuracy: 0.4954 - f1_score: 0.5715 - loss: 0.7592 - val_accuracy: 0.4600 - val_f1_score: 0.5846 - val_loss: 0.7486 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 31ms/step - accuracy: 0.4907 - f1_score: 0.4697 - loss: 0.7542 - val_accuracy: 0.5200 - val_f1_score: 0.2941 - val_loss: 0.7460 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 31ms/step - accuracy: 0.4975 - f1_score: 0.4439 - loss: 0.7611 - val_accuracy: 0.4800 - val_f1_score: 0.0714 - val_loss: 0.7462 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 31ms/step - accuracy: 0.5030 - f1_score: 0.4344 - loss: 0.7507 - val_accuracy: 0.4400 - val_f1_score: 0.5484 - val_loss: 0.7473 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 31ms/step - accuracy: 0.5245 - f1_score: 0.4566 - loss: 0.7467 - val_accuracy: 0.5600 - val_f1_score: 0.3529 - val_loss: 0.7462 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 31ms/step - accuracy: 0.5220 - f1_score: 0.5187 - loss: 0.7523 - val_accuracy: 0.4400 - val_f1_score: 0.6000 - val_loss: 0.7499 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 31ms/step - accuracy: 0.5034 - f1_score: 0.5794 - loss: 0.7488 - val_accuracy: 0.4800 - val_f1_score: 0.6176 - val_loss: 0.7467 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 31ms/step - accuracy: 0.4899 - f1_score: 0.4655 - loss: 0.7494 - val_accuracy: 0.4400 - val_f1_score: 0.3636 - val_loss: 0.7480 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 31ms/step - accuracy: 0.5211 - f1_score: 0.4726 - loss: 0.7453 - val_accuracy: 0.5600 - val_f1_score: 0.2667 - val_loss: 0.7457 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 31ms/step - accuracy: 0.5245 - f1_score: 0.3991 - loss: 0.7515 - val_accuracy: 0.5600 - val_f1_score: 0.2143 - val_loss: 0.7454 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 1s - 32ms/step - accuracy: 0.5063 - f1_score: 0.4635 - loss: 0.7455 - val_accuracy: 0.4600 - val_f1_score: 0.4490 - val_loss: 0.7474 - learning_rate: 6.1987e-04\nEpoch 30/200\n37/37 - 1s - 31ms/step - accuracy: 0.5131 - f1_score: 0.4723 - loss: 0.7456 - val_accuracy: 0.6200 - val_f1_score: 0.4865 - val_loss: 0.7446 - learning_rate: 5.8827e-04\nEpoch 31/200\n37/37 - 1s - 32ms/step - accuracy: 0.5220 - f1_score: 0.4640 - loss: 0.7463 - val_accuracy: 0.6400 - val_f1_score: 0.5000 - val_loss: 0.7420 - learning_rate: 5.5621e-04\nEpoch 32/200\n37/37 - 1s - 31ms/step - accuracy: 0.5177 - f1_score: 0.4301 - loss: 0.7439 - val_accuracy: 0.5400 - val_f1_score: 0.2581 - val_loss: 0.7457 - learning_rate: 5.2388e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 674ms/step\noldB → val F1 = 0.6792\n              precision    recall  f1-score   support\n\n        Left       0.79      0.54      0.64        28\n       Right       0.58      0.82      0.68        22\n\n    accuracy                           0.66        50\n   macro avg       0.69      0.68      0.66        50\nweighted avg       0.70      0.66      0.66        50\n\n\n>>> Training model1\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/200\n37/37 - 29s - 776ms/step - accuracy: 0.4928 - f1_score: 0.3819 - loss: 0.6945 - val_accuracy: 0.4400 - val_f1_score: 0.3000 - val_loss: 0.6947 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 2s - 42ms/step - accuracy: 0.5131 - f1_score: 0.2798 - loss: 0.6931 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7128 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 22ms/step - accuracy: 0.4823 - f1_score: 0.6027 - loss: 0.6943 - val_accuracy: 0.5000 - val_f1_score: 0.4186 - val_loss: 0.6948 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 22ms/step - accuracy: 0.5093 - f1_score: 0.5618 - loss: 0.6934 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6982 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 22ms/step - accuracy: 0.5097 - f1_score: 0.6738 - loss: 0.6933 - val_accuracy: 0.4200 - val_f1_score: 0.5915 - val_loss: 0.6971 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 22ms/step - accuracy: 0.5220 - f1_score: 0.6663 - loss: 0.6923 - val_accuracy: 0.4400 - val_f1_score: 0.6000 - val_loss: 0.6982 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 21ms/step - accuracy: 0.4949 - f1_score: 0.5162 - loss: 0.6916 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6941 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 22ms/step - accuracy: 0.5127 - f1_score: 0.6778 - loss: 0.6930 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6974 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 21ms/step - accuracy: 0.5236 - f1_score: 0.6874 - loss: 0.6926 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6976 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 21ms/step - accuracy: 0.5038 - f1_score: 0.5496 - loss: 0.6929 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6992 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 21ms/step - accuracy: 0.5215 - f1_score: 0.6855 - loss: 0.6924 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6991 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 22ms/step - accuracy: 0.4975 - f1_score: 0.5170 - loss: 0.6931 - val_accuracy: 0.5400 - val_f1_score: 0.5965 - val_loss: 0.6958 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 22ms/step - accuracy: 0.5068 - f1_score: 0.0875 - loss: 0.6933 - val_accuracy: 0.3800 - val_f1_score: 0.3922 - val_loss: 0.6943 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 22ms/step - accuracy: 0.5038 - f1_score: 0.5950 - loss: 0.6933 - val_accuracy: 0.5600 - val_f1_score: 0.5769 - val_loss: 0.6905 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 22ms/step - accuracy: 0.5068 - f1_score: 0.4379 - loss: 0.6933 - val_accuracy: 0.4200 - val_f1_score: 0.5797 - val_loss: 0.6948 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 21ms/step - accuracy: 0.5084 - f1_score: 0.5807 - loss: 0.6930 - val_accuracy: 0.4200 - val_f1_score: 0.5797 - val_loss: 0.6976 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 22ms/step - accuracy: 0.5346 - f1_score: 0.6552 - loss: 0.6898 - val_accuracy: 0.4800 - val_f1_score: 0.6061 - val_loss: 0.6969 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 22ms/step - accuracy: 0.5498 - f1_score: 0.6718 - loss: 0.6881 - val_accuracy: 0.5200 - val_f1_score: 0.6000 - val_loss: 0.6988 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 22ms/step - accuracy: 0.5144 - f1_score: 0.6672 - loss: 0.6924 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6976 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 21ms/step - accuracy: 0.5030 - f1_score: 0.6517 - loss: 0.6932 - val_accuracy: 0.5400 - val_f1_score: 0.0000e+00 - val_loss: 0.6926 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 21ms/step - accuracy: 0.5127 - f1_score: 0.3510 - loss: 0.6925 - val_accuracy: 0.5000 - val_f1_score: 0.3902 - val_loss: 0.7011 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 22ms/step - accuracy: 0.5283 - f1_score: 0.4316 - loss: 0.6865 - val_accuracy: 0.5800 - val_f1_score: 0.0870 - val_loss: 0.6868 - learning_rate: 8.1479e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5s/step\nmodel1 → val F1 = 0.6111\n              precision    recall  f1-score   support\n\n        Left       0.00      0.00      0.00        28\n       Right       0.44      1.00      0.61        22\n\n    accuracy                           0.44        50\n   macro avg       0.22      0.50      0.31        50\nweighted avg       0.19      0.44      0.27        50\n\n\n>>> Training model2\nEpoch 1/200\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"37/37 - 5s - 136ms/step - accuracy: 0.4996 - f1_score: 0.4908 - loss: 4.8915 - val_accuracy: 0.5400 - val_f1_score: 0.4103 - val_loss: 2.1693 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 22ms/step - accuracy: 0.5068 - f1_score: 0.4991 - loss: 1.1769 - val_accuracy: 0.5000 - val_f1_score: 0.5283 - val_loss: 2.7459 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 21ms/step - accuracy: 0.5443 - f1_score: 0.5430 - loss: 0.9913 - val_accuracy: 0.5200 - val_f1_score: 0.5714 - val_loss: 2.2741 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 19ms/step - accuracy: 0.5228 - f1_score: 0.5300 - loss: 1.0038 - val_accuracy: 0.5600 - val_f1_score: 0.3889 - val_loss: 2.2016 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 20ms/step - accuracy: 0.5389 - f1_score: 0.5404 - loss: 0.8933 - val_accuracy: 0.5600 - val_f1_score: 0.4500 - val_loss: 2.3249 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 20ms/step - accuracy: 0.5122 - f1_score: 0.5465 - loss: 0.8806 - val_accuracy: 0.5800 - val_f1_score: 0.5333 - val_loss: 1.1109 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 20ms/step - accuracy: 0.5346 - f1_score: 0.5041 - loss: 0.7962 - val_accuracy: 0.5400 - val_f1_score: 0.4889 - val_loss: 1.1652 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 20ms/step - accuracy: 0.5469 - f1_score: 0.5706 - loss: 0.7822 - val_accuracy: 0.4600 - val_f1_score: 0.4706 - val_loss: 1.7352 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 21ms/step - accuracy: 0.5359 - f1_score: 0.5325 - loss: 0.7817 - val_accuracy: 0.6000 - val_f1_score: 0.5455 - val_loss: 1.5399 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 22ms/step - accuracy: 0.5549 - f1_score: 0.5616 - loss: 0.7939 - val_accuracy: 0.3800 - val_f1_score: 0.3922 - val_loss: 1.4261 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 20ms/step - accuracy: 0.5557 - f1_score: 0.5696 - loss: 0.7304 - val_accuracy: 0.5000 - val_f1_score: 0.4681 - val_loss: 1.6335 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 20ms/step - accuracy: 0.5477 - f1_score: 0.5669 - loss: 0.7431 - val_accuracy: 0.3600 - val_f1_score: 0.4286 - val_loss: 1.5527 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 20ms/step - accuracy: 0.5477 - f1_score: 0.5229 - loss: 0.7700 - val_accuracy: 0.5600 - val_f1_score: 0.5217 - val_loss: 1.5124 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 20ms/step - accuracy: 0.5418 - f1_score: 0.5227 - loss: 0.7580 - val_accuracy: 0.3400 - val_f1_score: 0.4590 - val_loss: 1.4945 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 20ms/step - accuracy: 0.5422 - f1_score: 0.5483 - loss: 0.7429 - val_accuracy: 0.5600 - val_f1_score: 0.4211 - val_loss: 1.1965 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 21ms/step - accuracy: 0.5600 - f1_score: 0.5551 - loss: 0.7088 - val_accuracy: 0.4600 - val_f1_score: 0.5263 - val_loss: 1.3373 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 20ms/step - accuracy: 0.5549 - f1_score: 0.5549 - loss: 0.7217 - val_accuracy: 0.4000 - val_f1_score: 0.4643 - val_loss: 1.2164 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 20ms/step - accuracy: 0.5739 - f1_score: 0.5331 - loss: 0.7033 - val_accuracy: 0.3400 - val_f1_score: 0.3529 - val_loss: 1.3776 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 20ms/step - accuracy: 0.5739 - f1_score: 0.5715 - loss: 0.6961 - val_accuracy: 0.4400 - val_f1_score: 0.4615 - val_loss: 1.0839 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 20ms/step - accuracy: 0.5676 - f1_score: 0.5635 - loss: 0.6956 - val_accuracy: 0.3600 - val_f1_score: 0.3600 - val_loss: 1.1376 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 20ms/step - accuracy: 0.5532 - f1_score: 0.5632 - loss: 0.7123 - val_accuracy: 0.3600 - val_f1_score: 0.4074 - val_loss: 1.6316 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 20ms/step - accuracy: 0.5638 - f1_score: 0.5850 - loss: 0.7059 - val_accuracy: 0.3600 - val_f1_score: 0.4074 - val_loss: 1.6393 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 20ms/step - accuracy: 0.5633 - f1_score: 0.5759 - loss: 0.7012 - val_accuracy: 0.4800 - val_f1_score: 0.3500 - val_loss: 1.1481 - learning_rate: 7.9071e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 316ms/step\nmodel2 → val F1 = 0.5714\n              precision    recall  f1-score   support\n\n        Left       0.62      0.36      0.45        28\n       Right       0.47      0.73      0.57        22\n\n    accuracy                           0.52        50\n   macro avg       0.55      0.54      0.51        50\nweighted avg       0.56      0.52      0.51        50\n\n\n>>> Training model3\nEpoch 1/200\n37/37 - 13s - 348ms/step - accuracy: 0.5063 - f1_score: 0.6501 - loss: 0.7092 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7001 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 38ms/step - accuracy: 0.5135 - f1_score: 0.5758 - loss: 0.6901 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7114 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 38ms/step - accuracy: 0.5422 - f1_score: 0.6574 - loss: 0.6808 - val_accuracy: 0.4400 - val_f1_score: 0.3913 - val_loss: 0.6682 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 38ms/step - accuracy: 0.5536 - f1_score: 0.6639 - loss: 0.6763 - val_accuracy: 0.5200 - val_f1_score: 0.6000 - val_loss: 0.8103 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 38ms/step - accuracy: 0.6309 - f1_score: 0.6791 - loss: 0.6331 - val_accuracy: 0.5000 - val_f1_score: 0.4444 - val_loss: 1.3497 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 38ms/step - accuracy: 0.7023 - f1_score: 0.6896 - loss: 0.5478 - val_accuracy: 0.5800 - val_f1_score: 0.4615 - val_loss: 2.5437 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 38ms/step - accuracy: 0.7724 - f1_score: 0.7513 - loss: 0.4691 - val_accuracy: 0.5400 - val_f1_score: 0.3030 - val_loss: 1.5719 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 38ms/step - accuracy: 0.8488 - f1_score: 0.8475 - loss: 0.3380 - val_accuracy: 0.5000 - val_f1_score: 0.5614 - val_loss: 3.7157 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 3s - 86ms/step - accuracy: 0.8687 - f1_score: 0.8643 - loss: 0.3236 - val_accuracy: 0.6400 - val_f1_score: 0.6250 - val_loss: 1.8968 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 38ms/step - accuracy: 0.9189 - f1_score: 0.9195 - loss: 0.1856 - val_accuracy: 0.5400 - val_f1_score: 0.5106 - val_loss: 2.9521 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 38ms/step - accuracy: 0.9409 - f1_score: 0.9359 - loss: 0.1738 - val_accuracy: 0.6000 - val_f1_score: 0.5833 - val_loss: 2.6392 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 3s - 85ms/step - accuracy: 0.9582 - f1_score: 0.9575 - loss: 0.1105 - val_accuracy: 0.6000 - val_f1_score: 0.6429 - val_loss: 3.2276 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 38ms/step - accuracy: 0.9726 - f1_score: 0.9732 - loss: 0.0483 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 6.0114 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 38ms/step - accuracy: 0.9730 - f1_score: 0.9737 - loss: 0.0513 - val_accuracy: 0.4400 - val_f1_score: 0.4167 - val_loss: 5.5145 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 38ms/step - accuracy: 0.9810 - f1_score: 0.9808 - loss: 0.0460 - val_accuracy: 0.4800 - val_f1_score: 0.3810 - val_loss: 4.5358 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 38ms/step - accuracy: 0.9899 - f1_score: 0.9901 - loss: 0.0259 - val_accuracy: 0.4800 - val_f1_score: 0.5000 - val_loss: 8.3589 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 38ms/step - accuracy: 0.9924 - f1_score: 0.9927 - loss: 0.0268 - val_accuracy: 0.6000 - val_f1_score: 0.5000 - val_loss: 8.1521 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 38ms/step - accuracy: 0.9949 - f1_score: 0.9949 - loss: 0.0161 - val_accuracy: 0.5400 - val_f1_score: 0.4390 - val_loss: 7.4280 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 38ms/step - accuracy: 0.9966 - f1_score: 0.9968 - loss: 0.0114 - val_accuracy: 0.5600 - val_f1_score: 0.5217 - val_loss: 7.6770 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 38ms/step - accuracy: 0.9983 - f1_score: 0.9983 - loss: 0.0046 - val_accuracy: 0.5400 - val_f1_score: 0.5106 - val_loss: 8.1828 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 38ms/step - accuracy: 0.9987 - f1_score: 0.9988 - loss: 0.0058 - val_accuracy: 0.5400 - val_f1_score: 0.5106 - val_loss: 8.0611 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 0.0012 - val_accuracy: 0.5600 - val_f1_score: 0.4762 - val_loss: 9.4811 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 38ms/step - accuracy: 0.9958 - f1_score: 0.9959 - loss: 0.0182 - val_accuracy: 0.5400 - val_f1_score: 0.5660 - val_loss: 5.8467 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 38ms/step - accuracy: 0.9945 - f1_score: 0.9945 - loss: 0.0197 - val_accuracy: 0.6600 - val_f1_score: 0.6222 - val_loss: 5.6255 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 38ms/step - accuracy: 0.9983 - f1_score: 0.9983 - loss: 0.0068 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 4.7621 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 38ms/step - accuracy: 0.9996 - f1_score: 0.9996 - loss: 0.0033 - val_accuracy: 0.6400 - val_f1_score: 0.5909 - val_loss: 6.1352 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 38ms/step - accuracy: 0.9996 - f1_score: 0.9996 - loss: 0.0028 - val_accuracy: 0.5800 - val_f1_score: 0.5333 - val_loss: 7.2528 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 0.0011 - val_accuracy: 0.5400 - val_f1_score: 0.5306 - val_loss: 6.1347 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 7.0662e-04 - val_accuracy: 0.5600 - val_f1_score: 0.5000 - val_loss: 6.8801 - learning_rate: 6.1987e-04\nEpoch 30/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 4.0628e-04 - val_accuracy: 0.5600 - val_f1_score: 0.5217 - val_loss: 7.2601 - learning_rate: 5.8827e-04\nEpoch 31/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 2.3866e-04 - val_accuracy: 0.5600 - val_f1_score: 0.5217 - val_loss: 7.5003 - learning_rate: 5.5621e-04\nEpoch 32/200\n37/37 - 1s - 38ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 2.2950e-04 - val_accuracy: 0.5800 - val_f1_score: 0.5333 - val_loss: 7.7392 - learning_rate: 5.2388e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 331ms/step\nmodel3 → val F1 = 0.6429\n              precision    recall  f1-score   support\n\n        Left       0.75      0.43      0.55        28\n       Right       0.53      0.82      0.64        22\n\n    accuracy                           0.60        50\n   macro avg       0.64      0.62      0.59        50\nweighted avg       0.65      0.60      0.59        50\n\n\n>>> Training model4\nEpoch 1/200\n37/37 - 10s - 273ms/step - accuracy: 0.5063 - f1_score: 0.3559 - loss: 0.6940 - val_accuracy: 0.4600 - val_f1_score: 0.5091 - val_loss: 0.6971 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 4s - 117ms/step - accuracy: 0.5072 - f1_score: 0.6638 - loss: 0.6933 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7028 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 4s - 116ms/step - accuracy: 0.5097 - f1_score: 0.6616 - loss: 0.6921 - val_accuracy: 0.5200 - val_f1_score: 0.6000 - val_loss: 0.6934 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 4s - 116ms/step - accuracy: 0.5351 - f1_score: 0.5731 - loss: 0.6905 - val_accuracy: 0.5400 - val_f1_score: 0.5660 - val_loss: 0.7433 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 4s - 116ms/step - accuracy: 0.5194 - f1_score: 0.5709 - loss: 0.6916 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7126 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 4s - 116ms/step - accuracy: 0.5013 - f1_score: 0.5781 - loss: 0.6935 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7109 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 4s - 116ms/step - accuracy: 0.5291 - f1_score: 0.5673 - loss: 0.6902 - val_accuracy: 0.5200 - val_f1_score: 0.4286 - val_loss: 0.7224 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 4s - 116ms/step - accuracy: 0.5317 - f1_score: 0.3870 - loss: 0.6918 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 0.6929 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 4s - 116ms/step - accuracy: 0.5198 - f1_score: 0.4953 - loss: 0.6903 - val_accuracy: 0.5400 - val_f1_score: 0.4390 - val_loss: 0.6945 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 4s - 115ms/step - accuracy: 0.5372 - f1_score: 0.4333 - loss: 0.6890 - val_accuracy: 0.5800 - val_f1_score: 0.5714 - val_loss: 0.6954 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 4s - 116ms/step - accuracy: 0.5317 - f1_score: 0.2950 - loss: 0.6915 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 0.6976 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 4s - 116ms/step - accuracy: 0.5503 - f1_score: 0.4984 - loss: 0.6870 - val_accuracy: 0.5200 - val_f1_score: 0.5862 - val_loss: 0.7076 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 4s - 116ms/step - accuracy: 0.5456 - f1_score: 0.5807 - loss: 0.6871 - val_accuracy: 0.5400 - val_f1_score: 0.5106 - val_loss: 0.7198 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 4s - 116ms/step - accuracy: 0.5486 - f1_score: 0.3888 - loss: 0.6869 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 0.6963 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 4s - 116ms/step - accuracy: 0.5211 - f1_score: 0.5299 - loss: 0.6917 - val_accuracy: 0.4200 - val_f1_score: 0.5538 - val_loss: 0.7240 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 4s - 116ms/step - accuracy: 0.5519 - f1_score: 0.5464 - loss: 0.6862 - val_accuracy: 0.5000 - val_f1_score: 0.5283 - val_loss: 0.7195 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 4s - 117ms/step - accuracy: 0.5380 - f1_score: 0.4912 - loss: 0.6903 - val_accuracy: 0.5000 - val_f1_score: 0.6269 - val_loss: 0.7685 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 4s - 116ms/step - accuracy: 0.5431 - f1_score: 0.4862 - loss: 0.6840 - val_accuracy: 0.5600 - val_f1_score: 0.4762 - val_loss: 0.6859 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 4s - 116ms/step - accuracy: 0.5427 - f1_score: 0.4415 - loss: 0.6864 - val_accuracy: 0.5400 - val_f1_score: 0.5306 - val_loss: 0.6951 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 4s - 116ms/step - accuracy: 0.5587 - f1_score: 0.4915 - loss: 0.6828 - val_accuracy: 0.4800 - val_f1_score: 0.6061 - val_loss: 0.7140 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 4s - 116ms/step - accuracy: 0.5469 - f1_score: 0.5365 - loss: 0.6844 - val_accuracy: 0.5800 - val_f1_score: 0.5882 - val_loss: 0.7060 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 4s - 116ms/step - accuracy: 0.5612 - f1_score: 0.5292 - loss: 0.6815 - val_accuracy: 0.5600 - val_f1_score: 0.5769 - val_loss: 0.7312 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 4s - 116ms/step - accuracy: 0.5570 - f1_score: 0.4949 - loss: 0.6851 - val_accuracy: 0.5400 - val_f1_score: 0.5490 - val_loss: 0.6936 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 4s - 116ms/step - accuracy: 0.5701 - f1_score: 0.5423 - loss: 0.6831 - val_accuracy: 0.6000 - val_f1_score: 0.5652 - val_loss: 0.7070 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 4s - 116ms/step - accuracy: 0.5747 - f1_score: 0.4769 - loss: 0.6773 - val_accuracy: 0.5200 - val_f1_score: 0.5556 - val_loss: 0.7104 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 4s - 116ms/step - accuracy: 0.5553 - f1_score: 0.5589 - loss: 0.6817 - val_accuracy: 0.5000 - val_f1_score: 0.6032 - val_loss: 0.7600 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 4s - 116ms/step - accuracy: 0.5790 - f1_score: 0.5946 - loss: 0.6700 - val_accuracy: 0.6200 - val_f1_score: 0.4571 - val_loss: 0.6858 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 4s - 116ms/step - accuracy: 0.5701 - f1_score: 0.5356 - loss: 0.6675 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 0.7280 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 4s - 116ms/step - accuracy: 0.5815 - f1_score: 0.5371 - loss: 0.6676 - val_accuracy: 0.5000 - val_f1_score: 0.5455 - val_loss: 0.7600 - learning_rate: 6.1987e-04\nEpoch 30/200\n37/37 - 4s - 115ms/step - accuracy: 0.5705 - f1_score: 0.5998 - loss: 0.6678 - val_accuracy: 0.5200 - val_f1_score: 0.5862 - val_loss: 0.7498 - learning_rate: 5.8827e-04\nEpoch 31/200\n37/37 - 4s - 115ms/step - accuracy: 0.5883 - f1_score: 0.5399 - loss: 0.6636 - val_accuracy: 0.5200 - val_f1_score: 0.3684 - val_loss: 0.6941 - learning_rate: 5.5621e-04\nEpoch 32/200\n37/37 - 4s - 115ms/step - accuracy: 0.5912 - f1_score: 0.5556 - loss: 0.6605 - val_accuracy: 0.5600 - val_f1_score: 0.5000 - val_loss: 0.7317 - learning_rate: 5.2388e-04\nEpoch 33/200\n37/37 - 4s - 116ms/step - accuracy: 0.5976 - f1_score: 0.5997 - loss: 0.6574 - val_accuracy: 0.6000 - val_f1_score: 0.5833 - val_loss: 0.6429 - learning_rate: 4.9148e-04\nEpoch 34/200\n37/37 - 4s - 116ms/step - accuracy: 0.6001 - f1_score: 0.5527 - loss: 0.6532 - val_accuracy: 0.5800 - val_f1_score: 0.6038 - val_loss: 0.7569 - learning_rate: 4.5920e-04\nEpoch 35/200\n37/37 - 4s - 117ms/step - accuracy: 0.6030 - f1_score: 0.6197 - loss: 0.6529 - val_accuracy: 0.6000 - val_f1_score: 0.6296 - val_loss: 0.7148 - learning_rate: 4.2723e-04\nEpoch 36/200\n37/37 - 4s - 116ms/step - accuracy: 0.6237 - f1_score: 0.6437 - loss: 0.6315 - val_accuracy: 0.6000 - val_f1_score: 0.6296 - val_loss: 0.7271 - learning_rate: 3.9575e-04\nEpoch 37/200\n37/37 - 4s - 116ms/step - accuracy: 0.6377 - f1_score: 0.6449 - loss: 0.6216 - val_accuracy: 0.5400 - val_f1_score: 0.5965 - val_loss: 0.7713 - learning_rate: 3.6494e-04\nEpoch 38/200\n37/37 - 4s - 116ms/step - accuracy: 0.6187 - f1_score: 0.6334 - loss: 0.6400 - val_accuracy: 0.5400 - val_f1_score: 0.5660 - val_loss: 0.7570 - learning_rate: 3.3498e-04\nEpoch 39/200\n37/37 - 4s - 116ms/step - accuracy: 0.6258 - f1_score: 0.6302 - loss: 0.6277 - val_accuracy: 0.5400 - val_f1_score: 0.4390 - val_loss: 0.7427 - learning_rate: 3.0602e-04\nEpoch 40/200\n37/37 - 4s - 115ms/step - accuracy: 0.6309 - f1_score: 0.6109 - loss: 0.6178 - val_accuracy: 0.5000 - val_f1_score: 0.5283 - val_loss: 0.7477 - learning_rate: 2.7820e-04\nEpoch 41/200\n37/37 - 4s - 116ms/step - accuracy: 0.6529 - f1_score: 0.6631 - loss: 0.5997 - val_accuracy: 0.5000 - val_f1_score: 0.5455 - val_loss: 0.8337 - learning_rate: 2.5163e-04\nEpoch 42/200\n37/37 - 4s - 117ms/step - accuracy: 0.6571 - f1_score: 0.6775 - loss: 0.6022 - val_accuracy: 0.5800 - val_f1_score: 0.6182 - val_loss: 0.6962 - learning_rate: 2.2643e-04\nEpoch 43/200\n37/37 - 4s - 117ms/step - accuracy: 0.6774 - f1_score: 0.6882 - loss: 0.5768 - val_accuracy: 0.5600 - val_f1_score: 0.6071 - val_loss: 0.8603 - learning_rate: 2.0267e-04\nEpoch 44/200\n37/37 - 4s - 116ms/step - accuracy: 0.6681 - f1_score: 0.6736 - loss: 0.5900 - val_accuracy: 0.5200 - val_f1_score: 0.5556 - val_loss: 0.8112 - learning_rate: 1.8042e-04\nEpoch 45/200\n37/37 - 4s - 116ms/step - accuracy: 0.6799 - f1_score: 0.7023 - loss: 0.5764 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.8372 - learning_rate: 1.5972e-04\nEpoch 46/200\n37/37 - 4s - 116ms/step - accuracy: 0.6833 - f1_score: 0.6906 - loss: 0.5614 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.8559 - learning_rate: 1.4058e-04\nEpoch 47/200\n37/37 - 4s - 116ms/step - accuracy: 0.7006 - f1_score: 0.7133 - loss: 0.5550 - val_accuracy: 0.4800 - val_f1_score: 0.5185 - val_loss: 0.8843 - learning_rate: 1.2302e-04\nEpoch 48/200\n37/37 - 4s - 116ms/step - accuracy: 0.7124 - f1_score: 0.7201 - loss: 0.5372 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.9146 - learning_rate: 1.0700e-04\nEpoch 49/200\n37/37 - 4s - 117ms/step - accuracy: 0.7048 - f1_score: 0.7245 - loss: 0.5459 - val_accuracy: 0.5000 - val_f1_score: 0.5283 - val_loss: 0.8783 - learning_rate: 9.2503e-05\nEpoch 50/200\n37/37 - 4s - 116ms/step - accuracy: 0.7120 - f1_score: 0.7330 - loss: 0.5339 - val_accuracy: 0.5200 - val_f1_score: 0.5556 - val_loss: 0.9212 - learning_rate: 7.9466e-05\nEpoch 51/200\n37/37 - 4s - 115ms/step - accuracy: 0.7213 - f1_score: 0.7234 - loss: 0.5320 - val_accuracy: 0.5200 - val_f1_score: 0.5556 - val_loss: 0.9166 - learning_rate: 6.7829e-05\nEpoch 52/200\n37/37 - 4s - 116ms/step - accuracy: 0.7052 - f1_score: 0.7042 - loss: 0.5292 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.8530 - learning_rate: 5.7516e-05\nEpoch 53/200\n37/37 - 4s - 115ms/step - accuracy: 0.7251 - f1_score: 0.7355 - loss: 0.5226 - val_accuracy: 0.5400 - val_f1_score: 0.5660 - val_loss: 0.9072 - learning_rate: 4.8444e-05\nEpoch 54/200\n37/37 - 4s - 116ms/step - accuracy: 0.7120 - f1_score: 0.7241 - loss: 0.5233 - val_accuracy: 0.5400 - val_f1_score: 0.5660 - val_loss: 0.9027 - learning_rate: 4.0524e-05\nEpoch 55/200\n37/37 - 4s - 116ms/step - accuracy: 0.7213 - f1_score: 0.7238 - loss: 0.5191 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.8981 - learning_rate: 3.3661e-05\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 262ms/step\nmodel4 → val F1 = 0.6296\n              precision    recall  f1-score   support\n\n        Left       0.72      0.46      0.57        28\n       Right       0.53      0.77      0.63        22\n\n    accuracy                           0.60        50\n   macro avg       0.63      0.62      0.60        50\nweighted avg       0.64      0.60      0.59        50\n\n\n>>> Training model5\nEpoch 1/200\n37/37 - 10s - 270ms/step - accuracy: 0.5093 - f1_score: 0.5432 - loss: 1.0884 - val_accuracy: 0.6000 - val_f1_score: 0.3333 - val_loss: 0.8883 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 38ms/step - accuracy: 0.5008 - f1_score: 0.5017 - loss: 0.8488 - val_accuracy: 0.4200 - val_f1_score: 0.5538 - val_loss: 0.8954 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 23ms/step - accuracy: 0.4916 - f1_score: 0.5070 - loss: 0.7676 - val_accuracy: 0.5000 - val_f1_score: 0.2424 - val_loss: 0.7472 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 23ms/step - accuracy: 0.5101 - f1_score: 0.5353 - loss: 0.7241 - val_accuracy: 0.5800 - val_f1_score: 0.3636 - val_loss: 0.6636 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 24ms/step - accuracy: 0.5207 - f1_score: 0.4967 - loss: 0.7030 - val_accuracy: 0.5800 - val_f1_score: 0.2222 - val_loss: 0.6507 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 23ms/step - accuracy: 0.5570 - f1_score: 0.5923 - loss: 0.6776 - val_accuracy: 0.4200 - val_f1_score: 0.4912 - val_loss: 0.7796 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 37ms/step - accuracy: 0.6394 - f1_score: 0.6497 - loss: 0.6303 - val_accuracy: 0.5800 - val_f1_score: 0.5882 - val_loss: 0.7662 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 22ms/step - accuracy: 0.7158 - f1_score: 0.7080 - loss: 0.5285 - val_accuracy: 0.5400 - val_f1_score: 0.5818 - val_loss: 0.9316 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 22ms/step - accuracy: 0.7817 - f1_score: 0.7865 - loss: 0.4482 - val_accuracy: 0.5800 - val_f1_score: 0.5532 - val_loss: 0.9238 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 22ms/step - accuracy: 0.8476 - f1_score: 0.8526 - loss: 0.3471 - val_accuracy: 0.5800 - val_f1_score: 0.5714 - val_loss: 1.1161 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 23ms/step - accuracy: 0.8877 - f1_score: 0.8899 - loss: 0.2605 - val_accuracy: 0.5000 - val_f1_score: 0.4898 - val_loss: 1.6031 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 21ms/step - accuracy: 0.9075 - f1_score: 0.9091 - loss: 0.2328 - val_accuracy: 0.5400 - val_f1_score: 0.4889 - val_loss: 1.4265 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 26ms/step - accuracy: 0.9333 - f1_score: 0.9371 - loss: 0.1824 - val_accuracy: 0.5800 - val_f1_score: 0.5333 - val_loss: 1.8672 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 26ms/step - accuracy: 0.9257 - f1_score: 0.9262 - loss: 0.1906 - val_accuracy: 0.5600 - val_f1_score: 0.5600 - val_loss: 2.5529 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 22ms/step - accuracy: 0.9468 - f1_score: 0.9477 - loss: 0.1480 - val_accuracy: 0.5000 - val_f1_score: 0.5614 - val_loss: 2.6183 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 22ms/step - accuracy: 0.9662 - f1_score: 0.9673 - loss: 0.0949 - val_accuracy: 0.5400 - val_f1_score: 0.4390 - val_loss: 2.6048 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 22ms/step - accuracy: 0.9683 - f1_score: 0.9686 - loss: 0.0691 - val_accuracy: 0.5400 - val_f1_score: 0.4651 - val_loss: 1.6909 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 36ms/step - accuracy: 0.9814 - f1_score: 0.9812 - loss: 0.0485 - val_accuracy: 0.5800 - val_f1_score: 0.6182 - val_loss: 2.7288 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 22ms/step - accuracy: 0.9894 - f1_score: 0.9897 - loss: 0.0277 - val_accuracy: 0.6200 - val_f1_score: 0.5957 - val_loss: 2.8026 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 22ms/step - accuracy: 0.9920 - f1_score: 0.9920 - loss: 0.0267 - val_accuracy: 0.5000 - val_f1_score: 0.4681 - val_loss: 3.6147 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 22ms/step - accuracy: 0.9865 - f1_score: 0.9863 - loss: 0.0445 - val_accuracy: 0.5400 - val_f1_score: 0.5490 - val_loss: 4.0508 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 21ms/step - accuracy: 0.9856 - f1_score: 0.9859 - loss: 0.0420 - val_accuracy: 0.6200 - val_f1_score: 0.5957 - val_loss: 3.3707 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 35ms/step - accuracy: 0.9937 - f1_score: 0.9935 - loss: 0.0255 - val_accuracy: 0.6200 - val_f1_score: 0.6275 - val_loss: 3.3647 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 22ms/step - accuracy: 0.9907 - f1_score: 0.9908 - loss: 0.0293 - val_accuracy: 0.5600 - val_f1_score: 0.5769 - val_loss: 4.1584 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 22ms/step - accuracy: 0.9894 - f1_score: 0.9894 - loss: 0.0223 - val_accuracy: 0.5600 - val_f1_score: 0.5217 - val_loss: 4.9624 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 22ms/step - accuracy: 0.9966 - f1_score: 0.9965 - loss: 0.0087 - val_accuracy: 0.6200 - val_f1_score: 0.6122 - val_loss: 4.6552 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 21ms/step - accuracy: 0.9975 - f1_score: 0.9976 - loss: 0.0103 - val_accuracy: 0.6400 - val_f1_score: 0.6250 - val_loss: 4.9598 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 22ms/step - accuracy: 0.9966 - f1_score: 0.9967 - loss: 0.0068 - val_accuracy: 0.6200 - val_f1_score: 0.6122 - val_loss: 4.4716 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 1s - 22ms/step - accuracy: 0.9958 - f1_score: 0.9958 - loss: 0.0123 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 4.9042 - learning_rate: 6.1987e-04\nEpoch 30/200\n37/37 - 1s - 21ms/step - accuracy: 0.9983 - f1_score: 0.9983 - loss: 0.0069 - val_accuracy: 0.6000 - val_f1_score: 0.5833 - val_loss: 5.3680 - learning_rate: 5.8827e-04\nEpoch 31/200\n37/37 - 1s - 22ms/step - accuracy: 0.9962 - f1_score: 0.9963 - loss: 0.0128 - val_accuracy: 0.6000 - val_f1_score: 0.5833 - val_loss: 4.5969 - learning_rate: 5.5621e-04\nEpoch 32/200\n37/37 - 1s - 23ms/step - accuracy: 0.9966 - f1_score: 0.9966 - loss: 0.0117 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 4.6011 - learning_rate: 5.2388e-04\nEpoch 33/200\n37/37 - 1s - 21ms/step - accuracy: 0.9975 - f1_score: 0.9976 - loss: 0.0053 - val_accuracy: 0.6400 - val_f1_score: 0.6250 - val_loss: 4.6699 - learning_rate: 4.9148e-04\nEpoch 34/200\n37/37 - 1s - 36ms/step - accuracy: 0.9992 - f1_score: 0.9992 - loss: 0.0024 - val_accuracy: 0.6400 - val_f1_score: 0.6400 - val_loss: 4.7212 - learning_rate: 4.5920e-04\nEpoch 35/200\n37/37 - 1s - 21ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 0.0013 - val_accuracy: 0.6400 - val_f1_score: 0.6400 - val_loss: 4.8034 - learning_rate: 4.2723e-04\nEpoch 36/200\n37/37 - 1s - 21ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 0.0011 - val_accuracy: 0.6200 - val_f1_score: 0.6122 - val_loss: 4.8903 - learning_rate: 3.9575e-04\nEpoch 37/200\n37/37 - 1s - 21ms/step - accuracy: 0.9996 - f1_score: 0.9996 - loss: 0.0012 - val_accuracy: 0.6200 - val_f1_score: 0.6275 - val_loss: 4.9683 - learning_rate: 3.6494e-04\nEpoch 38/200\n37/37 - 1s - 21ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 6.7640e-04 - val_accuracy: 0.6200 - val_f1_score: 0.6122 - val_loss: 5.0170 - learning_rate: 3.3498e-04\nEpoch 39/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 4.3338e-04 - val_accuracy: 0.6200 - val_f1_score: 0.6122 - val_loss: 5.0541 - learning_rate: 3.0602e-04\nEpoch 40/200\n37/37 - 1s - 21ms/step - accuracy: 0.9996 - f1_score: 0.9996 - loss: 0.0011 - val_accuracy: 0.5800 - val_f1_score: 0.5882 - val_loss: 5.1081 - learning_rate: 2.7820e-04\nEpoch 41/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 3.3543e-04 - val_accuracy: 0.5800 - val_f1_score: 0.5882 - val_loss: 5.1227 - learning_rate: 2.5163e-04\nEpoch 42/200\n37/37 - 1s - 23ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 3.7878e-04 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 5.1400 - learning_rate: 2.2643e-04\nEpoch 43/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 7.4882e-04 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 5.1455 - learning_rate: 2.0267e-04\nEpoch 44/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 4.4871e-04 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 5.1687 - learning_rate: 1.8042e-04\nEpoch 45/200\n37/37 - 1s - 23ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 3.1486e-04 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 5.1822 - learning_rate: 1.5972e-04\nEpoch 46/200\n37/37 - 1s - 23ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 2.9351e-04 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 5.1923 - learning_rate: 1.4058e-04\nEpoch 47/200\n37/37 - 1s - 21ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 3.0617e-04 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 5.2031 - learning_rate: 1.2302e-04\nEpoch 48/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 2.4806e-04 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 5.2111 - learning_rate: 1.0700e-04\nEpoch 49/200\n37/37 - 1s - 23ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 3.8162e-04 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 5.2063 - learning_rate: 9.2503e-05\nEpoch 50/200\n37/37 - 1s - 26ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 2.5968e-04 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 5.2091 - learning_rate: 7.9466e-05\nEpoch 51/200\n37/37 - 1s - 23ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 2.7088e-04 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 5.2173 - learning_rate: 6.7829e-05\nEpoch 52/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 4.0686e-04 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 5.2258 - learning_rate: 5.7516e-05\nEpoch 53/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 2.2704e-04 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 5.2317 - learning_rate: 4.8444e-05\nEpoch 54/200\n37/37 - 1s - 22ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 4.1122e-04 - val_accuracy: 0.6000 - val_f1_score: 0.6000 - val_loss: 5.2351 - learning_rate: 4.0524e-05\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 431ms/step\nmodel5 → val F1 = 0.6400\n              precision    recall  f1-score   support\n\n        Left       0.73      0.57      0.64        28\n       Right       0.57      0.73      0.64        22\n\n    accuracy                           0.64        50\n   macro avg       0.65      0.65      0.64        50\nweighted avg       0.66      0.64      0.64        50\n\n\n>>> Training model6\nEpoch 1/200\n37/37 - 8s - 222ms/step - accuracy: 0.4970 - f1_score: 0.5246 - loss: 2.9897 - val_accuracy: 0.4400 - val_f1_score: 0.4400 - val_loss: 1.7963 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 38ms/step - accuracy: 0.5274 - f1_score: 0.5668 - loss: 1.1364 - val_accuracy: 0.4400 - val_f1_score: 0.1765 - val_loss: 1.9389 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 39ms/step - accuracy: 0.5156 - f1_score: 0.5266 - loss: 0.8270 - val_accuracy: 0.5600 - val_f1_score: 0.5600 - val_loss: 1.4828 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 38ms/step - accuracy: 0.5182 - f1_score: 0.4872 - loss: 0.7572 - val_accuracy: 0.4400 - val_f1_score: 0.3913 - val_loss: 1.1585 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 37ms/step - accuracy: 0.5093 - f1_score: 0.5222 - loss: 0.7380 - val_accuracy: 0.5000 - val_f1_score: 0.3243 - val_loss: 0.9512 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 37ms/step - accuracy: 0.4856 - f1_score: 0.4795 - loss: 0.7089 - val_accuracy: 0.4600 - val_f1_score: 0.5091 - val_loss: 0.7161 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 39ms/step - accuracy: 0.5055 - f1_score: 0.5098 - loss: 0.7185 - val_accuracy: 0.4200 - val_f1_score: 0.5797 - val_loss: 0.9874 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 37ms/step - accuracy: 0.5084 - f1_score: 0.4991 - loss: 0.7088 - val_accuracy: 0.4800 - val_f1_score: 0.2353 - val_loss: 0.7613 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 39ms/step - accuracy: 0.4958 - f1_score: 0.4862 - loss: 0.7205 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7276 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 38ms/step - accuracy: 0.5084 - f1_score: 0.4785 - loss: 0.7033 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7653 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 37ms/step - accuracy: 0.5262 - f1_score: 0.6139 - loss: 0.7005 - val_accuracy: 0.5000 - val_f1_score: 0.5763 - val_loss: 0.7070 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 37ms/step - accuracy: 0.4970 - f1_score: 0.6211 - loss: 0.6966 - val_accuracy: 0.4400 - val_f1_score: 0.4400 - val_loss: 0.7034 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 37ms/step - accuracy: 0.5118 - f1_score: 0.5924 - loss: 0.6987 - val_accuracy: 0.4200 - val_f1_score: 0.1212 - val_loss: 0.7306 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 38ms/step - accuracy: 0.5021 - f1_score: 0.5526 - loss: 0.7075 - val_accuracy: 0.4600 - val_f1_score: 0.4906 - val_loss: 0.7012 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 38ms/step - accuracy: 0.5072 - f1_score: 0.5961 - loss: 0.7023 - val_accuracy: 0.3000 - val_f1_score: 0.3137 - val_loss: 0.8596 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 39ms/step - accuracy: 0.4949 - f1_score: 0.5600 - loss: 0.7281 - val_accuracy: 0.5000 - val_f1_score: 0.6154 - val_loss: 0.7697 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 38ms/step - accuracy: 0.5013 - f1_score: 0.4994 - loss: 0.7216 - val_accuracy: 0.5400 - val_f1_score: 0.0800 - val_loss: 0.7308 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 39ms/step - accuracy: 0.5114 - f1_score: 0.4616 - loss: 0.7045 - val_accuracy: 0.4800 - val_f1_score: 0.6286 - val_loss: 0.7040 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 38ms/step - accuracy: 0.4941 - f1_score: 0.5985 - loss: 0.6929 - val_accuracy: 0.4600 - val_f1_score: 0.5574 - val_loss: 0.7128 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 38ms/step - accuracy: 0.5165 - f1_score: 0.5362 - loss: 0.6938 - val_accuracy: 0.3800 - val_f1_score: 0.3673 - val_loss: 0.7021 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 37ms/step - accuracy: 0.5148 - f1_score: 0.4905 - loss: 0.6928 - val_accuracy: 0.5000 - val_f1_score: 0.1379 - val_loss: 0.7288 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 38ms/step - accuracy: 0.5034 - f1_score: 0.4079 - loss: 0.6945 - val_accuracy: 0.5800 - val_f1_score: 0.4615 - val_loss: 0.6912 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 38ms/step - accuracy: 0.5156 - f1_score: 0.2950 - loss: 0.6931 - val_accuracy: 0.6400 - val_f1_score: 0.5000 - val_loss: 0.7011 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 38ms/step - accuracy: 0.5291 - f1_score: 0.3468 - loss: 0.6909 - val_accuracy: 0.5400 - val_f1_score: 0.5660 - val_loss: 0.7095 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 38ms/step - accuracy: 0.5114 - f1_score: 0.4033 - loss: 0.6914 - val_accuracy: 0.5400 - val_f1_score: 0.2069 - val_loss: 0.6840 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 37ms/step - accuracy: 0.5076 - f1_score: 0.4505 - loss: 0.6920 - val_accuracy: 0.5600 - val_f1_score: 0.3889 - val_loss: 0.7277 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 37ms/step - accuracy: 0.5203 - f1_score: 0.2556 - loss: 0.6919 - val_accuracy: 0.4600 - val_f1_score: 0.3077 - val_loss: 0.7342 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 37ms/step - accuracy: 0.5236 - f1_score: 0.5862 - loss: 0.6913 - val_accuracy: 0.6600 - val_f1_score: 0.5405 - val_loss: 0.6547 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 1s - 37ms/step - accuracy: 0.5165 - f1_score: 0.5661 - loss: 0.6919 - val_accuracy: 0.6200 - val_f1_score: 0.5778 - val_loss: 0.7149 - learning_rate: 6.1987e-04\nEpoch 30/200\n37/37 - 1s - 37ms/step - accuracy: 0.5405 - f1_score: 0.4944 - loss: 0.6894 - val_accuracy: 0.5200 - val_f1_score: 0.4783 - val_loss: 0.6887 - learning_rate: 5.8827e-04\nEpoch 31/200\n37/37 - 1s - 38ms/step - accuracy: 0.4996 - f1_score: 0.5384 - loss: 0.6941 - val_accuracy: 0.5600 - val_f1_score: 0.4762 - val_loss: 0.6987 - learning_rate: 5.5621e-04\nEpoch 32/200\n37/37 - 1s - 38ms/step - accuracy: 0.5169 - f1_score: 0.6352 - loss: 0.6931 - val_accuracy: 0.4600 - val_f1_score: 0.5424 - val_loss: 0.7099 - learning_rate: 5.2388e-04\nEpoch 33/200\n37/37 - 1s - 37ms/step - accuracy: 0.5118 - f1_score: 0.6486 - loss: 0.6903 - val_accuracy: 0.5400 - val_f1_score: 0.4889 - val_loss: 0.7025 - learning_rate: 4.9148e-04\nEpoch 34/200\n37/37 - 1s - 38ms/step - accuracy: 0.5274 - f1_score: 0.6200 - loss: 0.6893 - val_accuracy: 0.4600 - val_f1_score: 0.4706 - val_loss: 0.6965 - learning_rate: 4.5920e-04\nEpoch 35/200\n37/37 - 1s - 37ms/step - accuracy: 0.4903 - f1_score: 0.6152 - loss: 0.6907 - val_accuracy: 0.4800 - val_f1_score: 0.5937 - val_loss: 0.7152 - learning_rate: 4.2723e-04\nEpoch 36/200\n37/37 - 1s - 37ms/step - accuracy: 0.5084 - f1_score: 0.4249 - loss: 0.6936 - val_accuracy: 0.5800 - val_f1_score: 0.4878 - val_loss: 0.6917 - learning_rate: 3.9575e-04\nEpoch 37/200\n37/37 - 1s - 38ms/step - accuracy: 0.5266 - f1_score: 0.3796 - loss: 0.6914 - val_accuracy: 0.4400 - val_f1_score: 0.2632 - val_loss: 0.6965 - learning_rate: 3.6494e-04\nEpoch 38/200\n37/37 - 1s - 38ms/step - accuracy: 0.5270 - f1_score: 0.5421 - loss: 0.6917 - val_accuracy: 0.4800 - val_f1_score: 0.5937 - val_loss: 0.6841 - learning_rate: 3.3498e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 459ms/step\nmodel6 → val F1 = 0.6286\n              precision    recall  f1-score   support\n\n        Left       1.00      0.07      0.13        28\n       Right       0.46      1.00      0.63        22\n\n    accuracy                           0.48        50\n   macro avg       0.73      0.54      0.38        50\nweighted avg       0.76      0.48      0.35        50\n\n\n=== Final best: oldB (F1=0.6792) ===\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\nimport tensorflow as tf\n\n# Function to compute best threshold for a model\ndef find_best_threshold(model, val_x, y_true_bin):\n    probs = model.predict(val_x)[:, 1]\n    thresholds = np.linspace(0.1, 1.0, 100)\n    best_f1 = 0\n    best_threshold = 0.5\n    for t in thresholds:\n        preds = (probs > t).astype(int)\n        f1 = f1_score(y_true_bin, preds)\n        if f1 > best_f1:\n            best_f1 = f1\n            best_threshold = t\n    return best_threshold, best_f1\n\n# Load validation data\nval_npz = np.load(\"/kaggle/working/preprocessed/validation_MI.npz\")\nX_val_all, y_val = val_npz[\"X\"], val_npz[\"y\"]\ny_val_bin = (y_val == \"Right\").astype(int)\n\n# Use only EEG channels: C3, CZ, C4, PZ\neeg_indices = [1, 2, 3, 4]\nX_val_raw = X_val_all[:, eeg_indices, :].transpose(0, 2, 1).astype('float32')\n\n# Directories to check\ndirs = [\"/kaggle/working/models\", \"/kaggle/working/models2\", \"/kaggle/working/models3\", \n       \"/kaggle/input/model5-old\", \"/kaggle/input/model6\"]\n# \n# Loop through models\nresults = {}\nfor model_dir in dirs:\n    for file in os.listdir(model_dir):\n        if file.endswith(\".h5\"):\n            path = os.path.join(model_dir, file)\n            try:\n                model = tf.keras.models.load_model(path, compile=False)\n                threshold, f1 = find_best_threshold(model, X_val_raw, y_val_bin)\n                results[path] = (threshold, f1)\n            except Exception as e:\n                results[path] = f\"Error: {e}\"\n\n# Print results\nfor model_path, (threshold, f1) in results.items():\n    print(f\"{model_path} → Best threshold = {threshold:.2f}, F1 score = {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T11:37:10.433618Z","iopub.execute_input":"2025-06-29T11:37:10.433901Z","iopub.status.idle":"2025-06-29T11:38:12.212420Z","shell.execute_reply.started":"2025-06-29T11:37:10.433878Z","shell.execute_reply":"2025-06-29T11:38:12.211574Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 603ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 385ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 408ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 592ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 381ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 412ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 618ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 390ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 264ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 413ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 286ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 632ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 396ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 361ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 305ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 386ms/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2517885931.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/model5-old'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/model5-old'","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"    import os\n    import numpy as np\n    from sklearn.metrics import f1_score\n    import tensorflow as tf\n    \n    # Function to compute best threshold for a model\n    def find_best_threshold(model, val_x, y_true_bin):\n        probs = model.predict(val_x)[:, 1]\n        thresholds = np.linspace(0.1, 1.0, 100)\n        best_f1 = 0\n        best_threshold = 0.5\n        \n        for t in thresholds:\n            preds_bin = (probs > t).astype(int)\n            # Use macro averaging here\n            f1 = f1_score(y_true_bin, preds_bin, average='macro')  # Changed to macro\n            if f1 > best_f1:\n                best_f1 = f1\n                best_threshold = t\n                \n        return best_threshold, best_f1\n    \n    # Load validation data\n    val_npz = np.load(\"/kaggle/working/preprocessed/validation_MI.npz\")\n    X_val_all, y_val = val_npz[\"X\"], val_npz[\"y\"]\n    y_val_bin = (y_val == \"Right\").astype(int)\n    \n    # Use only EEG channels: C3, CZ, C4, PZ\n    eeg_indices = [1, 2, 3, 4]\n    X_val_raw = X_val_all[:, eeg_indices, :].transpose(0, 2, 1).astype('float32')\n    \n    # Directories to check\n    dirs = [\"/kaggle/working/models\", \"/kaggle/working/models2\", \"/kaggle/working/models3\", \n           \"/kaggle/input/model5-old\", \"/kaggle/input/model6\"]\n    # \n    # Loop through models\n    results = {}\n    for model_dir in dirs:\n        for file in os.listdir(model_dir):\n            if file.endswith(\".h5\"):\n                path = os.path.join(model_dir, file)\n                try:\n                    model = tf.keras.models.load_model(path, compile=False)\n                    threshold, f1 = find_best_threshold(model, X_val_raw, y_val_bin)\n                    results[path] = (threshold, f1)\n                except Exception as e:\n                    results[path] = f\"Error: {e}\"\n    \n    # Print results\n    for model_path, (threshold, f1) in results.items():\n        print(f\"{model_path} → Best threshold = {threshold:.2f}, F1 score = {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T11:38:12.212988Z","iopub.status.idle":"2025-06-29T11:38:12.213248Z","shell.execute_reply.started":"2025-06-29T11:38:12.213125Z","shell.execute_reply":"2025-06-29T11:38:12.213136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.metrics import f1_score, classification_report\nimport tensorflow as tf\n\n# Function to compute best threshold and generate classification report\ndef evaluate_model(model, val_x, y_true_str):\n    # Convert true labels to binary (Right=1, Left=0)\n    y_true_bin = np.array([1 if label == \"Right\" else 0 for label in y_true_str])\n    \n    # Get prediction probabilities for class 1 (Right)\n    probs = model.predict(val_x, verbose=0)[:, 1]\n    \n    # Find best threshold\n    thresholds = np.linspace(0.1, 1.0, 100)\n    best_f1 = 0\n    best_threshold = 0.5\n    for t in thresholds:\n        preds_bin = (probs > t).astype(int)\n        f1 = f1_score(y_true_bin, preds_bin)\n        if f1 > best_f1:\n            best_f1 = f1\n            best_threshold = t\n    \n    # Generate predictions using best threshold\n    pred_labels = [\"Right\" if prob > best_threshold else \"Left\" for prob in probs]\n    \n    # Generate classification report\n    report = classification_report(\n        y_true_str, \n        pred_labels,\n        target_names=[\"Left\", \"Right\"],\n        digits=4\n    )\n    \n    return best_threshold, best_f1, report\n\n# Load validation data\nval_npz = np.load(\"/kaggle/working/preprocessed/validation_MI.npz\")\nX_val_all, y_val = val_npz[\"X\"], val_npz[\"y\"]\n\n# Use only EEG channels: C3, CZ, C4, PZ\neeg_indices = [1, 2, 3, 4]  # Update if your channel order differs\nX_val_raw = X_val_all[:, eeg_indices, :].transpose(0, 2, 1).astype('float32')\n\n# Directories to check\ndirs = [\n    \"/kaggle/working/models3\", \"/kaggle/working/models2\", \"/kaggle/working/models1\"]\n\n# Evaluate models\nresults = {}\nfor model_dir in dirs:\n    for file in os.listdir(model_dir):\n        if file.endswith(\".h5\"):\n            path = os.path.join(model_dir, file)\n            try:\n                model = tf.keras.models.load_model(path, compile=False)\n                threshold, f1, report = evaluate_model(model, X_val_raw, y_val)\n                results[path] = {\n                    \"threshold\": threshold,\n                    \"f1\": f1,\n                    \"report\": report\n                }\n            except Exception as e:\n                results[path] = f\"Error: {e}\"\n\n# Print results with classification reports\nfor path, result in results.items():\n    if isinstance(result, str):\n        print(f\"\\n{path} → {result}\")\n    else:\n        print(f\"\\n{'-'*80}\")\n        print(f\"Model: {path}\")\n        print(f\"Best threshold: {result['threshold']:.4f}\")\n        print(f\"Best F1-score: {result['f1']:.4f}\")\n        print(\"\\nClassification Report:\")\n        print(result['report'])\n        print(f\"{'-'*80}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:46:35.587622Z","iopub.execute_input":"2025-06-29T10:46:35.588198Z","iopub.status.idle":"2025-06-29T10:46:55.000647Z","shell.execute_reply.started":"2025-06-29T10:46:35.588176Z","shell.execute_reply":"2025-06-29T10:46:54.999837Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_model1.h5\nBest threshold: 0.4091\nBest F1-score: 0.6377\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     1.0000    0.1071    0.1935        28\n       Right     0.4681    1.0000    0.6377        22\n\n    accuracy                         0.5000        50\n   macro avg     0.7340    0.5536    0.4156        50\nweighted avg     0.7660    0.5000    0.3890        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_model6.h5\nBest threshold: 0.1000\nBest F1-score: 0.6111\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     0.0000    0.0000    0.0000        28\n       Right     0.4400    1.0000    0.6111        22\n\n    accuracy                         0.4400        50\n   macro avg     0.2200    0.5000    0.3056        50\nweighted avg     0.1936    0.4400    0.2689        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_model2.h5\nBest threshold: 0.5636\nBest F1-score: 0.6909\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     0.8235    0.5000    0.6222        28\n       Right     0.5758    0.8636    0.6909        22\n\n    accuracy                         0.6600        50\n   macro avg     0.6996    0.6818    0.6566        50\nweighted avg     0.7145    0.6600    0.6524        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_oldB.h5\nBest threshold: 0.5000\nBest F1-score: 0.6176\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     0.7500    0.1071    0.1875        28\n       Right     0.4565    0.9545    0.6176        22\n\n    accuracy                         0.4800        50\n   macro avg     0.6033    0.5308    0.4026        50\nweighted avg     0.6209    0.4800    0.3768        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_model5.h5\nBest threshold: 0.1000\nBest F1-score: 0.6111\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     0.0000    0.0000    0.0000        28\n       Right     0.4400    1.0000    0.6111        22\n\n    accuracy                         0.4400        50\n   macro avg     0.2200    0.5000    0.3056        50\nweighted avg     0.1936    0.4400    0.2689        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_final.h5\nBest threshold: 0.5273\nBest F1-score: 0.6897\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     0.8571    0.4286    0.5714        28\n       Right     0.5556    0.9091    0.6897        22\n\n    accuracy                         0.6400        50\n   macro avg     0.7063    0.6688    0.6305        50\nweighted avg     0.7244    0.6400    0.6234        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_model3.h5\nBest threshold: 0.1000\nBest F1-score: 0.4865\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     0.6286    0.7857    0.6984        28\n       Right     0.6000    0.4091    0.4865        22\n\n    accuracy                         0.6200        50\n   macro avg     0.6143    0.5974    0.5924        50\nweighted avg     0.6160    0.6200    0.6052        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_model4.h5\nBest threshold: 0.1000\nBest F1-score: 0.6111\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     0.0000    0.0000    0.0000        28\n       Right     0.4400    1.0000    0.6111        22\n\n    accuracy                         0.4400        50\n   macro avg     0.2200    0.5000    0.3056        50\nweighted avg     0.1936    0.4400    0.2689        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_oldA.h5\nBest threshold: 0.5273\nBest F1-score: 0.6897\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     0.8571    0.4286    0.5714        28\n       Right     0.5556    0.9091    0.6897        22\n\n    accuracy                         0.6400        50\n   macro avg     0.7063    0.6688    0.6305        50\nweighted avg     0.7244    0.6400    0.6234        50\n\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Load the file\ndata = np.load('/kaggle/working/preprocessed/train_MI.npz')\n\n# Check contents\nprint(list(data.keys()))  # ['X', 'y']\n\n# Inspect shapes\nX = data['X']\ny = data['y']\nprint(f\"X shape: {X.shape}\")  # (num_trials, 8, 2250)\nprint(f\"y shape: {y.shape}\")  # (num_trials,)\n\n# Verify channel order\nchannels = ['FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8']\nprint(f\"Channels: {channels}\")\n\n# Example: Plot C3 from first trial\nimport matplotlib.pyplot as plt\nplt.plot(X[0, 1, :])  # Index 1 = C3\nplt.title(f\"Trial 0 (Label: {y[0]}), Channel: C3\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T04:11:04.903977Z","iopub.execute_input":"2025-06-27T04:11:04.904290Z","iopub.status.idle":"2025-06-27T04:11:04.927524Z","shell.execute_reply.started":"2025-06-27T04:11:04.904252Z","shell.execute_reply":"2025-06-27T04:11:04.926654Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/303494779.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/model6/best_model6.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Check contents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                 raise ValueError(\"Cannot load file containing pickled data \"\n\u001b[0m\u001b[1;32m    463\u001b[0m                                  \"when allow_pickle=False\")\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"],"ename":"ValueError","evalue":"Cannot load file containing pickled data when allow_pickle=False","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\n\n# Configuration\nbase_path = \"/kaggle/input/mtcaic3\"\npreprocessed_dir = '/kaggle/working/preprocessed'  # Directory with preprocessed .npz files\nmodel_path = '/kaggle/working/models2/best_oldA.h5'\noutput_file = '/kaggle/working/submission.csv'\nthreshold = 0.5  # Prediction threshold for MI task\n\n# Load test data and sample submission\ntest_df = pd.read_csv(os.path.join(base_path, 'test.csv'))\nsample_sub = pd.read_csv(os.path.join(base_path, 'sample_submission.csv'))\n\n# Fixed F1Score metric class\nclass F1Score(tf.keras.metrics.Metric):\n    def __init__(self, name=\"f1_score\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        preds = tf.argmax(y_pred, axis=1)\n        labels = tf.argmax(y_true, axis=1)\n        \n        true_pos = tf.logical_and(tf.equal(preds, 1), tf.equal(labels, 1))\n        false_pos = tf.logical_and(tf.equal(preds, 1), tf.equal(labels, 0))\n        false_neg = tf.logical_and(tf.equal(preds, 0), tf.equal(labels, 1))\n        \n        self.tp.assign_add(tf.reduce_sum(tf.cast(true_pos, tf.float32)))\n        self.fp.assign_add(tf.reduce_sum(tf.cast(false_pos, tf.float32)))\n        self.fn.assign_add(tf.reduce_sum(tf.cast(false_neg, tf.float32)))\n\n    def result(self):\n        precision = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n        recall = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n        return 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n\n    def reset_states(self):\n        self.tp.assign(0.0)\n        self.fp.assign(0.0)\n        self.fn.assign(0.0)\n\n# Load model with fixed metric\nmodel = load_model(model_path, custom_objects={'F1Score': F1Score})\n\n# Define EEG channels to extract and their indices in preprocessed data\nchannel_names = ['FZ','C3','CZ','C4','PZ','PO7','OZ','PO8']\ntarget_channels = ['C3','CZ','C4','PZ']\ntarget_indices = [channel_names.index(ch) for ch in target_channels]\n\n# Load preprocessed test data\ntest_mi = np.load(os.path.join(preprocessed_dir, 'test_MI.npz'))['X']  # Shape (n_mi, 8, 2250)\ntest_ssvep = np.load(os.path.join(preprocessed_dir, 'test_SSVEP.npz'))['X']  # Shape (n_ssvep, 8, 1750)\n\n# Extract target channels and transpose to (trials, time, channels)\ntest_mi = test_mi[:, target_indices, :].transpose(0, 2, 1)  # New shape: (n_mi, 2250, 4)\ntest_ssvep = test_ssvep[:, target_indices, :].transpose(0, 2, 1)  # New shape: (n_ssvep, 1750, 4)\n\n# Get indices of MI and SSVEP trials in test_df\nmi_mask = test_df['task'] == 'MI'\nssvep_mask = test_df['task'] == 'SSVEP'\n\n# Validate counts\nassert mi_mask.sum() == test_mi.shape[0], \"MI trial count mismatch\"\nassert ssvep_mask.sum() == test_ssvep.shape[0], \"SSVEP trial count mismatch\"\n\n# Generate predictions\npredictions = []\n# Predict MI trials\nif test_mi.shape[0] > 0:\n    mi_preds = model.predict(test_mi, verbose=0)\n    mi_labels = ['Right' if prob[1] >= threshold else 'Left' for prob in mi_preds]\n    predictions.extend(mi_labels)\n\n# Assign dummy labels for SSVEP trials\nif test_ssvep.shape[0] > 0:\n    predictions.extend(['Left'] * test_ssvep.shape[0])\n\n# Create submission file\nsubmission = sample_sub.copy()\nsubmission['label'] = predictions\nsubmission.to_csv(output_file, index=False)\n\nprint(f\"Submission file saved to {output_file}\")\nprint(\"Prediction distribution:\")\nprint(submission['label'].value_counts())\nprint(f\"\\nThreshold used for MI: {threshold}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T12:29:00.436577Z","iopub.execute_input":"2025-06-29T12:29:00.437086Z","iopub.status.idle":"2025-06-29T12:29:01.453157Z","shell.execute_reply.started":"2025-06-29T12:29:00.437067Z","shell.execute_reply":"2025-06-29T12:29:01.452082Z"}},"outputs":[{"name":"stdout","text":"Submission file saved to /kaggle/working/submission.csv\nPrediction distribution:\nlabel\nLeft     69\nRight    31\nName: count, dtype: int64\n\nThreshold used for MI: 0.5\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}