{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":98188,"databundleVersionId":12673416,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =======================================================\n# 1) Setup and imports\n# =======================================================\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score\nfrom scipy.signal import butter, lfilter\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# =======================================================\n# 2) Config\n# =======================================================\nBASE_PATH = '/kaggle/input/mtcaic3'\nCHANNELS = ['FZ', 'C3', 'CZ', 'C4']\nFS = 250\nMU_BAND = (8, 13)\nBETA_BAND = (13, 30)\nMAX_LEN = 2250  # pad/truncate to this length\n\n# =======================================================\n# 3) Bandpass filter\n# =======================================================\ndef butter_bandpass(lowcut, highcut, fs, order=4):\n    nyq = 0.5 * fs\n    low, high = lowcut / nyq, highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    return b, a\n\ndef bandpass_filter(data, lowcut, highcut, fs):\n    b, a = butter_bandpass(lowcut, highcut, fs)\n    return lfilter(b, a, data)\n\n# =======================================================\n# 4) EEG Dataset with light augmentation\n# =======================================================\nclass EEGDataset(Dataset):\n    def __init__(self, df, base_path, le, augment=False):\n        self.df = df.reset_index(drop=True)\n        self.base_path = base_path\n        self.le = le\n        self.augment = augment\n        if 'label' in df:\n            self.labels = self.le.transform(df['label'])\n        else:\n            self.labels = None\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        dataset = 'train' if row['id'] <= 4800 else 'validation' if row['id'] <= 4900 else 'test'\n        eeg_path = f\"{self.base_path}/{row['task']}/{dataset}/{row['subject_id']}/{row['trial_session']}/EEGdata.csv\"\n        eeg_data = pd.read_csv(eeg_path)\n\n        samples_per_trial = 2250 if row['task'] == 'MI' else 1750\n        start = (row['trial'] - 1) * samples_per_trial\n        end = start + samples_per_trial\n\n        trial = eeg_data.iloc[start:end][CHANNELS].values.T  # (4, T)\n\n        # Pad or truncate\n        if trial.shape[1] < MAX_LEN:\n            pad_width = MAX_LEN - trial.shape[1]\n            trial = np.pad(trial, ((0, 0), (0, pad_width)), mode='constant')\n        elif trial.shape[1] > MAX_LEN:\n            trial = trial[:, :MAX_LEN]\n\n        # Augment (if training)\n        if self.augment:\n            # Add Gaussian noise\n            noise = np.random.normal(0, 0.01, trial.shape)\n            trial = trial + noise\n            # Random scaling\n            scale = np.random.uniform(0.9, 1.1)\n            trial = trial * scale\n\n        # Filter bands\n        mu = bandpass_filter(trial, MU_BAND[0], MU_BAND[1], FS)\n        beta = bandpass_filter(trial, BETA_BAND[0], BETA_BAND[1], FS)\n\n        # Tabular features\n        means = np.mean(trial, axis=1)\n        diff_c3_c4 = means[1] - means[3]\n        mu_power = np.mean(mu ** 2, axis=1)\n        beta_power = np.mean(beta ** 2, axis=1)\n        tabular = np.concatenate([means, [diff_c3_c4], mu_power, beta_power]).astype(np.float32)\n\n        waveform = torch.tensor(trial, dtype=torch.float32)\n        tabular = torch.tensor(tabular, dtype=torch.float32)\n\n        if self.labels is not None:\n            label = torch.tensor(self.labels[idx], dtype=torch.long)\n            return waveform, tabular, label\n        else:\n            return waveform, tabular\n\n# =======================================================\n# 5) Improved CNN + MLP Model\n# =======================================================\nclass CNN_MLP(nn.Module):\n    def __init__(self, in_channels, tabular_dim, num_classes):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv1d(in_channels, 32, 7, padding=3),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.Conv1d(32, 64, 7, padding=3),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Conv1d(64, 128, 7, padding=3),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(1),\n            nn.Flatten()\n        )\n        self.mlp = nn.Sequential(\n            nn.Linear(tabular_dim, 32),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(32, 16),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        self.final = nn.Linear(128 + 16, num_classes)\n\n    def forward(self, x_wave, x_tab):\n        x1 = self.cnn(x_wave)\n        x2 = self.mlp(x_tab)\n        x = torch.cat([x1, x2], dim=1)\n        return self.final(x)\n\n# =======================================================\n# 6) Prepare Data\n# =======================================================\ntrain_df = pd.read_csv(f\"{BASE_PATH}/train.csv\")\nval_df = pd.read_csv(f\"{BASE_PATH}/validation.csv\")\n\n# Filter MI task only\ntrain_df = train_df[train_df['task'] == 'MI'].copy()\nval_df = val_df[val_df['task'] == 'MI'].copy()\n\nprint(\"Train MI shape:\", train_df.shape)\nprint(\"Val MI shape:\", val_df.shape)\n\n# Shared LabelEncoder\nall_labels = pd.concat([train_df['label'], val_df['label']])\nle = LabelEncoder()\nle.fit(all_labels)\n\nprint(\"Classes:\", le.classes_)\n\ntrain_set = EEGDataset(train_df, BASE_PATH, le, augment=True)\nval_set = EEGDataset(val_df, BASE_PATH, le, augment=False)\n\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_set, batch_size=32, shuffle=False, num_workers=2)\n\n# =======================================================\n# 7) Training Setup\n# =======================================================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = CNN_MLP(\n    in_channels=len(CHANNELS),\n    tabular_dim=4 + 1 + 4 + 4,  # means + diff + mu_power + beta_power\n    num_classes=len(le.classes_)\n).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\nbest_val_f1 = 0\npatience = 15\npatience_counter = 0\n\nhistory = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n\n# =======================================================\n# 8) Train with Early Stopping\n# =======================================================\nfor epoch in range(100):\n    model.train()\n    train_loss = 0.0\n    for x_wave, x_tab, y in train_loader:\n        x_wave, x_tab, y = x_wave.to(device), x_tab.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x_wave, x_tab)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * y.size(0)\n\n    train_loss /= len(train_loader.dataset)\n\n    model.eval()\n    val_loss = 0.0\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for x_wave, x_tab, y in val_loader:\n            x_wave, x_tab, y = x_wave.to(device), x_tab.to(device), y.to(device)\n            out = model(x_wave, x_tab)\n            loss = criterion(out, y)\n            val_loss += loss.item() * y.size(0)\n            y_true.extend(y.cpu().numpy())\n            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n\n    val_loss /= len(val_loader.dataset)\n    val_f1 = f1_score(y_true, y_pred, average='macro')\n\n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['val_f1'].append(val_f1)\n\n    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} | Val Loss={val_loss:.4f} | Val F1={val_f1:.4f}\")\n\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        patience_counter = 0\n        torch.save(model.state_dict(), 'best_model.pth')\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(\"✅ Early stopping triggered!\")\n            break\n\n# =======================================================\n# 9) Save logs\n# =======================================================\nnp.savez('training_log.npz', **history)\nprint(\"✅ Training log saved: training_log.npz\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:20:11.863010Z","iopub.execute_input":"2025-06-17T16:20:11.863330Z","iopub.status.idle":"2025-06-17T16:54:53.055664Z","shell.execute_reply.started":"2025-06-17T16:20:11.863307Z","shell.execute_reply":"2025-06-17T16:54:53.054419Z"}},"outputs":[{"name":"stdout","text":"Train MI shape: (2400, 6)\nVal MI shape: (50, 6)\nClasses: ['Left' 'Right']\nEpoch 1: Train Loss=357825.9558 | Val Loss=71288.6909 | Val F1=0.5895\nEpoch 2: Train Loss=262504.0104 | Val Loss=62985.7613 | Val F1=0.5659\nEpoch 3: Train Loss=198695.3972 | Val Loss=55712.9608 | Val F1=0.5895\nEpoch 4: Train Loss=158962.3735 | Val Loss=58957.6112 | Val F1=0.4907\nEpoch 5: Train Loss=129660.7802 | Val Loss=58120.9601 | Val F1=0.4652\nEpoch 6: Train Loss=98752.0924 | Val Loss=45285.6936 | Val F1=0.3333\nEpoch 7: Train Loss=70992.4940 | Val Loss=33537.6543 | Val F1=0.2958\nEpoch 8: Train Loss=57662.9590 | Val Loss=26534.2320 | Val F1=0.4325\nEpoch 9: Train Loss=48592.3281 | Val Loss=22395.1078 | Val F1=0.4167\nEpoch 10: Train Loss=37619.1287 | Val Loss=16054.8467 | Val F1=0.4492\nEpoch 11: Train Loss=28833.6121 | Val Loss=10973.7661 | Val F1=0.4945\nEpoch 12: Train Loss=21679.8944 | Val Loss=7806.3937 | Val F1=0.4945\nEpoch 13: Train Loss=15229.2676 | Val Loss=5468.4406 | Val F1=0.5098\nEpoch 14: Train Loss=10710.8325 | Val Loss=3735.5216 | Val F1=0.4945\nEpoch 15: Train Loss=8186.7008 | Val Loss=3036.0406 | Val F1=0.4945\nEpoch 16: Train Loss=6894.4081 | Val Loss=2021.2486 | Val F1=0.4945\n✅ Early stopping triggered!\n✅ Training log saved: training_log.npz\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =======================================================\n# 1) Setup and imports\n# =======================================================\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\nfrom scipy.signal import butter, lfilter\nimport matplotlib.pyplot as plt\nfrom mne.decoding import CSP\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# =======================================================\n# 2) Config\n# =======================================================\nBASE_PATH = '/kaggle/input/mtcaic3'\nCHANNELS = ['FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8']\nFS = 250\nMAX_LEN = 2250  # pad/truncate to this length\n\n# =======================================================\n# 3) Bandpass filter\n# =======================================================\ndef butter_bandpass(lowcut, highcut, fs, order=4):\n    nyq = 0.5 * fs\n    low, high = lowcut / nyq, highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    return b, a\n\ndef bandpass_filter(data, lowcut, highcut, fs):\n    b, a = butter_bandpass(lowcut, highcut, fs)\n    return lfilter(b, a, data)\n\n# =======================================================\n# 4) EEG Dataset with CSP and light augmentation\n# =======================================================\nclass EEGDataset(Dataset):\n    def __init__(self, df, base_path, le, csp=None, augment=False):\n        self.df = df.reset_index(drop=True)\n        self.base_path = base_path\n        self.le = le\n        self.csp = csp\n        self.augment = augment\n        if 'label' in df:\n            self.labels = self.le.transform(df['label'])\n        else:\n            self.labels = None\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        dataset = 'train' if row['id'] <= 4800 else 'validation' if row['id'] <= 4900 else 'test'\n        eeg_path = f\"{self.base_path}/{row['task']}/{dataset}/{row['subject_id']}/{row['trial_session']}/EEGdata.csv\"\n        eeg_data = pd.read_csv(eeg_path)\n\n        samples_per_trial = 2250 if row['task'] == 'MI' else 1750\n        start = (row['trial'] - 1) * samples_per_trial\n        end = start + samples_per_trial\n\n        trial = eeg_data.iloc[start:end][CHANNELS].values.T  # (C, T)\n\n        if trial.shape[1] < MAX_LEN:\n            trial = np.pad(trial, ((0, 0), (0, MAX_LEN - trial.shape[1])), mode='constant')\n        elif trial.shape[1] > MAX_LEN:\n            trial = trial[:, :MAX_LEN]\n\n        if self.augment:\n            noise = np.random.normal(0, 0.01, trial.shape)\n            trial += noise\n            scale = np.random.uniform(0.9, 1.1)\n            trial *= scale\n\n        if self.csp is not None:\n            trial = self.csp.transform(trial.T[np.newaxis, :, :])[0].T  # (CSP_components, T)\n\n        # Simple tabular: means + diff\n        means = np.mean(trial, axis=1)\n        diff_c3_c4 = means[1] - means[3] if len(means) >= 4 else 0.0\n        tabular = np.concatenate([means, [diff_c3_c4]]).astype(np.float32)\n\n        waveform = torch.tensor(trial, dtype=torch.float32)\n        tabular = torch.tensor(tabular, dtype=torch.float32)\n\n        if self.labels is not None:\n            label = torch.tensor(self.labels[idx], dtype=torch.long)\n            return waveform, tabular, label\n        else:\n            return waveform, tabular\n\n# =======================================================\n# 5) Improved CNN + MLP Model\n# =======================================================\nclass CNN_MLP(nn.Module):\n    def __init__(self, in_channels, tabular_dim, num_classes):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv1d(in_channels, 32, 7, padding=3),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.Conv1d(32, 64, 7, padding=3),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Conv1d(64, 128, 7, padding=3),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(1),\n            nn.Flatten()\n        )\n        self.mlp = nn.Sequential(\n            nn.Linear(tabular_dim, 32),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(32, 16),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        self.final = nn.Linear(128 + 16, num_classes)\n\n    def forward(self, x_wave, x_tab):\n        x1 = self.cnn(x_wave)\n        x2 = self.mlp(x_tab)\n        x = torch.cat([x1, x2], dim=1)\n        return self.final(x)\n\n# =======================================================\n# 6) Prepare Data\n# =======================================================\ntrain_df = pd.read_csv(f\"{BASE_PATH}/train.csv\")\nval_df = pd.read_csv(f\"{BASE_PATH}/validation.csv\")\n\ntrain_df = train_df[train_df['task'] == 'MI'].copy()\nval_df = val_df[val_df['task'] == 'MI'].copy()\n\nprint(\"Train MI shape:\", train_df.shape)\nprint(\"Val MI shape:\", val_df.shape)\n\n# Shared LabelEncoder\nall_labels = pd.concat([train_df['label'], val_df['label']])\nle = LabelEncoder()\nle.fit(all_labels)\n\nprint(\"Classes:\", le.classes_)\n\n# =======================================================\n# 7) Fit CSP on training data\n# =======================================================\nprint(\"Fitting CSP...\")\n\nX_waveforms = []\ny_labels = []\nfor i in range(len(train_df)):\n    row = train_df.iloc[i]\n    eeg_path = f\"{BASE_PATH}/{row['task']}/train/{row['subject_id']}/{row['trial_session']}/EEGdata.csv\"\n    eeg_data = pd.read_csv(eeg_path)\n    samples_per_trial = 2250\n    start = (row['trial'] - 1) * samples_per_trial\n    end = start + samples_per_trial\n    trial = eeg_data.iloc[start:end][CHANNELS].values.T\n    if trial.shape[1] < MAX_LEN:\n        trial = np.pad(trial, ((0, 0), (0, MAX_LEN - trial.shape[1])))\n    elif trial.shape[1] > MAX_LEN:\n        trial = trial[:, :MAX_LEN]\n    X_waveforms.append(trial)\n    y_labels.append(row['label'])\n\nX_waveforms = np.stack(X_waveforms, axis=0)  # (N, C, T)\nX_waveforms = np.transpose(X_waveforms, (0, 2, 1))  # (N, T, C)\ny_labels = le.transform(y_labels)\n\ncsp = CSP(n_components=4, reg=None, log=True, norm_trace=False)\ncsp.fit(X_waveforms, y_labels)\n\n# =======================================================\n# 8) Create Datasets and Loaders\n# =======================================================\ntrain_set = EEGDataset(train_df, BASE_PATH, le, csp=csp, augment=True)\nval_set = EEGDataset(val_df, BASE_PATH, le, csp=csp, augment=False)\n\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_set, batch_size=32, shuffle=False, num_workers=2)\n\n# =======================================================\n# 9) Training Setup\n# =======================================================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = CNN_MLP(\n    in_channels=4,  # CSP components\n    tabular_dim=4 + 1,  # means + diff\n    num_classes=len(le.classes_)\n).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\nbest_val_f1 = 0\npatience = 15\npatience_counter = 0\n\nhistory = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n\n# =======================================================\n# 10) Train with Early Stopping\n# =======================================================\nfor epoch in range(100):\n    model.train()\n    train_loss = 0.0\n    for x_wave, x_tab, y in train_loader:\n        x_wave, x_tab, y = x_wave.to(device), x_tab.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x_wave, x_tab)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * y.size(0)\n\n    train_loss /= len(train_loader.dataset)\n\n    model.eval()\n    val_loss = 0.0\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for x_wave, x_tab, y in val_loader:\n            x_wave, x_tab, y = x_wave.to(device), x_tab.to(device), y.to(device)\n            out = model(x_wave, x_tab)\n            loss = criterion(out, y)\n            val_loss += loss.item() * y.size(0)\n            y_true.extend(y.cpu().numpy())\n            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n\n    val_loss /= len(val_loader.dataset)\n    val_f1 = f1_score(y_true, y_pred, average='macro')\n\n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['val_f1'].append(val_f1)\n\n    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} | Val Loss={val_loss:.4f} | Val F1={val_f1:.4f}\")\n\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        patience_counter = 0\n        torch.save(model.state_dict(), 'best_model.pth')\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(\"✅ Early stopping triggered!\")\n            break\n\n# =======================================================\n# 11) Save logs\n# =======================================================\nnp.savez('training_log.npz', **history)\nprint(\"✅ Training log saved: training_log.npz\")\n\n# =======================================================\n# 12) Confusion Matrix\n# =======================================================\nmodel.load_state_dict(torch.load('best_model.pth'))\nmodel.eval()\n\nall_true, all_pred = [], []\nwith torch.no_grad():\n    for x_wave, x_tab, y in val_loader:\n        x_wave, x_tab = x_wave.to(device), x_tab.to(device)\n        out = model(x_wave, x_tab)\n        all_true.extend(y.numpy())\n        all_pred.extend(out.argmax(dim=1).cpu().numpy())\n\ncm = confusion_matrix(all_true, all_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Validation Confusion Matrix\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T20:24:55.504598Z","iopub.execute_input":"2025-06-17T20:24:55.504954Z","iopub.status.idle":"2025-06-17T20:29:53.204415Z","shell.execute_reply.started":"2025-06-17T20:24:55.504883Z","shell.execute_reply":"2025-06-17T20:29:53.202454Z"}},"outputs":[{"name":"stdout","text":"Train MI shape: (2400, 6)\nVal MI shape: (50, 6)\nClasses: ['Left' 'Right']\nFitting CSP...\nComputing rank from data with rank=None\n    Using tolerance 1e+09 (2.2e-16 eps * 2250 dim * 2.1e+21  max singular value)\n    Estimated rank (data): 2250\n    data: rank 2250 computed from 2250 data channels with 0 projectors\nReducing data rank from 2250 -> 2250\nEstimating class=0 covariance using EMPIRICAL\nDone.\nEstimating class=1 covariance using EMPIRICAL\nDone.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/4202493367.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx_wave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0mx_wave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_wave\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAxisError\u001b[0m: Caught AxisError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_35/4202493367.py\", line 86, in __getitem__\n    means = np.mean(trial, axis=1)\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\", line 3504, in mean\n    return _methods._mean(a, axis=axis, dtype=dtype,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py\", line 106, in _mean\n    rcount = _count_reduce_items(arr, axis, keepdims=keepdims, where=where)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py\", line 77, in _count_reduce_items\n    items *= arr.shape[mu.normalize_axis_index(ax, arr.ndim)]\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n"],"ename":"AxisError","evalue":"Caught AxisError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_35/4202493367.py\", line 86, in __getitem__\n    means = np.mean(trial, axis=1)\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\", line 3504, in mean\n    return _methods._mean(a, axis=axis, dtype=dtype,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py\", line 106, in _mean\n    rcount = _count_reduce_items(arr, axis, keepdims=keepdims, where=where)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py\", line 77, in _count_reduce_items\n    items *= arr.shape[mu.normalize_axis_index(ax, arr.ndim)]\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}