{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98188,"databundleVersionId":12673416,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom scipy.signal import butter, filtfilt, iirnotch\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configuration\nBASE_PATH = '/kaggle/input/mtcaic3'\nOUTPUT_DIR = './preprocessed'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Filter design\ndef design_filters(fs=250.0):\n    # Bandpass 1-40 Hz\n    bp_b, bp_a = butter(4, [1/(fs/2), 40/(fs/2)], btype='band')\n    # Notch at 50 Hz\n    notch_b, notch_a = iirnotch(50/(fs/2), Q=30)\n    return bp_b, bp_a, notch_b, notch_a\n\n# Preprocessing steps for one trial\ndef preprocess_trial(df):\n    eeg_cols = ['FZ','C3','CZ','C4','PZ','PO7','OZ','PO8']\n    motion_cols = ['AccX','AccY','AccZ','Gyro1','Gyro2','Gyro3']\n    val_col = 'Validation'\n\n    # 1) Motion artifact detection\n    motion_mag = np.sqrt((df[motion_cols]**2).sum(axis=1))\n    mot_thresh = np.percentile(motion_mag, 95)\n    bad_mask = motion_mag > mot_thresh\n\n    # 2) Mask EEG\n    data = df[eeg_cols].copy().values\n    data[bad_mask, :] = np.nan\n    data[df[val_col] == 0, :] = np.nan\n\n    # 3) Interpolation\n    for ch in range(data.shape[1]):\n        col = data[:, ch]\n        nans = np.isnan(col)\n        if nans.all():\n            continue\n        idx = np.arange(len(col))\n        data[nans, ch] = np.interp(idx[nans], idx[~nans], col[~nans])\n\n    # 4) Filtering\n    bp_b, bp_a, notch_b, notch_a = design_filters()\n    for ch in range(data.shape[1]):\n        data[:, ch] = filtfilt(bp_b, bp_a, data[:, ch])\n        data[:, ch] = filtfilt(notch_b, notch_a, data[:, ch])\n\n    # 5) Baseline correction (first 0.5s)\n    bs = int(0.5 * 250)\n    baseline = data[:bs].mean(axis=0)\n    data -= baseline\n    return data\n\n# Load index DataFrame\ndef load_index(fname, label_col=True):\n    df = pd.read_csv(os.path.join(BASE_PATH, fname))\n    cols = ['id','subject_id','task','trial_session','trial'] + (['label'] if label_col else [])\n    return df[cols]\n\n# Process a split with session-wise normalization and outlier removal\ndef process_split(df, has_label=True, split_name=''):\n    data_dict = {'MI': {'X': [], 'y': [], 'id': []}, 'SSVEP': {'X': [], 'y': [], 'id': []}}\n    total_trials = 0\n    total_dropped = 0\n    \n    # Group by session\n    session_groups = df.groupby(['subject_id', 'task', 'trial_session'])\n    \n    for (subject_id, task, trial_session), group in session_groups:\n        # Load entire session data\n        path = os.path.join(BASE_PATH, task, split_name, subject_id, str(trial_session), 'EEGdata.csv')\n        session_df = pd.read_csv(path)\n        \n        # Preprocess all trials in session\n        n_samp = 2250 if task == 'MI' else 1750\n        session_trials = []\n        valid_trial_ids = []\n        \n        # Process each trial in session\n        for _, row in group.iterrows():\n            start_idx = (row['trial'] - 1) * n_samp\n            end_idx = start_idx + n_samp\n            trial_data = session_df.iloc[start_idx:end_idx].copy()\n            preprocessed = preprocess_trial(trial_data)\n            session_trials.append(preprocessed)\n            valid_trial_ids.append(row['id'])\n        \n        # Concatenate for session normalization\n        session_data = np.concatenate(session_trials, axis=0)\n        \n        # Session-wise normalization\n        mean = np.nanmean(session_data, axis=0)\n        std = np.nanstd(session_data, axis=0)\n        std[std < 1e-8] = 1.0  # Avoid division by zero\n        \n        # Normalize and detect outliers\n        session_trials_norm = []\n        outlier_flags = []\n        \n        for trial_idx, trial_data in enumerate(session_trials):\n            # Normalize trial\n            norm_trial = (trial_data - mean) / std\n            \n            # Calculate outlier metrics\n            max_amp = np.max(np.abs(norm_trial))\n            mean_var = np.mean(np.var(norm_trial, axis=0))\n            \n            # Thresholds (3 std from session mean)\n            amp_threshold = 3 * np.std([np.max(np.abs(t)) for t in session_trials])\n            var_threshold = 3 * np.std([np.mean(np.var(t, axis=0)) for t in session_trials])\n            \n            # Check if outlier\n            is_outlier = (max_amp > amp_threshold) or (mean_var > var_threshold)\n            outlier_flags.append(is_outlier)\n            \n            if not is_outlier:\n                session_trials_norm.append(norm_trial.T)  # Transpose to (channels, time)\n        \n        # Update counters\n        n_outliers = sum(outlier_flags)\n        total_dropped += n_outliers\n        total_trials += len(session_trials)\n        \n        if n_outliers > 0:\n            print(f\"Session {subject_id}/{trial_session} ({task}): Dropped {n_outliers} trials\")\n        \n        # Store valid trials\n        for idx, is_outlier in enumerate(outlier_flags):\n            if not is_outlier:\n                trial_id = valid_trial_ids[idx]\n                data_dict[task]['X'].append(session_trials_norm.pop(0))\n                data_dict[task]['id'].append(trial_id)\n                if has_label:\n                    label = group[group['id'] == trial_id]['label'].values[0]\n                    data_dict[task]['y'].append(label)\n    \n    # Drop duplicates across entire split (based on ID)\n    for task in data_dict:\n        if data_dict[task]['id']:\n            df_task = pd.DataFrame({\n                'id': data_dict[task]['id'],\n                'X': data_dict[task]['X'],\n                'y': data_dict[task]['y'] if has_label else [None]*len(data_dict[task]['id'])\n            })\n            # Drop duplicate IDs\n            initial_count = len(df_task)\n            df_task = df_task.drop_duplicates(subset=['id'])\n            dup_dropped = initial_count - len(df_task)\n            if dup_dropped > 0:\n                print(f\"Dropped {dup_dropped} duplicates in {task} {split_name}\")\n                total_dropped += dup_dropped\n            \n            # Update data_dict\n            data_dict[task]['X'] = df_task['X'].tolist()\n            data_dict[task]['id'] = df_task['id'].tolist()\n            if has_label:\n                data_dict[task]['y'] = df_task['y'].tolist()\n    \n    # Stack data\n    for task in data_dict:\n        if data_dict[task]['X']:\n            data_dict[task]['X'] = np.stack(data_dict[task]['X'])\n            if has_label:\n                data_dict[task]['y'] = np.array(data_dict[task]['y'])\n            data_dict[task]['id'] = np.array(data_dict[task]['id'])\n        else:\n            data_dict[task]['X'] = np.array([])\n            data_dict[task]['y'] = np.array([])\n            data_dict[task]['id'] = np.array([])\n    \n    print(f\"{split_name}: Total trials {total_trials}, Dropped {total_dropped} \"\n          f\"({total_dropped/total_trials*100:.2f}%)\")\n    return data_dict\n\n# Execute processing and save\nfor fname, label_col in [('train.csv', True), ('validation.csv', True), ('test.csv', False)]:\n    split_name = fname.replace('.csv', '')\n    df_idx = load_index(fname, label_col)\n    results = process_split(df_idx, label_col, split_name=split_name)\n    \n    for task, d in results.items():\n        out_file = f\"{split_name}_{task}.npz\"\n        path = os.path.join(OUTPUT_DIR, out_file)\n        if label_col:\n            np.savez_compressed(path, X=d['X'], y=d['y'], id=d['id'])\n        else:\n            np.savez_compressed(path, X=d['X'], id=d['id'])\n        print(f\"Saved {out_file}: X={d['X'].shape}\" + \n              (f\", y={d['y'].shape}\" if label_col and d['y'].size > 0 else \"\") +\n              f\", id={d['id'].shape}\")\n\nprint('Preprocessing complete.')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T09:17:12.807735Z","iopub.execute_input":"2025-06-29T09:17:12.808032Z","iopub.status.idle":"2025-06-29T09:19:18.962266Z","shell.execute_reply.started":"2025-06-29T09:17:12.808011Z","shell.execute_reply":"2025-06-29T09:19:18.961498Z"}},"outputs":[{"name":"stdout","text":"train: Total trials 4800, Dropped 0 (0.00%)\nSaved train_MI.npz: X=(2400, 8, 2250), y=(2400,), id=(2400,)\nSaved train_SSVEP.npz: X=(2400, 8, 1750), y=(2400,), id=(2400,)\nvalidation: Total trials 100, Dropped 0 (0.00%)\nSaved validation_MI.npz: X=(50, 8, 2250), y=(50,), id=(50,)\nSaved validation_SSVEP.npz: X=(50, 8, 1750), y=(50,), id=(50,)\ntest: Total trials 100, Dropped 0 (0.00%)\nSaved test_MI.npz: X=(50, 8, 2250), id=(50,)\nSaved test_SSVEP.npz: X=(50, 8, 1750), id=(50,)\nPreprocessing complete.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\n\nfrom sklearn.metrics import f1_score, classification_report\nfrom mne.decoding import CSP\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, LearningRateScheduler\n\n# --- 0) Config ---\ndata_dir   = './preprocessed'\noutput_dir = './models3'\nos.makedirs(output_dir, exist_ok=True)\n\neeg_indices = [1, 2, 3, 4]  # C3, CZ, C4, PZ\n\n# --- 1) Load & filter data ---\ntrain_npz = np.load(os.path.join(data_dir, 'train_MI.npz'))\nval_npz   = np.load(os.path.join(data_dir, 'validation_MI.npz'))\n\nX_train_all, y_train = train_npz['X'], train_npz['y']\nX_val_all,   y_val   = val_npz['X'],   val_npz['y']\n\n# Filter for EEG channels only\nX_train_raw = X_train_all[:, eeg_indices, :].transpose(0, 2, 1).astype('float32')  # (n, T, 4)\nX_val_raw   = X_val_all[:,   eeg_indices, :].transpose(0, 2, 1).astype('float32')  # (n, T, 4)\n\n# --- 2) Binarize labels ---\ny_train_bin = (y_train == 'Right').astype(int)\ny_val_bin   = (y_val   == 'Right').astype(int)\n\n# --- 3) CSP for models 3, 4, 5 ---\ncsp = CSP(n_components=4, log=False, norm_trace=False)\nX_train_csp_input = X_train_raw.transpose(0, 2, 1).astype('float64').copy()  # (n, channels, time)\ncsp.fit(X_train_csp_input, y_train_bin)  # ← NO 'rank' argument\n\nW = csp.filters_[:4]\n\ndef apply_csp(X):  # X: (n, T, C)\n    return np.stack([W.dot(ep.T) for ep in X], axis=0)\n\n\nXtr_csp = apply_csp(X_train_raw).astype('float32')\nXvl_csp = apply_csp(X_val_raw).astype('float32')\n\nXtr_csp = Xtr_csp.transpose(0, 2, 1)  # (n, T, 4)\nXvl_csp = Xvl_csp.transpose(0, 2, 1)\n\n\n# For 2D models\nXtr_csp_2d = Xtr_csp[..., np.newaxis]\nXvl_csp_2d = Xvl_csp[..., np.newaxis]\n\n# --- 4) One-hot labels ---\nytr_oh = keras.utils.to_categorical(y_train_bin, 2)\nyvl_oh = keras.utils.to_categorical(y_val_bin,   2)\n\n# --- 5) Data augmentation ---\ndef aug_gen(X, y, seed=0, batch_size=32):\n    n = X.shape[0]\n    rng = np.random.RandomState(seed)\n    while True:\n        idx = rng.randint(0, n, batch_size)\n        bx, by = X[idx].copy(), y[idx]\n        bx += rng.normal(0, 0.005, bx.shape)\n        yield bx, by\n\ntrain_gen_raw   = aug_gen(X_train_raw,  ytr_oh, seed=0, batch_size=64)\ntrain_gen_csp1d = aug_gen(Xtr_csp,      ytr_oh, seed=1, batch_size=64)\ntrain_gen_csp2d = aug_gen(Xtr_csp_2d,   ytr_oh, seed=2, batch_size=64)\n\nsteps_raw   = len(X_train_raw)  // 64\nsteps_csp1d = len(Xtr_csp)      // 64\nsteps_csp2d = len(Xtr_csp_2d)   // 64\n\n# --- 6) Cosine LR schedule ---\ndef cosine_lr(epoch, lr_max=5e-5, epochs=200):\n    return lr_max * (1 + np.cos(np.pi * epoch / epochs)) / 2\n\n# --- 7) F1 Score Metric ---\nclass F1Score(tf.keras.metrics.Metric):\n    def __init__(self, name=\"f1_score\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        preds  = tf.argmax(y_pred, axis=1)\n        labels = tf.argmax(y_true, axis=1)\n        self.tp.assign_add(tf.reduce_sum(tf.cast(tf.logical_and(preds == 1, labels == 1), tf.float32)))\n        self.fp.assign_add(tf.reduce_sum(tf.cast(tf.logical_and(preds == 1, labels == 0), tf.float32)))\n        self.fn.assign_add(tf.reduce_sum(tf.cast(tf.logical_and(preds == 0, labels == 1), tf.float32)))\n\n    def result(self):\n        p = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n        r = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n        return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n\n    def reset_states(self):\n        self.tp.assign(0.0)\n        self.fp.assign(0.0)\n        self.fn.assign(0.0)\n\n# --- 8) Callback factory ---\ndef get_callbacks(name):\n    return [\n        EarlyStopping(\"val_f1_score\", mode=\"max\", patience=20, restore_best_weights=True),\n        ModelCheckpoint(os.path.join(output_dir, f\"best_{name}.h5\"),\n                        \"val_f1_score\", mode=\"max\", save_best_only=True),\n        CSVLogger(os.path.join(output_dir, f\"log_{name}.csv\")),\n        LearningRateScheduler(cosine_lr)\n    ]\n\n# --- 9) Model Builders ---\n# [All model builder functions remain unchanged — see previous message if needed.]\ndef build_modelA(input_shape):\n    m = keras.Sequential([\n        layers.Input(input_shape),\n        layers.Conv1D(32, 5, activation=\"relu\", padding=\"same\"),\n        layers.BatchNormalization(), layers.MaxPool1D(2),\n        layers.Conv1D(64, 5, activation=\"relu\", padding=\"same\"),\n        layers.BatchNormalization(), layers.MaxPool1D(2),\n        layers.Conv1D(128,5,activation=\"relu\",padding=\"same\"),\n        layers.BatchNormalization(),\n        layers.GlobalAveragePooling1D(),\n        layers.Dense(64, activation=\"relu\", \n                     kernel_regularizer=regularizers.l2(1e-4)),\n        layers.Dropout(0.7),\n        layers.Dense(2, activation=\"softmax\"),\n    ])\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_modelB(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for f in [16,32,64,128,256]:\n        x = layers.Conv1D(f,3,activation=\"relu\",padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.MaxPool1D(2)(x)\n    x = layers.Flatten()(x)\n    for u in [128,64,32]:\n        x = layers.Dense(u, activation=\"relu\",\n                         kernel_regularizer=regularizers.l2(1e-4))(x)\n        x = layers.Dropout(0.5)(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp, out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model1(input_shape):\n    inp = layers.Input(input_shape+(1,))\n    x = layers.Concatenate()([inp, inp, inp])\n    x = layers.Resizing(32,32)(x)\n    base = keras.applications.ResNet50(\n        include_top=False, weights=\"imagenet\",\n        input_shape=(32,32,3), pooling=\"avg\"\n    )\n    base.trainable = False\n    x = base(x)\n    x = layers.Reshape((1, x.shape[-1]))(x)\n    for _ in range(7):\n        x = layers.Conv1D(64,3,activation=\"relu\",padding=\"same\")(x)\n    x = layers.MultiHeadAttention(num_heads=4,key_dim=32)(x,x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(64,activation=\"relu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model2(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for _ in range(3):\n        x = layers.Conv1D(32,3,activation=\"elu\",padding=\"same\")(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(64,activation=\"elu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model3(input_shape):\n    inp = layers.Input(input_shape)  # (T, F, 1)\n    x = inp\n    for _ in range(5):\n        x = layers.Conv2D(32,(3,3),activation=\"relu\",padding=\"same\")(x)\n    x = layers.Flatten()(x)\n    for u in [128,64,32]:\n        x = layers.Dense(u, activation=\"relu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model4(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for _ in range(3):\n        x = layers.Conv1D(64,3,activation=\"relu\",padding=\"same\")(x)\n    x = layers.LSTM(128)(x)\n    for _ in range(4):\n        x = layers.Dense(64, activation=\"relu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model5(input_shape):\n    inp = layers.Input(input_shape)\n    x = inp\n    for _ in range(7):\n        x = layers.Conv1D(64,3,activation=\"elu\",padding=\"same\")(x)\n    x = layers.Flatten()(x)\n    for _ in range(3):\n        x = layers.Dense(64, activation=\"elu\")(x)\n    out = layers.Dense(2, activation=\"softmax\")(x)\n    m = models.Model(inp,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n\ndef build_model6(input_shape):\n    C3, C4 = 0,2\n    eeg_in = layers.Input(input_shape)\n    c3 = layers.Lambda(lambda x: x[:,:,C3:C3+1])(eeg_in)\n    c4 = layers.Lambda(lambda x: x[:,:,C4:C4+1])(eeg_in)\n    def branch():\n        return models.Sequential([\n            layers.Conv1D(16,250,activation=\"relu\",padding=\"same\"),\n            layers.MaxPool1D(3),\n            layers.Conv1D(32,50,activation=\"relu\",padding=\"same\"),\n            layers.GlobalAveragePooling1D()\n        ])\n    b3, b4 = branch()(c3), branch()(c4)\n    x = layers.Concatenate()([b3,b4])\n    for _ in range(4):\n        x = layers.Dense(64,activation=\"relu\")(x)\n    out = layers.Dense(2,activation=\"softmax\")(x)\n    m = models.Model(eeg_in,out)\n    m.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\", F1Score()])\n    return m\n# (You can paste model builders here unchanged if needed)\n\n# --- 10) Train & evaluate ---\nbuilders = {\n    'oldA': build_modelA,\n    'oldB': build_modelB,\n    'model1': build_model1,\n    'model2': build_model2,\n    'model3': build_model3,\n    'model4': build_model4,\n    'model5': build_model5,\n    'model6': build_model6,\n}\n\nresults = {}\nshape_raw   = X_train_raw.shape[1:]\nshape_csp1d = Xtr_csp.shape[1:]\nshape_csp2d = Xtr_csp_2d.shape[1:]\n\nfor name, build_fn in builders.items():\n    print(f\"\\n>>> Training {name}\")\n    if name in ['model3']:\n        model = build_fn(shape_csp2d)\n        gen, steps, val_x = train_gen_csp2d, steps_csp2d, Xvl_csp_2d\n    elif name in ['model4', 'model5']:\n        model = build_fn(shape_csp1d)\n        gen, steps, val_x = train_gen_csp1d, steps_csp1d, Xvl_csp\n    else:\n        model = build_fn(shape_raw)\n        gen, steps, val_x = train_gen_raw, steps_raw, X_val_raw\n\n    model.fit(\n        gen, steps_per_epoch=steps,\n        validation_data=(val_x, yvl_oh),\n        epochs=200, callbacks=get_callbacks(name), verbose=2\n    )\n\n    preds = np.argmax(model.predict(val_x), axis=1)\n    f1 = f1_score(y_val_bin, preds)\n    print(f\"{name} → val F1 = {f1:.4f}\")\n    print(classification_report(y_val_bin, preds, target_names=[\"Left\", \"Right\"]))\n    results[name] = (f1, model)\n\n# --- 11) Save best model ---\nbest_name, (best_f1, best_model) = max(results.items(), key=lambda kv: kv[1][0])\nprint(f\"\\n=== Final best: {best_name} (F1={best_f1:.4f}) ===\")\nbest_model.save(os.path.join(output_dir, 'best_final.h5'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T09:53:27.728690Z","iopub.execute_input":"2025-06-29T09:53:27.729000Z","iopub.status.idle":"2025-06-29T10:00:46.021434Z","shell.execute_reply.started":"2025-06-29T09:53:27.728978Z","shell.execute_reply":"2025-06-29T10:00:46.020734Z"}},"outputs":[{"name":"stdout","text":"Computing rank from data with rank=None\n    Using tolerance 3.9 (2.2e-16 eps * 4 dim * 4.4e+15  max singular value)\n    Estimated rank (data): 4\n    data: rank 4 computed from 4 data channels with 0 projectors\nReducing data rank from 4 -> 4\nEstimating class=0 covariance using EMPIRICAL\nDone.\nEstimating class=1 covariance using EMPIRICAL\nDone.\n\n>>> Training oldA\nEpoch 1/200\n37/37 - 9s - 240ms/step - accuracy: 0.4882 - f1_score: 0.4925 - loss: 0.8209 - val_accuracy: 0.4200 - val_f1_score: 0.3556 - val_loss: 0.7108 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 27ms/step - accuracy: 0.4970 - f1_score: 0.4779 - loss: 0.7429 - val_accuracy: 0.5000 - val_f1_score: 0.1935 - val_loss: 0.7042 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 27ms/step - accuracy: 0.5139 - f1_score: 0.4891 - loss: 0.7142 - val_accuracy: 0.5800 - val_f1_score: 0.1600 - val_loss: 0.6949 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 28ms/step - accuracy: 0.5351 - f1_score: 0.5285 - loss: 0.7010 - val_accuracy: 0.5200 - val_f1_score: 0.5862 - val_loss: 0.6977 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 29ms/step - accuracy: 0.5190 - f1_score: 0.5661 - loss: 0.7098 - val_accuracy: 0.5400 - val_f1_score: 0.6102 - val_loss: 0.6982 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 27ms/step - accuracy: 0.5220 - f1_score: 0.5203 - loss: 0.7078 - val_accuracy: 0.5200 - val_f1_score: 0.0769 - val_loss: 0.6964 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 27ms/step - accuracy: 0.5182 - f1_score: 0.5011 - loss: 0.7018 - val_accuracy: 0.4200 - val_f1_score: 0.3256 - val_loss: 0.7086 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 31ms/step - accuracy: 0.5148 - f1_score: 0.5666 - loss: 0.7015 - val_accuracy: 0.5000 - val_f1_score: 0.6154 - val_loss: 0.7050 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 27ms/step - accuracy: 0.5076 - f1_score: 0.5723 - loss: 0.7027 - val_accuracy: 0.4600 - val_f1_score: 0.5574 - val_loss: 0.7065 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 27ms/step - accuracy: 0.5274 - f1_score: 0.5644 - loss: 0.6992 - val_accuracy: 0.5400 - val_f1_score: 0.5306 - val_loss: 0.7047 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 27ms/step - accuracy: 0.5317 - f1_score: 0.5690 - loss: 0.6992 - val_accuracy: 0.4600 - val_f1_score: 0.5714 - val_loss: 0.7040 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 27ms/step - accuracy: 0.5490 - f1_score: 0.6322 - loss: 0.6965 - val_accuracy: 0.4800 - val_f1_score: 0.5185 - val_loss: 0.7099 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 27ms/step - accuracy: 0.5439 - f1_score: 0.6095 - loss: 0.6955 - val_accuracy: 0.4200 - val_f1_score: 0.5246 - val_loss: 0.7208 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 27ms/step - accuracy: 0.5363 - f1_score: 0.5721 - loss: 0.6970 - val_accuracy: 0.5000 - val_f1_score: 0.3902 - val_loss: 0.7202 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 27ms/step - accuracy: 0.5224 - f1_score: 0.5996 - loss: 0.7007 - val_accuracy: 0.3600 - val_f1_score: 0.4667 - val_loss: 0.7224 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 27ms/step - accuracy: 0.5296 - f1_score: 0.6287 - loss: 0.6976 - val_accuracy: 0.4200 - val_f1_score: 0.3830 - val_loss: 0.7298 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 27ms/step - accuracy: 0.5186 - f1_score: 0.5836 - loss: 0.6974 - val_accuracy: 0.4200 - val_f1_score: 0.4082 - val_loss: 0.7233 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 27ms/step - accuracy: 0.5367 - f1_score: 0.5989 - loss: 0.6974 - val_accuracy: 0.4400 - val_f1_score: 0.5333 - val_loss: 0.7204 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 27ms/step - accuracy: 0.5448 - f1_score: 0.5975 - loss: 0.6903 - val_accuracy: 0.4000 - val_f1_score: 0.4000 - val_loss: 0.7138 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 27ms/step - accuracy: 0.5427 - f1_score: 0.5969 - loss: 0.6934 - val_accuracy: 0.4200 - val_f1_score: 0.3556 - val_loss: 0.7286 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 27ms/step - accuracy: 0.5486 - f1_score: 0.6277 - loss: 0.6933 - val_accuracy: 0.5400 - val_f1_score: 0.5490 - val_loss: 0.7115 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 27ms/step - accuracy: 0.5329 - f1_score: 0.5820 - loss: 0.6938 - val_accuracy: 0.4800 - val_f1_score: 0.4800 - val_loss: 0.7191 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 27ms/step - accuracy: 0.5312 - f1_score: 0.5779 - loss: 0.6954 - val_accuracy: 0.4600 - val_f1_score: 0.3415 - val_loss: 0.7211 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 27ms/step - accuracy: 0.5249 - f1_score: 0.5688 - loss: 0.6973 - val_accuracy: 0.4600 - val_f1_score: 0.5091 - val_loss: 0.7061 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 27ms/step - accuracy: 0.5427 - f1_score: 0.5718 - loss: 0.6907 - val_accuracy: 0.5600 - val_f1_score: 0.4500 - val_loss: 0.7019 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 27ms/step - accuracy: 0.5427 - f1_score: 0.5697 - loss: 0.6939 - val_accuracy: 0.4400 - val_f1_score: 0.3333 - val_loss: 0.7128 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 27ms/step - accuracy: 0.5410 - f1_score: 0.5782 - loss: 0.6921 - val_accuracy: 0.5800 - val_f1_score: 0.5116 - val_loss: 0.7113 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 27ms/step - accuracy: 0.5579 - f1_score: 0.5918 - loss: 0.6886 - val_accuracy: 0.4800 - val_f1_score: 0.3500 - val_loss: 0.7158 - learning_rate: 6.5084e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394ms/step\noldA → val F1 = 0.6154\n              precision    recall  f1-score   support\n\n        Left       0.71      0.18      0.29        28\n       Right       0.47      0.91      0.62        22\n\n    accuracy                           0.50        50\n   macro avg       0.59      0.54      0.45        50\nweighted avg       0.60      0.50      0.43        50\n\n\n>>> Training oldB\nEpoch 1/200\n37/37 - 14s - 375ms/step - accuracy: 0.5093 - f1_score: 0.5230 - loss: 2.2433 - val_accuracy: 0.4600 - val_f1_score: 0.5970 - val_loss: 0.7570 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 32ms/step - accuracy: 0.5279 - f1_score: 0.5206 - loss: 1.2567 - val_accuracy: 0.4800 - val_f1_score: 0.4091 - val_loss: 0.7434 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 36ms/step - accuracy: 0.5106 - f1_score: 0.5275 - loss: 1.0671 - val_accuracy: 0.5400 - val_f1_score: 0.6102 - val_loss: 0.7364 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 32ms/step - accuracy: 0.5156 - f1_score: 0.5134 - loss: 0.8874 - val_accuracy: 0.4800 - val_f1_score: 0.5517 - val_loss: 0.7434 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 36ms/step - accuracy: 0.5110 - f1_score: 0.5000 - loss: 0.8480 - val_accuracy: 0.6000 - val_f1_score: 0.6875 - val_loss: 0.7440 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 32ms/step - accuracy: 0.4886 - f1_score: 0.4960 - loss: 0.8326 - val_accuracy: 0.4800 - val_f1_score: 0.5357 - val_loss: 0.7478 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 32ms/step - accuracy: 0.5093 - f1_score: 0.5158 - loss: 0.7822 - val_accuracy: 0.4600 - val_f1_score: 0.4706 - val_loss: 0.7537 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 31ms/step - accuracy: 0.5139 - f1_score: 0.5675 - loss: 0.7930 - val_accuracy: 0.5000 - val_f1_score: 0.5763 - val_loss: 0.7479 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 32ms/step - accuracy: 0.5004 - f1_score: 0.4664 - loss: 0.7792 - val_accuracy: 0.5200 - val_f1_score: 0.6129 - val_loss: 0.7453 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 32ms/step - accuracy: 0.5021 - f1_score: 0.5999 - loss: 0.7870 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7479 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 31ms/step - accuracy: 0.5215 - f1_score: 0.5836 - loss: 0.7598 - val_accuracy: 0.4200 - val_f1_score: 0.5538 - val_loss: 0.7483 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 32ms/step - accuracy: 0.5127 - f1_score: 0.5417 - loss: 0.7570 - val_accuracy: 0.4000 - val_f1_score: 0.4828 - val_loss: 0.7482 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 32ms/step - accuracy: 0.5165 - f1_score: 0.5798 - loss: 0.7610 - val_accuracy: 0.4400 - val_f1_score: 0.5625 - val_loss: 0.7478 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 32ms/step - accuracy: 0.5249 - f1_score: 0.5908 - loss: 0.7531 - val_accuracy: 0.4400 - val_f1_score: 0.6000 - val_loss: 0.7474 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 32ms/step - accuracy: 0.5211 - f1_score: 0.6220 - loss: 0.7560 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7481 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 32ms/step - accuracy: 0.5224 - f1_score: 0.6082 - loss: 0.7594 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7505 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 32ms/step - accuracy: 0.5127 - f1_score: 0.5773 - loss: 0.7577 - val_accuracy: 0.4800 - val_f1_score: 0.5806 - val_loss: 0.7465 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 32ms/step - accuracy: 0.5190 - f1_score: 0.6264 - loss: 0.7573 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7484 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 32ms/step - accuracy: 0.5127 - f1_score: 0.5710 - loss: 0.7544 - val_accuracy: 0.4200 - val_f1_score: 0.5085 - val_loss: 0.7476 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 32ms/step - accuracy: 0.5258 - f1_score: 0.5796 - loss: 0.7550 - val_accuracy: 0.4800 - val_f1_score: 0.4583 - val_loss: 0.7442 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 32ms/step - accuracy: 0.5329 - f1_score: 0.6036 - loss: 0.7467 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7503 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 32ms/step - accuracy: 0.5139 - f1_score: 0.5951 - loss: 0.7508 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7493 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 32ms/step - accuracy: 0.5207 - f1_score: 0.6162 - loss: 0.7465 - val_accuracy: 0.4600 - val_f1_score: 0.6197 - val_loss: 0.7470 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 32ms/step - accuracy: 0.5080 - f1_score: 0.6118 - loss: 0.7508 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7484 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 33ms/step - accuracy: 0.5114 - f1_score: 0.6208 - loss: 0.7487 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7490 - learning_rate: 7.3832e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611ms/step\noldB → val F1 = 0.6875\n              precision    recall  f1-score   support\n\n        Left       1.00      0.29      0.44        28\n       Right       0.52      1.00      0.69        22\n\n    accuracy                           0.60        50\n   macro avg       0.76      0.64      0.57        50\nweighted avg       0.79      0.60      0.55        50\n\n\n>>> Training model1\nEpoch 1/200\n37/37 - 27s - 716ms/step - accuracy: 0.5110 - f1_score: 0.6760 - loss: 0.6936 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6943 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 24ms/step - accuracy: 0.5084 - f1_score: 0.6156 - loss: 0.6938 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6891 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 22ms/step - accuracy: 0.4894 - f1_score: 0.4862 - loss: 0.6937 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6942 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 25ms/step - accuracy: 0.5114 - f1_score: 0.6767 - loss: 0.6930 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6952 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 24ms/step - accuracy: 0.4987 - f1_score: 0.5239 - loss: 0.6938 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6941 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 24ms/step - accuracy: 0.4818 - f1_score: 0.5471 - loss: 0.6935 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6943 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 22ms/step - accuracy: 0.4966 - f1_score: 0.5391 - loss: 0.6932 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6927 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 22ms/step - accuracy: 0.5025 - f1_score: 0.5413 - loss: 0.6930 - val_accuracy: 0.4600 - val_f1_score: 0.6087 - val_loss: 0.6940 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 23ms/step - accuracy: 0.4937 - f1_score: 0.6454 - loss: 0.6925 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6923 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 22ms/step - accuracy: 0.5114 - f1_score: 0.6250 - loss: 0.6929 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6930 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 22ms/step - accuracy: 0.5059 - f1_score: 0.2687 - loss: 0.6934 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6954 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 22ms/step - accuracy: 0.5042 - f1_score: 0.6704 - loss: 0.6934 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6936 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 22ms/step - accuracy: 0.5055 - f1_score: 0.6715 - loss: 0.6935 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6950 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 22ms/step - accuracy: 0.4840 - f1_score: 0.4268 - loss: 0.6935 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6937 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 21ms/step - accuracy: 0.4992 - f1_score: 0.6659 - loss: 0.6932 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6935 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 22ms/step - accuracy: 0.4987 - f1_score: 0.6466 - loss: 0.6932 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6929 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 22ms/step - accuracy: 0.4882 - f1_score: 0.5538 - loss: 0.6932 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6931 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 24ms/step - accuracy: 0.4996 - f1_score: 0.0000e+00 - loss: 0.6933 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6927 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 22ms/step - accuracy: 0.4937 - f1_score: 0.3880 - loss: 0.6932 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6939 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 22ms/step - accuracy: 0.5051 - f1_score: 0.6712 - loss: 0.6933 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6934 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 23ms/step - accuracy: 0.4928 - f1_score: 0.5343 - loss: 0.6933 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6925 - learning_rate: 8.3736e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5s/step\nmodel1 → val F1 = 0.6111\n              precision    recall  f1-score   support\n\n        Left       0.00      0.00      0.00        28\n       Right       0.44      1.00      0.61        22\n\n    accuracy                           0.44        50\n   macro avg       0.22      0.50      0.31        50\nweighted avg       0.19      0.44      0.27        50\n\n\n>>> Training model2\nEpoch 1/200\n37/37 - 4s - 118ms/step - accuracy: 0.5013 - f1_score: 0.4231 - loss: 0.6985 - val_accuracy: 0.6000 - val_f1_score: 0.6429 - val_loss: 0.6807 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 21ms/step - accuracy: 0.5013 - f1_score: 0.4977 - loss: 0.6955 - val_accuracy: 0.5000 - val_f1_score: 0.6032 - val_loss: 0.6968 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 21ms/step - accuracy: 0.5152 - f1_score: 0.5081 - loss: 0.6939 - val_accuracy: 0.5600 - val_f1_score: 0.5217 - val_loss: 0.6783 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 23ms/step - accuracy: 0.5118 - f1_score: 0.4952 - loss: 0.6937 - val_accuracy: 0.5600 - val_f1_score: 0.2143 - val_loss: 0.6902 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 23ms/step - accuracy: 0.4894 - f1_score: 0.5330 - loss: 0.6950 - val_accuracy: 0.4600 - val_f1_score: 0.4706 - val_loss: 0.6956 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 22ms/step - accuracy: 0.5160 - f1_score: 0.5090 - loss: 0.6932 - val_accuracy: 0.4400 - val_f1_score: 0.3333 - val_loss: 0.7015 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 22ms/step - accuracy: 0.5004 - f1_score: 0.5363 - loss: 0.6953 - val_accuracy: 0.4200 - val_f1_score: 0.5915 - val_loss: 0.6989 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 21ms/step - accuracy: 0.5296 - f1_score: 0.6837 - loss: 0.6916 - val_accuracy: 0.4200 - val_f1_score: 0.5538 - val_loss: 0.7116 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 21ms/step - accuracy: 0.5025 - f1_score: 0.5935 - loss: 0.6930 - val_accuracy: 0.5000 - val_f1_score: 0.5098 - val_loss: 0.6953 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 21ms/step - accuracy: 0.4852 - f1_score: 0.3621 - loss: 0.6945 - val_accuracy: 0.4600 - val_f1_score: 0.4706 - val_loss: 0.6950 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 22ms/step - accuracy: 0.5000 - f1_score: 0.4780 - loss: 0.6938 - val_accuracy: 0.4200 - val_f1_score: 0.4314 - val_loss: 0.6970 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 22ms/step - accuracy: 0.5270 - f1_score: 0.5960 - loss: 0.6919 - val_accuracy: 0.4400 - val_f1_score: 0.5484 - val_loss: 0.6984 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 21ms/step - accuracy: 0.4954 - f1_score: 0.5489 - loss: 0.6937 - val_accuracy: 0.4200 - val_f1_score: 0.5915 - val_loss: 0.7000 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 21ms/step - accuracy: 0.5122 - f1_score: 0.6241 - loss: 0.6940 - val_accuracy: 0.4000 - val_f1_score: 0.4231 - val_loss: 0.7005 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 22ms/step - accuracy: 0.5122 - f1_score: 0.4341 - loss: 0.6926 - val_accuracy: 0.4000 - val_f1_score: 0.4000 - val_loss: 0.7014 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 22ms/step - accuracy: 0.5042 - f1_score: 0.5312 - loss: 0.6931 - val_accuracy: 0.4800 - val_f1_score: 0.4348 - val_loss: 0.6985 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 21ms/step - accuracy: 0.5000 - f1_score: 0.4705 - loss: 0.6941 - val_accuracy: 0.4000 - val_f1_score: 0.4000 - val_loss: 0.7056 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 21ms/step - accuracy: 0.5207 - f1_score: 0.5663 - loss: 0.6920 - val_accuracy: 0.4200 - val_f1_score: 0.3830 - val_loss: 0.7108 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 23ms/step - accuracy: 0.5220 - f1_score: 0.5083 - loss: 0.6912 - val_accuracy: 0.4200 - val_f1_score: 0.4314 - val_loss: 0.7055 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 22ms/step - accuracy: 0.5236 - f1_score: 0.5963 - loss: 0.6919 - val_accuracy: 0.5200 - val_f1_score: 0.6000 - val_loss: 0.7023 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 21ms/step - accuracy: 0.5228 - f1_score: 0.5484 - loss: 0.6923 - val_accuracy: 0.5200 - val_f1_score: 0.5000 - val_loss: 0.6959 - learning_rate: 8.3736e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260ms/step\nmodel2 → val F1 = 0.6429\n              precision    recall  f1-score   support\n\n        Left       0.75      0.43      0.55        28\n       Right       0.53      0.82      0.64        22\n\n    accuracy                           0.60        50\n   macro avg       0.64      0.62      0.59        50\nweighted avg       0.65      0.60      0.59        50\n\n\n>>> Training model3\nEpoch 1/200\n37/37 - 10s - 262ms/step - accuracy: 0.5152 - f1_score: 0.5578 - loss: 0.7102 - val_accuracy: 0.4400 - val_f1_score: 0.5882 - val_loss: 0.7034 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 3s - 87ms/step - accuracy: 0.5646 - f1_score: 0.6485 - loss: 0.6770 - val_accuracy: 0.5600 - val_f1_score: 0.5926 - val_loss: 0.7029 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 39ms/step - accuracy: 0.6457 - f1_score: 0.6585 - loss: 0.6413 - val_accuracy: 0.5200 - val_f1_score: 0.5714 - val_loss: 0.7289 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 39ms/step - accuracy: 0.7475 - f1_score: 0.7604 - loss: 0.5298 - val_accuracy: 0.4800 - val_f1_score: 0.4800 - val_loss: 0.9992 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 39ms/step - accuracy: 0.8530 - f1_score: 0.8587 - loss: 0.3617 - val_accuracy: 0.4600 - val_f1_score: 0.3077 - val_loss: 1.2264 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 39ms/step - accuracy: 0.9231 - f1_score: 0.9231 - loss: 0.2368 - val_accuracy: 0.4800 - val_f1_score: 0.5185 - val_loss: 1.7606 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 39ms/step - accuracy: 0.9561 - f1_score: 0.9573 - loss: 0.1288 - val_accuracy: 0.4400 - val_f1_score: 0.5172 - val_loss: 1.8940 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 39ms/step - accuracy: 0.9772 - f1_score: 0.9775 - loss: 0.0818 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 2.6832 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 39ms/step - accuracy: 0.9797 - f1_score: 0.9798 - loss: 0.0704 - val_accuracy: 0.4600 - val_f1_score: 0.5091 - val_loss: 2.4576 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 39ms/step - accuracy: 0.9958 - f1_score: 0.9960 - loss: 0.0185 - val_accuracy: 0.4600 - val_f1_score: 0.5091 - val_loss: 3.9814 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 39ms/step - accuracy: 0.9987 - f1_score: 0.9987 - loss: 0.0070 - val_accuracy: 0.4200 - val_f1_score: 0.4912 - val_loss: 5.6273 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 39ms/step - accuracy: 0.9992 - f1_score: 0.9992 - loss: 0.0030 - val_accuracy: 0.4800 - val_f1_score: 0.5357 - val_loss: 5.0269 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 39ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 7.9036e-04 - val_accuracy: 0.4200 - val_f1_score: 0.5085 - val_loss: 6.4344 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 39ms/step - accuracy: 0.9966 - f1_score: 0.9966 - loss: 0.0122 - val_accuracy: 0.4800 - val_f1_score: 0.5000 - val_loss: 4.1338 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 39ms/step - accuracy: 0.9954 - f1_score: 0.9955 - loss: 0.0114 - val_accuracy: 0.5000 - val_f1_score: 0.4681 - val_loss: 4.1160 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 39ms/step - accuracy: 0.9962 - f1_score: 0.9963 - loss: 0.0138 - val_accuracy: 0.4200 - val_f1_score: 0.4314 - val_loss: 4.5441 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 39ms/step - accuracy: 0.9966 - f1_score: 0.9966 - loss: 0.0084 - val_accuracy: 0.3600 - val_f1_score: 0.3600 - val_loss: 5.4916 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 39ms/step - accuracy: 0.9949 - f1_score: 0.9951 - loss: 0.0237 - val_accuracy: 0.4600 - val_f1_score: 0.5091 - val_loss: 4.0318 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 39ms/step - accuracy: 0.9996 - f1_score: 0.9996 - loss: 0.0021 - val_accuracy: 0.5000 - val_f1_score: 0.5283 - val_loss: 5.7710 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 39ms/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 2.7808e-04 - val_accuracy: 0.4600 - val_f1_score: 0.5091 - val_loss: 6.6225 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 39ms/step - accuracy: 0.9975 - f1_score: 0.9975 - loss: 0.0088 - val_accuracy: 0.4800 - val_f1_score: 0.4091 - val_loss: 5.8489 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 39ms/step - accuracy: 0.9996 - f1_score: 0.9996 - loss: 0.0016 - val_accuracy: 0.4200 - val_f1_score: 0.4727 - val_loss: 6.8015 - learning_rate: 8.1479e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 308ms/step\nmodel3 → val F1 = 0.5926\n              precision    recall  f1-score   support\n\n        Left       0.67      0.43      0.52        28\n       Right       0.50      0.73      0.59        22\n\n    accuracy                           0.56        50\n   macro avg       0.58      0.58      0.56        50\nweighted avg       0.59      0.56      0.55        50\n\n\n>>> Training model4\nEpoch 1/200\n37/37 - 9s - 243ms/step - accuracy: 0.5182 - f1_score: 0.5534 - loss: 0.6923 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7142 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 4s - 117ms/step - accuracy: 0.5190 - f1_score: 0.6462 - loss: 0.6939 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7259 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 4s - 117ms/step - accuracy: 0.5334 - f1_score: 0.5891 - loss: 0.6899 - val_accuracy: 0.4600 - val_f1_score: 0.5714 - val_loss: 0.7068 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 4s - 117ms/step - accuracy: 0.5355 - f1_score: 0.5865 - loss: 0.6913 - val_accuracy: 0.5000 - val_f1_score: 0.6032 - val_loss: 0.7130 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 4s - 117ms/step - accuracy: 0.5245 - f1_score: 0.5866 - loss: 0.6922 - val_accuracy: 0.4400 - val_f1_score: 0.5333 - val_loss: 0.6983 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 4s - 117ms/step - accuracy: 0.5359 - f1_score: 0.5556 - loss: 0.6908 - val_accuracy: 0.5600 - val_f1_score: 0.5600 - val_loss: 0.6932 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 4s - 118ms/step - accuracy: 0.5418 - f1_score: 0.5408 - loss: 0.6884 - val_accuracy: 0.5800 - val_f1_score: 0.6316 - val_loss: 0.6998 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 4s - 116ms/step - accuracy: 0.5393 - f1_score: 0.5166 - loss: 0.6869 - val_accuracy: 0.5600 - val_f1_score: 0.6071 - val_loss: 0.7008 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 4s - 117ms/step - accuracy: 0.5562 - f1_score: 0.5987 - loss: 0.6856 - val_accuracy: 0.4200 - val_f1_score: 0.3256 - val_loss: 0.6963 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 4s - 117ms/step - accuracy: 0.5410 - f1_score: 0.6066 - loss: 0.6874 - val_accuracy: 0.5200 - val_f1_score: 0.6250 - val_loss: 0.7116 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 4s - 117ms/step - accuracy: 0.5448 - f1_score: 0.5769 - loss: 0.6877 - val_accuracy: 0.5600 - val_f1_score: 0.5417 - val_loss: 0.6883 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 4s - 117ms/step - accuracy: 0.5490 - f1_score: 0.5568 - loss: 0.6859 - val_accuracy: 0.5000 - val_f1_score: 0.4681 - val_loss: 0.7011 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 4s - 117ms/step - accuracy: 0.5528 - f1_score: 0.5253 - loss: 0.6863 - val_accuracy: 0.5000 - val_f1_score: 0.2857 - val_loss: 0.7113 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 4s - 118ms/step - accuracy: 0.5325 - f1_score: 0.5409 - loss: 0.6892 - val_accuracy: 0.3400 - val_f1_score: 0.4590 - val_loss: 0.7014 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 4s - 117ms/step - accuracy: 0.5625 - f1_score: 0.6052 - loss: 0.6848 - val_accuracy: 0.4800 - val_f1_score: 0.5667 - val_loss: 0.6972 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 4s - 116ms/step - accuracy: 0.5422 - f1_score: 0.5059 - loss: 0.6876 - val_accuracy: 0.5200 - val_f1_score: 0.5200 - val_loss: 0.6894 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 4s - 117ms/step - accuracy: 0.5562 - f1_score: 0.6311 - loss: 0.6798 - val_accuracy: 0.4200 - val_f1_score: 0.5538 - val_loss: 0.7143 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 4s - 117ms/step - accuracy: 0.5515 - f1_score: 0.5789 - loss: 0.6836 - val_accuracy: 0.4600 - val_f1_score: 0.5714 - val_loss: 0.6954 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 4s - 117ms/step - accuracy: 0.5490 - f1_score: 0.6266 - loss: 0.6813 - val_accuracy: 0.4800 - val_f1_score: 0.6176 - val_loss: 0.6979 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 4s - 117ms/step - accuracy: 0.5351 - f1_score: 0.6334 - loss: 0.6872 - val_accuracy: 0.5800 - val_f1_score: 0.6182 - val_loss: 0.6994 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 4s - 117ms/step - accuracy: 0.5359 - f1_score: 0.5644 - loss: 0.6853 - val_accuracy: 0.5200 - val_f1_score: 0.6129 - val_loss: 0.6962 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 4s - 117ms/step - accuracy: 0.5579 - f1_score: 0.6121 - loss: 0.6850 - val_accuracy: 0.5000 - val_f1_score: 0.4681 - val_loss: 0.6920 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 4s - 116ms/step - accuracy: 0.5532 - f1_score: 0.6105 - loss: 0.6801 - val_accuracy: 0.4600 - val_f1_score: 0.4490 - val_loss: 0.6918 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 4s - 117ms/step - accuracy: 0.5726 - f1_score: 0.5790 - loss: 0.6749 - val_accuracy: 0.5200 - val_f1_score: 0.4286 - val_loss: 0.6980 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 4s - 117ms/step - accuracy: 0.5435 - f1_score: 0.5935 - loss: 0.6801 - val_accuracy: 0.4600 - val_f1_score: 0.4490 - val_loss: 0.6953 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 4s - 117ms/step - accuracy: 0.5743 - f1_score: 0.6035 - loss: 0.6752 - val_accuracy: 0.5000 - val_f1_score: 0.5902 - val_loss: 0.7370 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 4s - 117ms/step - accuracy: 0.5612 - f1_score: 0.5901 - loss: 0.6775 - val_accuracy: 0.4200 - val_f1_score: 0.4528 - val_loss: 0.7227 - learning_rate: 6.8101e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275ms/step\nmodel4 → val F1 = 0.6316\n              precision    recall  f1-score   support\n\n        Left       0.73      0.39      0.51        28\n       Right       0.51      0.82      0.63        22\n\n    accuracy                           0.58        50\n   macro avg       0.62      0.61      0.57        50\nweighted avg       0.64      0.58      0.56        50\n\n\n>>> Training model5\nEpoch 1/200\n37/37 - 9s - 236ms/step - accuracy: 0.5135 - f1_score: 0.5139 - loss: 1.1822 - val_accuracy: 0.5200 - val_f1_score: 0.4545 - val_loss: 0.7030 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 23ms/step - accuracy: 0.5186 - f1_score: 0.5718 - loss: 0.7513 - val_accuracy: 0.5200 - val_f1_score: 0.2000 - val_loss: 0.7377 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 24ms/step - accuracy: 0.5300 - f1_score: 0.5657 - loss: 0.7095 - val_accuracy: 0.4400 - val_f1_score: 0.1765 - val_loss: 0.7134 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 37ms/step - accuracy: 0.5448 - f1_score: 0.5230 - loss: 0.6930 - val_accuracy: 0.5000 - val_f1_score: 0.6269 - val_loss: 0.7409 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 27ms/step - accuracy: 0.5587 - f1_score: 0.5596 - loss: 0.6809 - val_accuracy: 0.5600 - val_f1_score: 0.6071 - val_loss: 0.7121 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 26ms/step - accuracy: 0.6571 - f1_score: 0.6858 - loss: 0.6162 - val_accuracy: 0.4400 - val_f1_score: 0.4615 - val_loss: 0.9038 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 23ms/step - accuracy: 0.7310 - f1_score: 0.7461 - loss: 0.5442 - val_accuracy: 0.4400 - val_f1_score: 0.4400 - val_loss: 0.9941 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 24ms/step - accuracy: 0.7779 - f1_score: 0.7835 - loss: 0.4755 - val_accuracy: 0.5000 - val_f1_score: 0.4186 - val_loss: 1.2685 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 23ms/step - accuracy: 0.8353 - f1_score: 0.8421 - loss: 0.3811 - val_accuracy: 0.5200 - val_f1_score: 0.4545 - val_loss: 1.3296 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 23ms/step - accuracy: 0.8758 - f1_score: 0.8749 - loss: 0.3053 - val_accuracy: 0.5400 - val_f1_score: 0.5660 - val_loss: 1.2015 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 22ms/step - accuracy: 0.9345 - f1_score: 0.9353 - loss: 0.1827 - val_accuracy: 0.4800 - val_f1_score: 0.4348 - val_loss: 1.9157 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 23ms/step - accuracy: 0.9654 - f1_score: 0.9660 - loss: 0.0993 - val_accuracy: 0.4600 - val_f1_score: 0.4906 - val_loss: 2.5300 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 23ms/step - accuracy: 0.9772 - f1_score: 0.9768 - loss: 0.0713 - val_accuracy: 0.5200 - val_f1_score: 0.5000 - val_loss: 2.7400 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 23ms/step - accuracy: 0.9861 - f1_score: 0.9862 - loss: 0.0445 - val_accuracy: 0.5600 - val_f1_score: 0.5217 - val_loss: 2.9548 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 22ms/step - accuracy: 0.9911 - f1_score: 0.9913 - loss: 0.0341 - val_accuracy: 0.5000 - val_f1_score: 0.4898 - val_loss: 3.3740 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 23ms/step - accuracy: 0.9962 - f1_score: 0.9962 - loss: 0.0205 - val_accuracy: 0.5200 - val_f1_score: 0.4545 - val_loss: 3.4174 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 23ms/step - accuracy: 0.9954 - f1_score: 0.9953 - loss: 0.0158 - val_accuracy: 0.5200 - val_f1_score: 0.5000 - val_loss: 3.7464 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 23ms/step - accuracy: 0.9970 - f1_score: 0.9972 - loss: 0.0109 - val_accuracy: 0.5600 - val_f1_score: 0.5217 - val_loss: 3.9922 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 22ms/step - accuracy: 0.9983 - f1_score: 0.9983 - loss: 0.0066 - val_accuracy: 0.5400 - val_f1_score: 0.5106 - val_loss: 3.8519 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 22ms/step - accuracy: 0.9987 - f1_score: 0.9988 - loss: 0.0057 - val_accuracy: 0.5200 - val_f1_score: 0.5000 - val_loss: 4.0916 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 23ms/step - accuracy: 0.9992 - f1_score: 0.9991 - loss: 0.0030 - val_accuracy: 0.5400 - val_f1_score: 0.5106 - val_loss: 4.1712 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 23ms/step - accuracy: 0.9979 - f1_score: 0.9979 - loss: 0.0058 - val_accuracy: 0.5400 - val_f1_score: 0.4889 - val_loss: 4.3774 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 23ms/step - accuracy: 0.9979 - f1_score: 0.9979 - loss: 0.0059 - val_accuracy: 0.5400 - val_f1_score: 0.4889 - val_loss: 4.3804 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 22ms/step - accuracy: 0.9987 - f1_score: 0.9987 - loss: 0.0035 - val_accuracy: 0.5400 - val_f1_score: 0.5306 - val_loss: 4.3938 - learning_rate: 7.6518e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 415ms/step\nmodel5 → val F1 = 0.6269\n              precision    recall  f1-score   support\n\n        Left       0.80      0.14      0.24        28\n       Right       0.47      0.95      0.63        22\n\n    accuracy                           0.50        50\n   macro avg       0.63      0.55      0.43        50\nweighted avg       0.65      0.50      0.41        50\n\n\n>>> Training model6\nEpoch 1/200\n37/37 - 7s - 198ms/step - accuracy: 0.5034 - f1_score: 0.5208 - loss: 0.6948 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7043 - learning_rate: 0.0010\nEpoch 2/200\n37/37 - 1s - 38ms/step - accuracy: 0.4932 - f1_score: 0.5420 - loss: 0.6954 - val_accuracy: 0.4000 - val_f1_score: 0.4643 - val_loss: 0.7035 - learning_rate: 9.9994e-04\nEpoch 3/200\n37/37 - 1s - 38ms/step - accuracy: 0.5173 - f1_score: 0.5753 - loss: 0.6933 - val_accuracy: 0.5200 - val_f1_score: 0.3684 - val_loss: 0.6688 - learning_rate: 9.9969e-04\nEpoch 4/200\n37/37 - 1s - 38ms/step - accuracy: 0.5008 - f1_score: 0.6365 - loss: 0.6950 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6967 - learning_rate: 9.9914e-04\nEpoch 5/200\n37/37 - 1s - 38ms/step - accuracy: 0.5042 - f1_score: 0.5017 - loss: 0.6937 - val_accuracy: 0.3600 - val_f1_score: 0.1579 - val_loss: 0.7076 - learning_rate: 9.9815e-04\nEpoch 6/200\n37/37 - 1s - 38ms/step - accuracy: 0.4894 - f1_score: 0.5895 - loss: 0.6940 - val_accuracy: 0.4400 - val_f1_score: 0.5484 - val_loss: 0.7009 - learning_rate: 9.9661e-04\nEpoch 7/200\n37/37 - 1s - 38ms/step - accuracy: 0.5097 - f1_score: 0.5953 - loss: 0.6935 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6960 - learning_rate: 9.9440e-04\nEpoch 8/200\n37/37 - 1s - 38ms/step - accuracy: 0.5114 - f1_score: 0.6566 - loss: 0.6928 - val_accuracy: 0.4200 - val_f1_score: 0.5672 - val_loss: 0.6977 - learning_rate: 9.9140e-04\nEpoch 9/200\n37/37 - 1s - 38ms/step - accuracy: 0.5030 - f1_score: 0.6659 - loss: 0.6941 - val_accuracy: 0.5400 - val_f1_score: 0.5490 - val_loss: 0.6931 - learning_rate: 9.8749e-04\nEpoch 10/200\n37/37 - 1s - 38ms/step - accuracy: 0.5156 - f1_score: 0.6339 - loss: 0.6929 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6938 - learning_rate: 9.8256e-04\nEpoch 11/200\n37/37 - 1s - 38ms/step - accuracy: 0.5068 - f1_score: 0.5497 - loss: 0.6931 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6931 - learning_rate: 9.7652e-04\nEpoch 12/200\n37/37 - 1s - 38ms/step - accuracy: 0.5072 - f1_score: 0.5349 - loss: 0.6960 - val_accuracy: 0.4600 - val_f1_score: 0.0000e+00 - val_loss: 0.6813 - learning_rate: 9.6924e-04\nEpoch 13/200\n37/37 - 1s - 39ms/step - accuracy: 0.4835 - f1_score: 0.5221 - loss: 0.6951 - val_accuracy: 0.4600 - val_f1_score: 0.6197 - val_loss: 0.6936 - learning_rate: 9.6066e-04\nEpoch 14/200\n37/37 - 1s - 38ms/step - accuracy: 0.5169 - f1_score: 0.4875 - loss: 0.6929 - val_accuracy: 0.4000 - val_f1_score: 0.4000 - val_loss: 0.6995 - learning_rate: 9.5068e-04\nEpoch 15/200\n37/37 - 1s - 38ms/step - accuracy: 0.4907 - f1_score: 0.5191 - loss: 0.6947 - val_accuracy: 0.3800 - val_f1_score: 0.3673 - val_loss: 0.6947 - learning_rate: 9.3923e-04\nEpoch 16/200\n37/37 - 1s - 38ms/step - accuracy: 0.4983 - f1_score: 0.0646 - loss: 0.6936 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6921 - learning_rate: 9.2626e-04\nEpoch 17/200\n37/37 - 1s - 38ms/step - accuracy: 0.5131 - f1_score: 0.4674 - loss: 0.6928 - val_accuracy: 0.3800 - val_f1_score: 0.4151 - val_loss: 0.6996 - learning_rate: 9.1171e-04\nEpoch 18/200\n37/37 - 1s - 38ms/step - accuracy: 0.5097 - f1_score: 0.5686 - loss: 0.6940 - val_accuracy: 0.3600 - val_f1_score: 0.4286 - val_loss: 0.6977 - learning_rate: 8.9555e-04\nEpoch 19/200\n37/37 - 1s - 38ms/step - accuracy: 0.5046 - f1_score: 0.6203 - loss: 0.6935 - val_accuracy: 0.4400 - val_f1_score: 0.4167 - val_loss: 0.6951 - learning_rate: 8.7777e-04\nEpoch 20/200\n37/37 - 1s - 38ms/step - accuracy: 0.5008 - f1_score: 0.6060 - loss: 0.6935 - val_accuracy: 0.4200 - val_f1_score: 0.5672 - val_loss: 0.6948 - learning_rate: 8.5837e-04\nEpoch 21/200\n37/37 - 1s - 38ms/step - accuracy: 0.5186 - f1_score: 0.6787 - loss: 0.6931 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6982 - learning_rate: 8.3736e-04\nEpoch 22/200\n37/37 - 1s - 38ms/step - accuracy: 0.5101 - f1_score: 0.6474 - loss: 0.6934 - val_accuracy: 0.4400 - val_f1_score: 0.5625 - val_loss: 0.6967 - learning_rate: 8.1479e-04\nEpoch 23/200\n37/37 - 1s - 38ms/step - accuracy: 0.5131 - f1_score: 0.6682 - loss: 0.6925 - val_accuracy: 0.4200 - val_f1_score: 0.5672 - val_loss: 0.7006 - learning_rate: 7.9071e-04\nEpoch 24/200\n37/37 - 1s - 38ms/step - accuracy: 0.5021 - f1_score: 0.5586 - loss: 0.6938 - val_accuracy: 0.3400 - val_f1_score: 0.3774 - val_loss: 0.6966 - learning_rate: 7.6518e-04\nEpoch 25/200\n37/37 - 1s - 38ms/step - accuracy: 0.4903 - f1_score: 0.6103 - loss: 0.6936 - val_accuracy: 0.4800 - val_f1_score: 0.3810 - val_loss: 0.6936 - learning_rate: 7.3832e-04\nEpoch 26/200\n37/37 - 1s - 38ms/step - accuracy: 0.5051 - f1_score: 0.2563 - loss: 0.6931 - val_accuracy: 0.3800 - val_f1_score: 0.4151 - val_loss: 0.6949 - learning_rate: 7.1022e-04\nEpoch 27/200\n37/37 - 1s - 38ms/step - accuracy: 0.5046 - f1_score: 0.6474 - loss: 0.6934 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6984 - learning_rate: 6.8101e-04\nEpoch 28/200\n37/37 - 1s - 38ms/step - accuracy: 0.5148 - f1_score: 0.6797 - loss: 0.6927 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.6970 - learning_rate: 6.5084e-04\nEpoch 29/200\n37/37 - 1s - 38ms/step - accuracy: 0.5160 - f1_score: 0.6613 - loss: 0.6927 - val_accuracy: 0.3400 - val_f1_score: 0.3774 - val_loss: 0.6996 - learning_rate: 6.1987e-04\nEpoch 30/200\n37/37 - 1s - 38ms/step - accuracy: 0.5135 - f1_score: 0.6203 - loss: 0.6929 - val_accuracy: 0.4200 - val_f1_score: 0.5915 - val_loss: 0.6941 - learning_rate: 5.8827e-04\nEpoch 31/200\n37/37 - 1s - 38ms/step - accuracy: 0.5030 - f1_score: 0.5587 - loss: 0.6929 - val_accuracy: 0.4400 - val_f1_score: 0.6000 - val_loss: 0.6961 - learning_rate: 5.5621e-04\nEpoch 32/200\n37/37 - 1s - 40ms/step - accuracy: 0.5144 - f1_score: 0.6220 - loss: 0.6927 - val_accuracy: 0.6200 - val_f1_score: 0.6415 - val_loss: 0.6923 - learning_rate: 5.2388e-04\nEpoch 33/200\n37/37 - 1s - 38ms/step - accuracy: 0.5004 - f1_score: 0.6459 - loss: 0.6934 - val_accuracy: 0.4200 - val_f1_score: 0.5246 - val_loss: 0.7009 - learning_rate: 4.9148e-04\nEpoch 34/200\n37/37 - 1s - 38ms/step - accuracy: 0.5152 - f1_score: 0.5628 - loss: 0.6927 - val_accuracy: 0.3400 - val_f1_score: 0.4000 - val_loss: 0.6994 - learning_rate: 4.5920e-04\nEpoch 35/200\n37/37 - 1s - 38ms/step - accuracy: 0.4907 - f1_score: 0.5969 - loss: 0.6940 - val_accuracy: 0.3600 - val_f1_score: 0.4286 - val_loss: 0.6957 - learning_rate: 4.2723e-04\nEpoch 36/200\n37/37 - 1s - 38ms/step - accuracy: 0.5186 - f1_score: 0.5960 - loss: 0.6928 - val_accuracy: 0.3600 - val_f1_score: 0.3846 - val_loss: 0.6963 - learning_rate: 3.9575e-04\nEpoch 37/200\n37/37 - 1s - 38ms/step - accuracy: 0.4932 - f1_score: 0.5546 - loss: 0.6934 - val_accuracy: 0.3600 - val_f1_score: 0.3846 - val_loss: 0.6949 - learning_rate: 3.6494e-04\nEpoch 38/200\n37/37 - 1s - 38ms/step - accuracy: 0.5097 - f1_score: 0.5424 - loss: 0.6931 - val_accuracy: 0.4000 - val_f1_score: 0.5000 - val_loss: 0.6958 - learning_rate: 3.3498e-04\nEpoch 39/200\n37/37 - 1s - 38ms/step - accuracy: 0.5211 - f1_score: 0.6001 - loss: 0.6926 - val_accuracy: 0.3600 - val_f1_score: 0.4286 - val_loss: 0.6978 - learning_rate: 3.0602e-04\nEpoch 40/200\n37/37 - 1s - 38ms/step - accuracy: 0.5122 - f1_score: 0.6027 - loss: 0.6926 - val_accuracy: 0.4200 - val_f1_score: 0.5246 - val_loss: 0.6963 - learning_rate: 2.7820e-04\nEpoch 41/200\n37/37 - 1s - 38ms/step - accuracy: 0.5279 - f1_score: 0.6419 - loss: 0.6919 - val_accuracy: 0.4200 - val_f1_score: 0.5915 - val_loss: 0.6947 - learning_rate: 2.5163e-04\nEpoch 42/200\n37/37 - 1s - 38ms/step - accuracy: 0.5160 - f1_score: 0.6672 - loss: 0.6927 - val_accuracy: 0.4400 - val_f1_score: 0.5758 - val_loss: 0.6974 - learning_rate: 2.2643e-04\nEpoch 43/200\n37/37 - 1s - 38ms/step - accuracy: 0.5135 - f1_score: 0.6240 - loss: 0.6923 - val_accuracy: 0.4200 - val_f1_score: 0.5797 - val_loss: 0.6983 - learning_rate: 2.0267e-04\nEpoch 44/200\n37/37 - 1s - 38ms/step - accuracy: 0.5118 - f1_score: 0.6473 - loss: 0.6926 - val_accuracy: 0.4400 - val_f1_score: 0.5333 - val_loss: 0.6991 - learning_rate: 1.8042e-04\nEpoch 45/200\n37/37 - 1s - 38ms/step - accuracy: 0.5093 - f1_score: 0.5859 - loss: 0.6928 - val_accuracy: 0.4400 - val_f1_score: 0.5333 - val_loss: 0.6990 - learning_rate: 1.5972e-04\nEpoch 46/200\n37/37 - 1s - 38ms/step - accuracy: 0.5122 - f1_score: 0.6102 - loss: 0.6923 - val_accuracy: 0.4600 - val_f1_score: 0.5846 - val_loss: 0.6982 - learning_rate: 1.4058e-04\nEpoch 47/200\n37/37 - 1s - 38ms/step - accuracy: 0.5084 - f1_score: 0.6115 - loss: 0.6926 - val_accuracy: 0.4600 - val_f1_score: 0.5714 - val_loss: 0.6980 - learning_rate: 1.2302e-04\nEpoch 48/200\n37/37 - 1s - 38ms/step - accuracy: 0.5224 - f1_score: 0.6346 - loss: 0.6915 - val_accuracy: 0.4600 - val_f1_score: 0.5714 - val_loss: 0.6988 - learning_rate: 1.0700e-04\nEpoch 49/200\n37/37 - 1s - 38ms/step - accuracy: 0.5160 - f1_score: 0.6162 - loss: 0.6921 - val_accuracy: 0.4600 - val_f1_score: 0.5714 - val_loss: 0.6991 - learning_rate: 9.2503e-05\nEpoch 50/200\n37/37 - 1s - 38ms/step - accuracy: 0.5279 - f1_score: 0.6184 - loss: 0.6910 - val_accuracy: 0.4400 - val_f1_score: 0.5333 - val_loss: 0.6999 - learning_rate: 7.9466e-05\nEpoch 51/200\n37/37 - 1s - 38ms/step - accuracy: 0.5030 - f1_score: 0.5872 - loss: 0.6928 - val_accuracy: 0.4000 - val_f1_score: 0.5161 - val_loss: 0.6993 - learning_rate: 6.7829e-05\nEpoch 52/200\n37/37 - 1s - 38ms/step - accuracy: 0.5203 - f1_score: 0.5989 - loss: 0.6927 - val_accuracy: 0.4200 - val_f1_score: 0.5085 - val_loss: 0.6994 - learning_rate: 5.7516e-05\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 411ms/step\nmodel6 → val F1 = 0.6415\n              precision    recall  f1-score   support\n\n        Left       0.74      0.50      0.60        28\n       Right       0.55      0.77      0.64        22\n\n    accuracy                           0.62        50\n   macro avg       0.64      0.64      0.62        50\nweighted avg       0.65      0.62      0.62        50\n\n\n=== Final best: oldB (F1=0.6875) ===\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"    import os\n    import numpy as np\n    from sklearn.metrics import f1_score\n    import tensorflow as tf\n    \n    # Function to compute best threshold for a model\n    def find_best_threshold(model, val_x, y_true_bin):\n        probs = model.predict(val_x)[:, 1]\n        thresholds = np.linspace(0.1, 1.0, 100)\n        best_f1 = 0\n        best_threshold = 0.5\n        \n        for t in thresholds:\n            preds_bin = (probs > t).astype(int)\n            # Use macro averaging here\n            f1 = f1_score(y_true_bin, preds_bin, average='macro')  # Changed to macro\n            if f1 > best_f1:\n                best_f1 = f1\n                best_threshold = t\n                \n        return best_threshold, best_f1\n    \n    # Load validation data\n    val_npz = np.load(\"/kaggle/working/preprocessed/validation_MI.npz\")\n    X_val_all, y_val = val_npz[\"X\"], val_npz[\"y\"]\n    y_val_bin = (y_val == \"Right\").astype(int)\n    \n    # Use only EEG channels: C3, CZ, C4, PZ\n    eeg_indices = [1, 2, 3, 4]\n    X_val_raw = X_val_all[:, eeg_indices, :].transpose(0, 2, 1).astype('float32')\n    \n    # Directories to check\n    dirs = [\"/kaggle/working/models3\"]\n    # \n    # Loop through models\n    results = {}\n    for model_dir in dirs:\n        for file in os.listdir(model_dir):\n            if file.endswith(\".h5\"):\n                path = os.path.join(model_dir, file)\n                try:\n                    model = tf.keras.models.load_model(path, compile=False)\n                    threshold, f1 = find_best_threshold(model, X_val_raw, y_val_bin)\n                    results[path] = (threshold, f1)\n                except Exception as e:\n                    results[path] = f\"Error: {e}\"\n    \n    # Print results\n    for model_path, (threshold, f1) in results.items():\n        print(f\"{model_path} → Best threshold = {threshold:.2f}, F1 score = {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:00:46.022767Z","iopub.execute_input":"2025-06-29T10:00:46.022975Z","iopub.status.idle":"2025-06-29T10:01:06.195809Z","shell.execute_reply.started":"2025-06-29T10:00:46.022960Z","shell.execute_reply":"2025-06-29T10:01:06.195061Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 385ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 631ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 418ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 414ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 327ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261ms/step\n/kaggle/working/models3/best_model5.h5 → Best threshold = 0.52, F1 score = 0.5000\n/kaggle/working/models3/best_oldB.h5 → Best threshold = 0.51, F1 score = 0.6970\n/kaggle/working/models3/best_oldA.h5 → Best threshold = 0.52, F1 score = 0.5798\n/kaggle/working/models3/best_model6.h5 → Best threshold = 0.50, F1 score = 0.6186\n/kaggle/working/models3/best_model4.h5 → Best threshold = 0.50, F1 score = 0.5308\n/kaggle/working/models3/best_final.h5 → Best threshold = 0.51, F1 score = 0.6970\n/kaggle/working/models3/best_model1.h5 → Best threshold = 0.51, F1 score = 0.3590\n/kaggle/working/models3/best_model3.h5 → Best threshold = 0.40, F1 score = 0.5994\n/kaggle/working/models3/best_model2.h5 → Best threshold = 0.52, F1 score = 0.6162\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.metrics import f1_score, classification_report\nimport tensorflow as tf\n\n# Function to compute best threshold and generate classification report\ndef evaluate_model(model, val_x, y_true_str):\n    # Convert true labels to binary (Right=1, Left=0)\n    y_true_bin = np.array([1 if label == \"Right\" else 0 for label in y_true_str])\n    \n    # Get prediction probabilities for class 1 (Right)\n    probs = model.predict(val_x, verbose=0)[:, 1]\n    \n    # Find best threshold\n    thresholds = np.linspace(0.1, 1.0, 100)\n    best_f1 = 0\n    best_threshold = 0.5\n    for t in thresholds:\n        preds_bin = (probs > t).astype(int)\n        f1 = f1_score(y_true_bin, preds_bin)\n        if f1 > best_f1:\n            best_f1 = f1\n            best_threshold = t\n    \n    # Generate predictions using best threshold\n    pred_labels = [\"Right\" if prob > best_threshold else \"Left\" for prob in probs]\n    \n    # Generate classification report\n    report = classification_report(\n        y_true_str, \n        pred_labels,\n        target_names=[\"Left\", \"Right\"],\n        digits=4\n    )\n    \n    return best_threshold, best_f1, report\n\n# Load validation data\nval_npz = np.load(\"/kaggle/working/preprocessed/validation_MI.npz\")\nX_val_all, y_val = val_npz[\"X\"], val_npz[\"y\"]\n\n# Use only EEG channels: C3, CZ, C4, PZ\neeg_indices = [1, 2, 3, 4]  # Update if your channel order differs\nX_val_raw = X_val_all[:, eeg_indices, :].transpose(0, 2, 1).astype('float32')\n\n# Directories to check\ndirs = [\n    \"/kaggle/working/models3\"\n]\n\n# Evaluate models\nresults = {}\nfor model_dir in dirs:\n    for file in os.listdir(model_dir):\n        if file.endswith(\".h5\"):\n            path = os.path.join(model_dir, file)\n            try:\n                model = tf.keras.models.load_model(path, compile=False)\n                threshold, f1, report = evaluate_model(model, X_val_raw, y_val)\n                results[path] = {\n                    \"threshold\": threshold,\n                    \"f1\": f1,\n                    \"report\": report\n                }\n            except Exception as e:\n                results[path] = f\"Error: {e}\"\n\n# Print results with classification reports\nfor path, result in results.items():\n    if isinstance(result, str):\n        print(f\"\\n{path} → {result}\")\n    else:\n        print(f\"\\n{'-'*80}\")\n        print(f\"Model: {path}\")\n        print(f\"Best threshold: {result['threshold']:.4f}\")\n        print(f\"Best F1-score: {result['f1']:.4f}\")\n        print(\"\\nClassification Report:\")\n        print(result['report'])\n        print(f\"{'-'*80}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:01:06.196707Z","iopub.execute_input":"2025-06-29T10:01:06.196955Z","iopub.status.idle":"2025-06-29T10:01:26.134071Z","shell.execute_reply.started":"2025-06-29T10:01:06.196929Z","shell.execute_reply":"2025-06-29T10:01:26.133327Z"}},"outputs":[{"name":"stdout","text":"\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_model5.h5\nBest threshold: 0.4364\nBest F1-score: 0.6197\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     1.0000    0.0357    0.0690        28\n       Right     0.4490    1.0000    0.6197        22\n\n    accuracy                         0.4600        50\n   macro avg     0.7245    0.5179    0.3443        50\nweighted avg     0.7576    0.4600    0.3113        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_oldB.h5\nBest threshold: 0.5000\nBest F1-score: 0.6875\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     1.0000    0.2857    0.4444        28\n       Right     0.5238    1.0000    0.6875        22\n\n    accuracy                         0.6000        50\n   macro avg     0.7619    0.6429    0.5660        50\nweighted avg     0.7905    0.6000    0.5514        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_oldA.h5\nBest threshold: 0.5000\nBest F1-score: 0.6154\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     0.7143    0.1786    0.2857        28\n       Right     0.4651    0.9091    0.6154        22\n\n    accuracy                         0.5000        50\n   macro avg     0.5897    0.5438    0.4505        50\nweighted avg     0.6047    0.5000    0.4308        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_model6.h5\nBest threshold: 0.5000\nBest F1-score: 0.6415\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     0.7368    0.5000    0.5957        28\n       Right     0.5484    0.7727    0.6415        22\n\n    accuracy                         0.6200        50\n   macro avg     0.6426    0.6364    0.6186        50\nweighted avg     0.6539    0.6200    0.6159        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_model4.h5\nBest threshold: 0.4545\nBest F1-score: 0.6176\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     0.7500    0.1071    0.1875        28\n       Right     0.4565    0.9545    0.6176        22\n\n    accuracy                         0.4800        50\n   macro avg     0.6033    0.5308    0.4026        50\nweighted avg     0.6209    0.4800    0.3768        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_final.h5\nBest threshold: 0.5000\nBest F1-score: 0.6875\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     1.0000    0.2857    0.4444        28\n       Right     0.5238    1.0000    0.6875        22\n\n    accuracy                         0.6000        50\n   macro avg     0.7619    0.6429    0.5660        50\nweighted avg     0.7905    0.6000    0.5514        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_model1.h5\nBest threshold: 0.1000\nBest F1-score: 0.6111\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     0.0000    0.0000    0.0000        28\n       Right     0.4400    1.0000    0.6111        22\n\n    accuracy                         0.4400        50\n   macro avg     0.2200    0.5000    0.3056        50\nweighted avg     0.1936    0.4400    0.2689        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_model3.h5\nBest threshold: 0.2909\nBest F1-score: 0.6452\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     0.8000    0.2857    0.4211        28\n       Right     0.5000    0.9091    0.6452        22\n\n    accuracy                         0.5600        50\n   macro avg     0.6500    0.5974    0.5331        50\nweighted avg     0.6680    0.5600    0.5197        50\n\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\nModel: /kaggle/working/models3/best_model2.h5\nBest threshold: 0.4727\nBest F1-score: 0.6562\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Left     0.8750    0.2500    0.3889        28\n       Right     0.5000    0.9545    0.6562        22\n\n    accuracy                         0.5600        50\n   macro avg     0.6875    0.6023    0.5226        50\nweighted avg     0.7100    0.5600    0.5065        50\n\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\n\n# Configuration\nbase_path = \"/kaggle/input/mtcaic3\"\npreprocessed_dir = '/kaggle/working/preprocessed'  # Directory with preprocessed .npz files\nmodel_path = '/kaggle/working/models3/best_oldA.h5'\noutput_file = '/kaggle/working/submission.csv'\nthreshold = 0.5  # Prediction threshold for MI task\n\n# Load test data and sample submission\ntest_df = pd.read_csv(os.path.join(base_path, 'test.csv'))\nsample_sub = pd.read_csv(os.path.join(base_path, 'sample_submission.csv'))\n\n# Fixed F1Score metric class\nclass F1Score(tf.keras.metrics.Metric):\n    def __init__(self, name=\"f1_score\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        preds = tf.argmax(y_pred, axis=1)\n        labels = tf.argmax(y_true, axis=1)\n        \n        true_pos = tf.logical_and(tf.equal(preds, 1), tf.equal(labels, 1))\n        false_pos = tf.logical_and(tf.equal(preds, 1), tf.equal(labels, 0))\n        false_neg = tf.logical_and(tf.equal(preds, 0), tf.equal(labels, 1))\n        \n        self.tp.assign_add(tf.reduce_sum(tf.cast(true_pos, tf.float32)))\n        self.fp.assign_add(tf.reduce_sum(tf.cast(false_pos, tf.float32)))\n        self.fn.assign_add(tf.reduce_sum(tf.cast(false_neg, tf.float32)))\n\n    def result(self):\n        precision = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n        recall = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n        return 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n\n    def reset_states(self):\n        self.tp.assign(0.0)\n        self.fp.assign(0.0)\n        self.fn.assign(0.0)\n\n# Load model with fixed metric\nmodel = load_model(model_path, custom_objects={'F1Score': F1Score})\n\n# Define EEG channels to extract and their indices in preprocessed data\nchannel_names = ['FZ','C3','CZ','C4','PZ','PO7','OZ','PO8']\ntarget_channels = ['C3','CZ','C4','PZ']\ntarget_indices = [channel_names.index(ch) for ch in target_channels]\n\n# Load preprocessed test data\ntest_mi = np.load(os.path.join(preprocessed_dir, 'test_MI.npz'))['X']  # Shape (n_mi, 8, 2250)\ntest_ssvep = np.load(os.path.join(preprocessed_dir, 'test_SSVEP.npz'))['X']  # Shape (n_ssvep, 8, 1750)\n\n# Extract target channels and transpose to (trials, time, channels)\ntest_mi = test_mi[:, target_indices, :].transpose(0, 2, 1)  # New shape: (n_mi, 2250, 4)\ntest_ssvep = test_ssvep[:, target_indices, :].transpose(0, 2, 1)  # New shape: (n_ssvep, 1750, 4)\n\n# Get indices of MI and SSVEP trials in test_df\nmi_mask = test_df['task'] == 'MI'\nssvep_mask = test_df['task'] == 'SSVEP'\n\n# Validate counts\nassert mi_mask.sum() == test_mi.shape[0], \"MI trial count mismatch\"\nassert ssvep_mask.sum() == test_ssvep.shape[0], \"SSVEP trial count mismatch\"\n\n# Generate predictions\npredictions = []\n# Predict MI trials\nif test_mi.shape[0] > 0:\n    mi_preds = model.predict(test_mi, verbose=0)\n    mi_labels = ['Right' if prob[1] >= threshold else 'Left' for prob in mi_preds]\n    predictions.extend(mi_labels)\n\n# Assign dummy labels for SSVEP trials\nif test_ssvep.shape[0] > 0:\n    predictions.extend(['Left'] * test_ssvep.shape[0])\n\n# Create submission file\nsubmission = sample_sub.copy()\nsubmission['label'] = predictions\nsubmission.to_csv(output_file, index=False)\n\nprint(f\"Submission file saved to {output_file}\")\nprint(\"Prediction distribution:\")\nprint(submission['label'].value_counts())\nprint(f\"\\nThreshold used for MI: {threshold}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:11:57.285693Z","iopub.execute_input":"2025-06-29T10:11:57.285961Z","iopub.status.idle":"2025-06-29T10:11:58.369289Z","shell.execute_reply.started":"2025-06-29T10:11:57.285941Z","shell.execute_reply":"2025-06-29T10:11:58.368588Z"}},"outputs":[{"name":"stdout","text":"Submission file saved to /kaggle/working/submission.csv\nPrediction distribution:\nlabel\nLeft     59\nRight    41\nName: count, dtype: int64\n\nThreshold used for MI: 0.5\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}