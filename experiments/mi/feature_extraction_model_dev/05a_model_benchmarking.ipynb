{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12230174,"sourceType":"datasetVersion","datasetId":7705828}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.metrics import f1_score, classification_report\nfrom mne.decoding import CSP\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, LearningRateScheduler\n\n# --- 0) Config ---\nPREPROCESSED_PATH = '/kaggle/input/preprocessed-mtc-aic/mtc-aic3_dataset_preprocessed'\noutput_dir = './models'\nos.makedirs(output_dir, exist_ok=True)\n\neeg_indices = [1, 3]  # C3 (index1), C4 (index3)\n\n# --- 1) Load metadata ---\ntrain_df = pd.read_csv(os.path.join(PREPROCESSED_PATH, 'train.csv'))\nval_df = pd.read_csv(os.path.join(PREPROCESSED_PATH, 'validation.csv'))\n\n# Filter MI trials\ntrain_mi = train_df[train_df['task'] == 'MI']\nval_mi = val_df[val_df['task'] == 'MI']\n\n# --- 2) Data Loading Functions ---\ndef get_trial_path(row, split):\n    \"\"\"Reconstruct absolute path to NPZ file\"\"\"\n    return os.path.join(\n        PREPROCESSED_PATH,\n        row['task'],\n        split,\n        row['subject_id'],\n        str(row['trial_session']),\n        f\"trial_{row['trial']}.npz\"\n    )\n\ndef load_trials(df, split):\n    \"\"\"Load preprocessed EEG data\"\"\"\n    X_list, y_list = [], []\n    for _, row in df.iterrows():\n        # Build correct file path\n        npz_path = get_trial_path(row, split)\n        \n        # Load data\n        with np.load(npz_path) as data:\n            eeg_data = data['data']\n        \n        # Select C3/C4 channels\n        data_sub = eeg_data[:, eeg_indices].T  # (2, time)\n        \n        # Binary label\n        label_bin = 1 if row['label'] == 'Right' else 0\n        \n        X_list.append(data_sub)\n        y_list.append(label_bin)\n    \n    return np.array(X_list), np.array(y_list)\n\n# Load data\nX_train_all, y_train = load_trials(train_mi, 'train')\nX_val_all, y_val = load_trials(val_mi, 'validation')\n\n# Transpose to (n_trials, time, channels)\nX_train_raw = X_train_all.transpose(0, 2, 1).astype('float32')  # (n, 2250, 2)\nX_val_raw = X_val_all.transpose(0, 2, 1).astype('float32')      # (n, 2250, 2)\n\n# --- 3) CSP Processing ---\ncsp = CSP(n_components=2, log=False, norm_trace=False)\nX_train_csp_input = X_train_raw.transpose(0, 2, 1).astype('float64')\ncsp.fit(X_train_csp_input, y_train)\n\nW = csp.filters_[:2]\n\ndef apply_csp(X):\n    X_t = X.transpose(0, 2, 1)\n    transformed = np.stack([W.dot(ep) for ep in X_t], axis=0)\n    return transformed.transpose(0, 2, 1).astype('float32')\n\nXtr_csp = apply_csp(X_train_raw)\nXvl_csp = apply_csp(X_val_raw)\n\n# For 2D models\nXtr_csp_2d = Xtr_csp[..., np.newaxis]\nXvl_csp_2d = Xvl_csp[..., np.newaxis]\n\n# --- 4) One-hot labels ---\nytr_oh = keras.utils.to_categorical(y_train, 2)\nyvl_oh = keras.utils.to_categorical(y_val, 2)\n\n# --- 5) Data augmentation ---\ndef aug_gen(X, y, seed=0, batch_size=32):\n    n = X.shape[0]\n    rng = np.random.RandomState(seed)\n    while True:\n        idx = rng.randint(0, n, batch_size)\n        bx, by = X[idx], y[idx]\n        # Gentle noise augmentation\n        noise = rng.normal(0, 0.1, bx.shape) * np.std(bx, axis=(1,2), keepdims=True)\n        yield bx + noise, by\n\n# Calculate steps\nbatch_size = 64\nsteps_raw = int(np.ceil(len(X_train_raw) / batch_size))\nsteps_csp1d = int(np.ceil(len(Xtr_csp) / batch_size))\nsteps_csp2d = int(np.ceil(len(Xtr_csp_2d) / batch_size))\n\ntrain_gen_raw = aug_gen(X_train_raw, ytr_oh, seed=0, batch_size=batch_size)\ntrain_gen_csp1d = aug_gen(Xtr_csp, ytr_oh, seed=1, batch_size=batch_size)\ntrain_gen_csp2d = aug_gen(Xtr_csp_2d, ytr_oh, seed=2, batch_size=batch_size)\n\n# --- 6) Cosine LR schedule ---\ndef cosine_lr(epoch, lr_max=1e-3, epochs=100):\n    \"\"\"Cosine learning rate decay\"\"\"\n    return lr_max * (1 + np.cos(np.pi * epoch / epochs)) / 2\n\n# --- 7) F1 Score Metric ---\nclass F1Score(tf.keras.metrics.Metric):\n    def __init__(self, name='f1_score', **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.tp = self.add_weight(name='tp', initializer='zeros')\n        self.fp = self.add_weight(name='fp', initializer='zeros')\n        self.fn = self.add_weight(name='fn', initializer='zeros')\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_pred = tf.argmax(y_pred, axis=1)\n        y_true = tf.argmax(y_true, axis=1)\n        \n        tp = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 1)), tf.float32))\n        fp = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 0), tf.equal(y_pred, 1)), tf.float32))\n        fn = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 0)), tf.float32))\n        \n        self.tp.assign_add(tp)\n        self.fp.assign_add(fp)\n        self.fn.assign_add(fn)\n\n    def result(self):\n        p = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n        r = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n        return 2 * p * r / (p + r + tf.keras.backend.epsilon())\n\n    def reset_states(self):\n        self.tp.assign(0)\n        self.fp.assign(0)\n        self.fn.assign(0)\n\n# --- 8) Callback factory ---\ndef get_callbacks(name):\n    return [\n        EarlyStopping(monitor=\"val_f1_score\", mode=\"max\", patience=15, restore_best_weights=True),\n        ModelCheckpoint(os.path.join(output_dir, f\"best_{name}.h5\"),\n                        monitor=\"val_f1_score\", mode=\"max\", save_best_only=True),\n        CSVLogger(os.path.join(output_dir, f\"log_{name}.csv\")),\n        LearningRateScheduler(cosine_lr)\n    ]\n\n# --- 9) Simplified Model Builders ---\ndef build_model_simple(input_shape):\n    \"\"\"Simplified model for EEG classification\"\"\"\n    model = keras.Sequential([\n        layers.Input(input_shape),\n        layers.Conv1D(32, 50, activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling1D(4),\n        layers.Conv1D(64, 25, activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling1D(4),\n        layers.Conv1D(128, 10, activation='relu'),\n        layers.BatchNormalization(),\n        layers.GlobalAveragePooling1D(),\n        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n        layers.Dropout(0.5),\n        layers.Dense(2, activation='softmax')\n    ])\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy', F1Score()])\n    return model\n\ndef build_model_lstm(input_shape):\n    \"\"\"LSTM model for temporal patterns\"\"\"\n    model = keras.Sequential([\n        layers.Input(input_shape),\n        layers.Conv1D(32, 10, activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling1D(2),\n        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n        layers.Bidirectional(layers.LSTM(32)),\n        layers.Dense(64, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(2, activation='softmax')\n    ])\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy', F1Score()])\n    return model\n\ndef build_model_csp(input_shape):\n    \"\"\"Model specifically designed for CSP features\"\"\"\n    model = keras.Sequential([\n        layers.Input(input_shape),\n        layers.Conv1D(64, 10, activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling1D(3),\n        layers.Conv1D(128, 5, activation='relu'),\n        layers.BatchNormalization(),\n        layers.GlobalAveragePooling1D(),\n        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n        layers.Dropout(0.5),\n        layers.Dense(2, activation='softmax')\n    ])\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy', F1Score()])\n    return model\n\n# --- 10) Train & evaluate ---\nbuilders = {\n    'simple_raw': build_model_simple,\n    'lstm_csp': build_model_lstm,\n    'csp_enhanced': build_model_csp,\n}\n\nresults = {}\nshape_raw   = X_train_raw.shape[1:]    # (2250, 2)\nshape_csp1d = Xtr_csp.shape[1:]        # (2250, 2)\nshape_csp2d = Xtr_csp_2d.shape[1:]     # (2250, 2, 1)\n\nfor name, build_fn in builders.items():\n    print(f\"\\n>>> Training {name}\")\n    \n    # Select appropriate input shape and data\n    if 'csp' in name:\n        if name == 'csp_enhanced':\n            model = build_fn(shape_csp1d)\n            gen, steps, val_x = train_gen_csp1d, steps_csp1d, Xvl_csp\n        else:  # lstm_csp\n            model = build_fn(shape_csp1d)\n            gen, steps, val_x = train_gen_csp1d, steps_csp1d, Xvl_csp\n    else:  # simple_raw\n        model = build_fn(shape_raw)\n        gen, steps, val_x = train_gen_raw, steps_raw, X_val_raw\n\n    history = model.fit(\n        gen, \n        steps_per_epoch=steps,\n        validation_data=(val_x, yvl_oh),\n        epochs=100,\n        callbacks=get_callbacks(name),\n        verbose=2\n    )\n\n    # Evaluate\n    preds = np.argmax(model.predict(val_x), axis=1)\n    f1 = f1_score(y_val, preds)\n    print(f\"{name} → val F1 = {f1:.4f}\")\n    print(classification_report(y_val, preds, target_names=[\"Left\", \"Right\"]))\n    results[name] = (f1, model)\n\n# --- 11) Save best model ---\nif results:\n    best_name, (best_f1, best_model) = max(results.items(), key=lambda kv: kv[1][0])\n    print(f\"\\n=== Final best: {best_name} (F1={best_f1:.4f}) ===\")\n    best_model.save(os.path.join(output_dir, 'best_final.h5'))\nelse:\n    print(\"No models trained successfully\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:13:14.736545Z","iopub.execute_input":"2025-06-26T06:13:14.736853Z","execution_failed":"2025-06-26T06:46:30.878Z"}},"outputs":[{"name":"stdout","text":"Computing rank from data with rank=None\n    Using tolerance 0.45 (2.2e-16 eps * 2 dim * 1e+15  max singular value)\n    Estimated rank (data): 2\n    data: rank 2 computed from 2 data channels with 0 projectors\nReducing data rank from 2 -> 2\nEstimating class=0 covariance using EMPIRICAL\nDone.\nEstimating class=1 covariance using EMPIRICAL\nDone.\n\n>>> Training simple_raw\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"2025-06-26 06:13:37.963197: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"38/38 - 18s - 465ms/step - accuracy: 0.5078 - f1_score: 0.5015 - loss: 0.7840 - val_accuracy: 0.4600 - val_f1_score: 0.5424 - val_loss: 0.7281 - learning_rate: 0.0010\nEpoch 2/100\n38/38 - 12s - 310ms/step - accuracy: 0.5062 - f1_score: 0.5407 - loss: 0.7454 - val_accuracy: 0.4600 - val_f1_score: 0.5424 - val_loss: 0.7680 - learning_rate: 9.9975e-04\nEpoch 3/100\n38/38 - 11s - 294ms/step - accuracy: 0.5181 - f1_score: 0.5769 - loss: 0.7203 - val_accuracy: 0.5000 - val_f1_score: 0.6032 - val_loss: 0.7297 - learning_rate: 9.9877e-04\nEpoch 4/100\n38/38 - 12s - 308ms/step - accuracy: 0.5086 - f1_score: 0.5305 - loss: 0.7128 - val_accuracy: 0.4600 - val_f1_score: 0.5091 - val_loss: 0.6972 - learning_rate: 9.9655e-04\nEpoch 5/100\n38/38 - 11s - 301ms/step - accuracy: 0.5152 - f1_score: 0.5879 - loss: 0.7142 - val_accuracy: 0.5600 - val_f1_score: 0.0000e+00 - val_loss: 0.6826 - learning_rate: 9.9262e-04\nEpoch 6/100\n38/38 - 11s - 298ms/step - accuracy: 0.5325 - f1_score: 0.5894 - loss: 0.6994 - val_accuracy: 0.5200 - val_f1_score: 0.6129 - val_loss: 0.6936 - learning_rate: 9.8651e-04\nEpoch 7/100\n38/38 - 12s - 316ms/step - accuracy: 0.5436 - f1_score: 0.6196 - loss: 0.6993 - val_accuracy: 0.4600 - val_f1_score: 0.5714 - val_loss: 0.7101 - learning_rate: 9.7777e-04\nEpoch 8/100\n38/38 - 11s - 298ms/step - accuracy: 0.5226 - f1_score: 0.5189 - loss: 0.7038 - val_accuracy: 0.4800 - val_f1_score: 0.5667 - val_loss: 0.7565 - learning_rate: 9.6600e-04\nEpoch 9/100\n38/38 - 11s - 301ms/step - accuracy: 0.5218 - f1_score: 0.5716 - loss: 0.6996 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.8286 - learning_rate: 9.5083e-04\nEpoch 10/100\n38/38 - 12s - 318ms/step - accuracy: 0.5407 - f1_score: 0.6192 - loss: 0.6955 - val_accuracy: 0.4800 - val_f1_score: 0.6176 - val_loss: 0.7419 - learning_rate: 9.3195e-04\nEpoch 11/100\n38/38 - 11s - 298ms/step - accuracy: 0.5325 - f1_score: 0.5372 - loss: 0.6974 - val_accuracy: 0.4600 - val_f1_score: 0.5424 - val_loss: 0.7791 - learning_rate: 9.0914e-04\nEpoch 12/100\n38/38 - 11s - 298ms/step - accuracy: 0.5510 - f1_score: 0.6086 - loss: 0.6938 - val_accuracy: 0.4800 - val_f1_score: 0.5806 - val_loss: 0.8705 - learning_rate: 8.8227e-04\nEpoch 13/100\n38/38 - 12s - 314ms/step - accuracy: 0.5329 - f1_score: 0.5597 - loss: 0.6987 - val_accuracy: 0.5200 - val_f1_score: 0.4783 - val_loss: 0.7158 - learning_rate: 8.5129e-04\nEpoch 14/100\n38/38 - 11s - 297ms/step - accuracy: 0.5461 - f1_score: 0.5718 - loss: 0.6921 - val_accuracy: 0.4600 - val_f1_score: 0.3721 - val_loss: 0.6974 - learning_rate: 8.1628e-04\nEpoch 15/100\n38/38 - 11s - 299ms/step - accuracy: 0.5666 - f1_score: 0.5824 - loss: 0.6887 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7630 - learning_rate: 7.7744e-04\nEpoch 16/100\n38/38 - 12s - 316ms/step - accuracy: 0.5814 - f1_score: 0.6123 - loss: 0.6866 - val_accuracy: 0.4800 - val_f1_score: 0.4800 - val_loss: 0.7037 - learning_rate: 7.3507e-04\nEpoch 17/100\n38/38 - 11s - 297ms/step - accuracy: 0.5629 - f1_score: 0.5715 - loss: 0.6854 - val_accuracy: 0.5000 - val_f1_score: 0.2857 - val_loss: 0.7087 - learning_rate: 6.8961e-04\nEpoch 18/100\n38/38 - 11s - 299ms/step - accuracy: 0.5715 - f1_score: 0.6132 - loss: 0.6791 - val_accuracy: 0.5200 - val_f1_score: 0.5385 - val_loss: 0.7412 - learning_rate: 6.4159e-04\nEpoch 19/100\n38/38 - 12s - 312ms/step - accuracy: 0.5798 - f1_score: 0.5798 - loss: 0.6772 - val_accuracy: 0.4600 - val_f1_score: 0.4000 - val_loss: 0.8054 - learning_rate: 5.9165e-04\nEpoch 20/100\n38/38 - 11s - 301ms/step - accuracy: 0.5699 - f1_score: 0.6044 - loss: 0.6783 - val_accuracy: 0.4000 - val_f1_score: 0.4231 - val_loss: 0.7715 - learning_rate: 5.4050e-04\nEpoch 21/100\n38/38 - 12s - 303ms/step - accuracy: 0.5991 - f1_score: 0.6095 - loss: 0.6725 - val_accuracy: 0.5800 - val_f1_score: 0.4324 - val_loss: 0.6816 - learning_rate: 4.8889e-04\nEpoch 22/100\n38/38 - 12s - 312ms/step - accuracy: 0.5995 - f1_score: 0.5728 - loss: 0.6649 - val_accuracy: 0.4400 - val_f1_score: 0.4615 - val_loss: 0.7706 - learning_rate: 4.3759e-04\nEpoch 23/100\n38/38 - 11s - 298ms/step - accuracy: 0.6217 - f1_score: 0.6242 - loss: 0.6501 - val_accuracy: 0.4400 - val_f1_score: 0.4815 - val_loss: 0.7852 - learning_rate: 3.8738e-04\nEpoch 24/100\n38/38 - 12s - 317ms/step - accuracy: 0.6229 - f1_score: 0.6368 - loss: 0.6465 - val_accuracy: 0.5000 - val_f1_score: 0.1935 - val_loss: 0.6971 - learning_rate: 3.3898e-04\nEpoch 25/100\n38/38 - 11s - 301ms/step - accuracy: 0.6484 - f1_score: 0.6460 - loss: 0.6282 - val_accuracy: 0.5000 - val_f1_score: 0.1935 - val_loss: 0.7082 - learning_rate: 2.9304e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\nsimple_raw → val F1 = 0.6176\n              precision    recall  f1-score   support\n\n        Left       0.75      0.11      0.19        28\n       Right       0.46      0.95      0.62        22\n\n    accuracy                           0.48        50\n   macro avg       0.60      0.53      0.40        50\nweighted avg       0.62      0.48      0.38        50\n\n\n>>> Training lstm_csp\nEpoch 1/100\n38/38 - 78s - 2s/step - accuracy: 0.5214 - f1_score: 0.5333 - loss: 0.7017 - val_accuracy: 0.4400 - val_f1_score: 0.4815 - val_loss: 0.6968 - learning_rate: 0.0010\nEpoch 2/100\n38/38 - 65s - 2s/step - accuracy: 0.5378 - f1_score: 0.5397 - loss: 0.6913 - val_accuracy: 0.5000 - val_f1_score: 0.5614 - val_loss: 0.7017 - learning_rate: 9.9975e-04\nEpoch 3/100\n38/38 - 75s - 2s/step - accuracy: 0.5127 - f1_score: 0.5285 - loss: 0.6957 - val_accuracy: 0.3400 - val_f1_score: 0.4762 - val_loss: 0.7046 - learning_rate: 9.9877e-04\nEpoch 4/100\n38/38 - 66s - 2s/step - accuracy: 0.5366 - f1_score: 0.5900 - loss: 0.6898 - val_accuracy: 0.3800 - val_f1_score: 0.4561 - val_loss: 0.7175 - learning_rate: 9.9655e-04\nEpoch 5/100\n38/38 - 66s - 2s/step - accuracy: 0.5407 - f1_score: 0.5692 - loss: 0.6876 - val_accuracy: 0.4400 - val_f1_score: 0.4815 - val_loss: 0.7015 - learning_rate: 9.9262e-04\nEpoch 6/100\n38/38 - 66s - 2s/step - accuracy: 0.5432 - f1_score: 0.5964 - loss: 0.6869 - val_accuracy: 0.4200 - val_f1_score: 0.4727 - val_loss: 0.7064 - learning_rate: 9.8651e-04\nEpoch 7/100\n38/38 - 66s - 2s/step - accuracy: 0.5477 - f1_score: 0.5769 - loss: 0.6817 - val_accuracy: 0.4800 - val_f1_score: 0.6061 - val_loss: 0.7152 - learning_rate: 9.7777e-04\nEpoch 8/100\n38/38 - 66s - 2s/step - accuracy: 0.5448 - f1_score: 0.5947 - loss: 0.6840 - val_accuracy: 0.5000 - val_f1_score: 0.4681 - val_loss: 0.7060 - learning_rate: 9.6600e-04\nEpoch 9/100\n38/38 - 66s - 2s/step - accuracy: 0.5621 - f1_score: 0.5601 - loss: 0.6803 - val_accuracy: 0.4200 - val_f1_score: 0.5085 - val_loss: 0.7164 - learning_rate: 9.5083e-04\nEpoch 10/100\n38/38 - 66s - 2s/step - accuracy: 0.5781 - f1_score: 0.5932 - loss: 0.6764 - val_accuracy: 0.3600 - val_f1_score: 0.4074 - val_loss: 0.7314 - learning_rate: 9.3195e-04\nEpoch 11/100\n38/38 - 67s - 2s/step - accuracy: 0.5572 - f1_score: 0.6217 - loss: 0.6807 - val_accuracy: 0.3400 - val_f1_score: 0.4590 - val_loss: 0.7237 - learning_rate: 9.0914e-04\nEpoch 12/100\n38/38 - 66s - 2s/step - accuracy: 0.5929 - f1_score: 0.5923 - loss: 0.6683 - val_accuracy: 0.2800 - val_f1_score: 0.3793 - val_loss: 0.7364 - learning_rate: 8.8227e-04\nEpoch 13/100\n38/38 - 78s - 2s/step - accuracy: 0.5691 - f1_score: 0.5991 - loss: 0.6678 - val_accuracy: 0.3600 - val_f1_score: 0.4074 - val_loss: 0.7751 - learning_rate: 8.5129e-04\nEpoch 14/100\n38/38 - 66s - 2s/step - accuracy: 0.5748 - f1_score: 0.5810 - loss: 0.6740 - val_accuracy: 0.4600 - val_f1_score: 0.4490 - val_loss: 0.7126 - learning_rate: 8.1628e-04\nEpoch 15/100\n38/38 - 67s - 2s/step - accuracy: 0.5905 - f1_score: 0.5958 - loss: 0.6657 - val_accuracy: 0.4600 - val_f1_score: 0.4000 - val_loss: 0.7518 - learning_rate: 7.7744e-04\nEpoch 16/100\n38/38 - 67s - 2s/step - accuracy: 0.5913 - f1_score: 0.5820 - loss: 0.6655 - val_accuracy: 0.5600 - val_f1_score: 0.4762 - val_loss: 0.7105 - learning_rate: 7.3507e-04\nEpoch 17/100\n38/38 - 67s - 2s/step - accuracy: 0.5921 - f1_score: 0.6026 - loss: 0.6611 - val_accuracy: 0.4000 - val_f1_score: 0.3182 - val_loss: 0.7739 - learning_rate: 6.8961e-04\nEpoch 18/100\n38/38 - 67s - 2s/step - accuracy: 0.5979 - f1_score: 0.6079 - loss: 0.6572 - val_accuracy: 0.5000 - val_f1_score: 0.4444 - val_loss: 0.7526 - learning_rate: 6.4159e-04\nEpoch 19/100\n38/38 - 67s - 2s/step - accuracy: 0.5946 - f1_score: 0.5878 - loss: 0.6597 - val_accuracy: 0.4600 - val_f1_score: 0.3415 - val_loss: 0.7438 - learning_rate: 5.9165e-04\nEpoch 20/100\n38/38 - 67s - 2s/step - accuracy: 0.5950 - f1_score: 0.6166 - loss: 0.6456 - val_accuracy: 0.4600 - val_f1_score: 0.3721 - val_loss: 0.7626 - learning_rate: 5.4050e-04\nEpoch 21/100\n38/38 - 66s - 2s/step - accuracy: 0.6160 - f1_score: 0.6046 - loss: 0.6478 - val_accuracy: 0.5200 - val_f1_score: 0.3684 - val_loss: 0.7750 - learning_rate: 4.8889e-04\nEpoch 22/100\n38/38 - 67s - 2s/step - accuracy: 0.6250 - f1_score: 0.6407 - loss: 0.6358 - val_accuracy: 0.4600 - val_f1_score: 0.4000 - val_loss: 0.7677 - learning_rate: 4.3759e-04\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step\nlstm_csp → val F1 = 0.6061\n              precision    recall  f1-score   support\n\n        Left       0.67      0.14      0.24        28\n       Right       0.45      0.91      0.61        22\n\n    accuracy                           0.48        50\n   macro avg       0.56      0.53      0.42        50\nweighted avg       0.57      0.48      0.40        50\n\n\n>>> Training csp_enhanced\nEpoch 1/100\n38/38 - 21s - 561ms/step - accuracy: 0.4827 - f1_score: 0.4344 - loss: 0.7508 - val_accuracy: 0.4400 - val_f1_score: 0.3913 - val_loss: 0.7052 - learning_rate: 0.0010\nEpoch 2/100\n38/38 - 16s - 416ms/step - accuracy: 0.5021 - f1_score: 0.5200 - loss: 0.7226 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7114 - learning_rate: 9.9975e-04\nEpoch 3/100\n38/38 - 16s - 420ms/step - accuracy: 0.5053 - f1_score: 0.4767 - loss: 0.7114 - val_accuracy: 0.4400 - val_f1_score: 0.5484 - val_loss: 0.7012 - learning_rate: 9.9877e-04\nEpoch 4/100\n38/38 - 16s - 420ms/step - accuracy: 0.5234 - f1_score: 0.5410 - loss: 0.7049 - val_accuracy: 0.4400 - val_f1_score: 0.5333 - val_loss: 0.7146 - learning_rate: 9.9655e-04\nEpoch 5/100\n38/38 - 16s - 432ms/step - accuracy: 0.5099 - f1_score: 0.5255 - loss: 0.7073 - val_accuracy: 0.4400 - val_f1_score: 0.5333 - val_loss: 0.7156 - learning_rate: 9.9262e-04\nEpoch 6/100\n38/38 - 16s - 420ms/step - accuracy: 0.5214 - f1_score: 0.5396 - loss: 0.7032 - val_accuracy: 0.4600 - val_f1_score: 0.5846 - val_loss: 0.7197 - learning_rate: 9.8651e-04\nEpoch 7/100\n38/38 - 16s - 432ms/step - accuracy: 0.5292 - f1_score: 0.4450 - loss: 0.7017 - val_accuracy: 0.4600 - val_f1_score: 0.5263 - val_loss: 0.7410 - learning_rate: 9.7777e-04\nEpoch 8/100\n38/38 - 16s - 417ms/step - accuracy: 0.5185 - f1_score: 0.5094 - loss: 0.7027 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7605 - learning_rate: 9.6600e-04\nEpoch 9/100\n38/38 - 17s - 439ms/step - accuracy: 0.5201 - f1_score: 0.5164 - loss: 0.6969 - val_accuracy: 0.4400 - val_f1_score: 0.6111 - val_loss: 0.7762 - learning_rate: 9.5083e-04\nEpoch 10/100\n38/38 - 15s - 398ms/step - accuracy: 0.5214 - f1_score: 0.5749 - loss: 0.7059 - val_accuracy: 0.4200 - val_f1_score: 0.5797 - val_loss: 0.7445 - learning_rate: 9.3195e-04\nEpoch 11/100\n38/38 - 17s - 437ms/step - accuracy: 0.5259 - f1_score: 0.5455 - loss: 0.6981 - val_accuracy: 0.4200 - val_f1_score: 0.4912 - val_loss: 0.7126 - learning_rate: 9.0914e-04\nEpoch 12/100\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport joblib\nimport pywt\nfrom scipy import signal, stats\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom tensorflow.keras.models import Model, save_model\nfrom tensorflow.keras.layers import (Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, \n                                     Dense, Flatten, LSTM, Reshape, Dropout, BatchNormalization,\n                                     Attention, Multiply, GlobalAveragePooling1D, Permute, \n                                     concatenate, SimpleRNN, GRU)\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.activations import elu, relu\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mne.decoding import CSP\n\n# Configuration\nDATA_ROOT = '/kaggle/input/preprocessed-mtc-aic'  # Root directory of the dataset\nBASE_DATA_PATH = os.path.join(DATA_ROOT, 'mtc-aic3_dataset_preprocessed')  # Full path to preprocessed data\nFS = 250  # Sampling rate\nMI_CHANNELS = ['C3', 'CZ', 'C4']  # Focus on central channels for motor imagery\nCHANNEL_INDICES = [1, 2, 3]  # Indices of C3, CZ, C4 in the original data\nN_CHANNELS = len(MI_CHANNELS)\nCLASSES = ['left', 'right']\nN_CLASSES = len(CLASSES)\nRESULTS_DIR = '/kaggle/working/results'\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\n# Load dataframes\ntrain_df = pd.read_csv(os.path.join(BASE_DATA_PATH, 'train.csv'))\nval_df = pd.read_csv(os.path.join(BASE_DATA_PATH, 'validation.csv'))\ntest_df = pd.read_csv(os.path.join(BASE_DATA_PATH, 'test.csv'))\n\n# Filter MI trials and focus on central channels\ndef filter_mi_channels(df):\n    mi_df = df[df['task'] == 'MI'].copy()\n    return mi_df\n\ntrain_mi = filter_mi_channels(train_df)\nval_mi = filter_mi_channels(val_df)\ntest_mi = filter_mi_channels(test_df)\n\n# Load preprocessed data and select MI channels\ndef load_data(df):\n    data = []\n    for path in df['processed_path']:\n        # Construct full path by joining with DATA_ROOT\n        full_path = os.path.join(DATA_ROOT, path.lstrip('./'))\n        with np.load(full_path) as npz_file:\n            full_data = npz_file['data']\n            # Select only central channels (C3, CZ, C4)\n            data.append(full_data[:, CHANNEL_INDICES])\n    return np.array(data)\n\nX_train_raw = load_data(train_mi)\nX_val_raw = load_data(val_mi)\nX_test_raw = load_data(test_mi)  # Test data for final predictions\n\n# Convert labels - handle case sensitivity\ndef map_label(label):\n    # Normalize label to lowercase for case-insensitive matching\n    normalized = label.strip().lower()\n    if normalized == 'left':\n        return 0\n    elif normalized == 'right':\n        return 1\n    else:\n        raise ValueError(f\"Invalid label: {label}\")\n\n# Use case-insensitive column lookup\ndef get_label_column(df):\n    for col in df.columns:\n        if col.lower() == 'label':\n            return col\n    return None  # Return None if not found\n\n# Get labels for train and validation\nlabel_col_train = get_label_column(train_mi)\nlabel_col_val = get_label_column(val_mi)\n\nif label_col_train is None or label_col_val is None:\n    raise KeyError(\"No 'label' column found in training or validation data\")\n\ny_train = train_mi[label_col_train].apply(map_label).values\ny_val = val_mi[label_col_val].apply(map_label).values\n\n# Standardize data\nscaler = StandardScaler()\nX_train_raw = scaler.fit_transform(\n    X_train_raw.reshape(-1, X_train_raw.shape[-1])\n).reshape(X_train_raw.shape)\nX_val_raw = scaler.transform(\n    X_val_raw.reshape(-1, X_val_raw.shape[-1])\n).reshape(X_val_raw.shape)\nX_test_raw = scaler.transform(\n    X_test_raw.reshape(-1, X_test_raw.shape[-1])\n).reshape(X_test_raw.shape)\n\n# Frequency bands for feature extraction\nFREQ_BANDS = {\n    'delta': (1, 4),\n    'theta': (4, 8),\n    'alpha': (8, 13),\n    'beta': (13, 30),\n    'gamma': (30, 45)\n}\n\n# Feature extraction functions\ndef compute_stft(data, nperseg=256, noverlap=192):\n    n_trials, n_times, n_channels = data.shape\n    f, t, Zxx = signal.stft(data[:, :, 0], fs=FS, nperseg=nperseg, noverlap=noverlap, axis=1)\n    n_freq, n_time = Zxx.shape[1:3]\n    stft_data = np.zeros((n_trials, n_time, n_freq, n_channels), dtype=np.float32)\n    \n    for c in range(n_channels):\n        _, _, Zxx = signal.stft(data[:, :, c], fs=FS, nperseg=nperseg, noverlap=noverlap, axis=1)\n        stft_data[..., c] = np.abs(Zxx).transpose(0, 2, 1)\n    return stft_data\n\n# Fixed CSP feature extraction function\ndef compute_csp_features(X, y, n_components=4):\n    \"\"\"Return full CSP time-series (n_trials, timesteps, n_components)\"\"\"\n    # Create CSP with transform_into='csp_space' to get time-series output\n    # Set log=None as required by transform_into='csp_space'\n    csp = CSP(n_components=n_components, reg=None, log=None, norm_trace=False,\n              transform_into='csp_space')\n    # Input shape: (trials, channels, time)\n    X_csp_time = csp.fit_transform(X.transpose(0, 2, 1), y)\n    # Output is (trials, components, time) -> transpose to (trials, time, components)\n    return X_csp_time.transpose(0, 2, 1), csp\n\n# Compute STFT features\nX_train_stft = compute_stft(X_train_raw)\nX_val_stft = compute_stft(X_val_raw)\n\n# Print STFT shapes for verification\nprint(\"\\n\" + \"=\"*50)\nprint(\"STFT Shape Verification\")\nprint(\"=\"*50)\nprint(f\"STFT Training Shape: {X_train_stft.shape}\")\nprint(f\"STFT Validation Shape: {X_val_stft.shape}\")\nprint(\"=\"*50 + \"\\n\")\n\n# Compute CSP features\nX_train_csp, csp = compute_csp_features(X_train_raw, y_train)\n# Transform validation set (ensure same format as training)\nX_val_csp = csp.transform(X_val_raw.transpose(0, 2, 1)).transpose(0, 2, 1)\n\n# Print CSP shapes for verification\nprint(\"\\n\" + \"=\"*50)\nprint(\"CSP Shape Verification\")\nprint(\"=\"*50)\nprint(f\"CSP Training Shape: {X_train_csp.shape}\")\nprint(f\"CSP Validation Shape: {X_val_csp.shape}\")\nprint(\"=\"*50 + \"\\n\")\n\n# Handcrafted feature extraction (focused on MI channels)\ndef extract_handcrafted_features(X):\n    \"\"\"Extract domain-specific features for traditional ML models\"\"\"\n    n_trials, n_timesteps, n_channels = X.shape\n    features = []\n    \n    for i in range(n_trials):\n        trial_features = []\n        \n        # Channel-specific features\n        for ch in range(n_channels):\n            channel_data = X[i, :, ch]\n            \n            # Time-domain features\n            trial_features.append(np.mean(channel_data))\n            trial_features.append(np.std(channel_data))\n            trial_features.append(stats.skew(channel_data))\n            trial_features.append(stats.kurtosis(channel_data))\n            trial_features.append(np.median(np.abs(channel_data)))\n            \n            # Frequency-domain features\n            f, Pxx = signal.welch(channel_data, fs=FS, nperseg=256)\n            for band, (low, high) in FREQ_BANDS.items():\n                band_mask = (f >= low) & (f <= high)\n                trial_features.append(np.log1p(np.sum(Pxx[band_mask])))\n        \n        # Cross-channel features (C3-C4 asymmetry - most important for MI)\n        c3_data = X[i, :, 0]  # C3 is first channel\n        c4_data = X[i, :, 2]  # C4 is third channel\n            \n        # Time-domain asymmetry\n        trial_features.append(np.mean(c3_data - c4_data))\n        trial_features.append(np.mean(np.abs(c3_data) - np.mean(np.abs(c4_data))))\n        \n        # Frequency-domain asymmetry\n        for band in FREQ_BANDS:\n            c3_band = trial_features[5*0 + 4 + list(FREQ_BANDS.keys()).index(band) + 1]\n            c4_band = trial_features[5*2 + 4 + list(FREQ_BANDS.keys()).index(band) + 1]\n            trial_features.append(c3_band - c4_band)\n        \n        # Hjorth parameters\n        def hjorth_parameters(data):\n            diff1 = np.diff(data)\n            diff2 = np.diff(diff1)\n            var0 = np.var(data)\n            var1 = np.var(diff1)\n            var2 = np.var(diff2)\n            activity = var0\n            mobility = np.sqrt(var1 / var0)\n            complexity = np.sqrt(var2 / var1) / mobility\n            return activity, mobility, complexity\n        \n        for ch, name in zip([0, 2], ['C3', 'C4']):\n            activity, mobility, complexity = hjorth_parameters(X[i, :, ch])\n            trial_features.extend([activity, mobility, complexity])\n        \n        features.append(trial_features)\n    \n    return np.array(features)\n\n# Data augmentation for CNN models\ndef augment_data(X, y, augmentation_factor=1):\n    X_aug = [X]\n    y_aug = [y]\n    \n    for _ in range(augmentation_factor):\n        # Gaussian noise\n        noise = np.random.normal(0, 0.05, X.shape)\n        X_aug.append(X + noise)\n        y_aug.append(y)\n        \n        # Time warping\n        warp_factor = 0.2\n        warp_points = int(X.shape[1] * warp_factor)\n        X_warped = np.zeros_like(X)\n        for i in range(X.shape[0]):\n            start = np.random.randint(0, warp_points)\n            end = np.random.randint(X.shape[1] - warp_points, X.shape[1])\n            for c in range(X.shape[2]):\n                X_warped[i, :, c] = np.interp(\n                    np.arange(X.shape[1]),\n                    np.linspace(0, X.shape[1]-1, num=end-start),\n                    X[i, start:end, c]\n                )\n        X_aug.append(X_warped)\n        y_aug.append(y)\n    \n    return np.vstack(X_aug), np.hstack(y_aug)\n\n# Model definitions\ndef build_model1(input_shape):\n    model = tf.keras.Sequential([\n        Conv1D(32, 5, activation='relu', input_shape=input_shape),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Conv1D(64, 5, activation='relu'),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Flatten(),\n        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n        Dropout(0.5),\n        Dense(N_CLASSES, activation='softmax')\n    ])\n    return model\n\ndef build_model2(input_shape):\n    model = tf.keras.Sequential([\n        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Conv2D(64, (3, 3), activation='relu'),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Conv2D(128, (3, 3), activation='relu'),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Flatten(),\n        Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n        Dropout(0.5),\n        Dense(128, activation='relu'),\n        Dense(N_CLASSES, activation='softmax')\n    ])\n    return model\n\ndef build_model3(input_shape):\n    inputs = Input(shape=input_shape)\n    x = Conv1D(32, 5, activation='elu')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(64, 5, activation='elu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(128, 5, activation='elu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(256, 5, activation='elu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Flatten()(x)\n    x = Dense(256, activation='elu', kernel_regularizer=regularizers.l2(0.001))(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(N_CLASSES, activation='softmax')(x)\n    return Model(inputs, outputs)\n\ndef build_model4(input_shape):\n    model = tf.keras.Sequential([\n        Conv1D(32, 5, activation='elu', input_shape=input_shape),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Conv1D(64, 5, activation='elu'),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Conv1D(128, 5, activation='elu'),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Flatten(),\n        Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.001)),\n        Dropout(0.4),\n        Dense(N_CLASSES, activation='softmax')\n    ])\n    return model\n\ndef build_model5(input_shape):\n    inputs = Input(shape=input_shape)\n    x = Conv1D(32, 5, activation='relu')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(64, 5, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(128, 5, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = LSTM(64, return_sequences=True)(x)\n    x = LSTM(32)(x)\n    x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n    x = Dropout(0.5)(x)\n    x = Dense(64, activation='relu')(x)\n    outputs = Dense(N_CLASSES, activation='softmax')(x)\n    return Model(inputs, outputs)\n\ndef build_model6(input_shape):\n    model = tf.keras.Sequential([\n        Conv1D(32, 5, activation='elu', input_shape=input_shape),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Conv1D(64, 5, activation='elu'),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Conv1D(128, 5, activation='elu'),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Conv1D(256, 5, activation='elu'),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Flatten(),\n        Dense(512, activation='elu', kernel_regularizer=regularizers.l2(0.001)),\n        Dropout(0.6),\n        Dense(256, activation='elu'),\n        Dense(128, activation='elu'),\n        Dense(N_CLASSES, activation='softmax')\n    ])\n    return model\n\ndef build_model7(input_shape):\n    inputs = Input(shape=input_shape)\n    \n    # CNN Branch\n    x = Conv1D(32, 5, activation='relu')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(64, 5, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    \n    # Self-Attention\n    attention = Dense(1, activation='tanh')(x)\n    attention = Flatten()(attention)\n    attention = tf.keras.activations.softmax(attention)\n    attention = Reshape((-1, 1))(attention)\n    x = Multiply()([x, attention])\n    \n    x = Flatten()(x)\n    x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(N_CLASSES, activation='softmax')(x)\n    return Model(inputs, outputs)\n\ndef build_model8(input_shape):\n    inputs = Input(shape=input_shape)\n    \n    # CNN Part\n    x = Conv1D(32, 5, activation='relu')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(64, 5, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Flatten()(x)\n    \n    # MLP Part\n    x = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n    x = Dropout(0.5)(x)\n    x = Dense(128, activation='relu')(x)\n    outputs = Dense(N_CLASSES, activation='softmax')(x)\n    return Model(inputs, outputs)\n\ndef build_model9(input_shape):\n    inputs = Input(shape=input_shape)\n    x = Conv1D(32, 5, activation='relu')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(64, 5, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Flatten()(x)\n    x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n    regression = Dense(1, activation='linear')(x)\n    outputs = Dense(N_CLASSES, activation='softmax')(x)\n    return Model(inputs, [outputs, regression])\n\n# Simplified Regression Model\ndef build_model9_simple(input_shape):\n    \"\"\"Single-output version without regression head\"\"\"\n    inputs = Input(shape=input_shape)\n    x = Conv1D(32, 5, activation='relu')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(64, 5, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Flatten()(x)\n    x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n    outputs = Dense(N_CLASSES, activation='softmax')(x)\n    return Model(inputs, outputs)\n\ndef build_model10(input_shape):\n    inputs = Input(shape=input_shape)\n    x = Conv1D(32, 5, activation='relu')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(64, 5, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Flatten()(x)\n    feature_extractor = Model(inputs, x)\n    return feature_extractor\n\ndef build_model11(input_shape):\n    return build_model10(input_shape)\n\ndef build_model12(input_shape):\n    # SVM model will use handcrafted features\n    pass\n\ndef build_model13(input_shape):\n    # Logistic Regression model will use handcrafted features\n    pass\n\n# Model configurations with updated STFT_CNN input shape\nmodels_config = [\n    {\"name\": \"SimpleCNN\", \"build_func\": build_model1, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"keras\"},\n    {\"name\": \"STFT_CNN\", \"build_func\": build_model2, \"input_shape\": (37, 129, N_CHANNELS), \"rep\": \"stft\", \"type\": \"keras\"},  # Updated shape\n    {\"name\": \"7Conv_ELU\", \"build_func\": build_model3, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"keras\"},\n    {\"name\": \"3Conv_ELU\", \"build_func\": build_model4, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"keras\"},\n    {\"name\": \"CNN_LSTM\", \"build_func\": build_model5, \"input_shape\": (2250, 4), \"rep\": \"csp\", \"type\": \"keras\"},\n    {\"name\": \"CSP_CNN\", \"build_func\": build_model6, \"input_shape\": (2250, 4), \"rep\": \"csp\", \"type\": \"keras\"},\n    {\"name\": \"CNN_Attention\", \"build_func\": build_model7, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"keras\"},\n    {\"name\": \"CNN_MLP\", \"build_func\": build_model8, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"keras\"},\n    {\"name\": \"CNN_Regression\", \"build_func\": build_model9_simple, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"keras\"},\n    {\"name\": \"CNN_RF\", \"build_func\": build_model10, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"hybrid\"},\n    {\"name\": \"CNN_XGB\", \"build_func\": build_model11, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"hybrid\"},\n    {\"name\": \"Handcrafted_SVM\", \"type\": \"handcrafted\"},\n    {\"name\": \"Handcrafted_LR\", \"type\": \"handcrafted\"},\n]\n\n# Training and evaluation functions\ndef train_evaluate_keras_model(model_cfg, X_train, y_train, X_val, y_val):\n    model_name = model_cfg[\"name\"]\n    print(f\"\\n{'='*50}\\nTraining {model_name}\\n{'='*50}\")\n    \n    # Data preparation\n    if model_cfg[\"rep\"] == \"raw\":\n        X_tr, X_v = X_train, X_val\n    elif model_cfg[\"rep\"] == \"stft\":\n        X_tr, X_v = X_train_stft, X_val_stft\n    elif model_cfg[\"rep\"] == \"csp\":\n        X_tr, X_v = X_train_csp, X_val_csp\n    \n    # Data augmentation\n    if model_cfg[\"rep\"] == \"raw\":\n        X_tr, y_tr = augment_data(X_tr, y_train, augmentation_factor=2)\n    else:\n        X_tr, y_tr = X_tr, y_train\n\n    expected_shape = model_cfg[\"input_shape\"]\n    if X_tr.shape[1:] != expected_shape:\n        raise ValueError(f\"Input shape mismatch! Expected {expected_shape}, \"f\"got {X_tr.shape[1:]} for {model_cfg['name']}\")\n    \n    # Build and compile model\n    model = model_cfg[\"build_func\"](model_cfg[\"input_shape\"])\n    \n    # Special compilation for regression hybrid\n    if model_name == \"CNN_Regression\":\n        model.compile(optimizer=Adam(learning_rate=0.001),\n                      loss=['sparse_categorical_crossentropy', 'mse'],\n                      metrics={'dense_2': 'accuracy'},\n                      loss_weights=[0.9, 0.1])\n    else:\n        model.compile(optimizer=Adam(learning_rate=0.001),\n                      loss='sparse_categorical_crossentropy',\n                      metrics=['accuracy'])\n    \n    # Callbacks\n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n        ModelCheckpoint(os.path.join(RESULTS_DIR, f\"{model_name}_best_model.keras\"), \n                        save_best_only=True, monitor='val_accuracy'),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n    ]\n    \n    # Train model\n    history = model.fit(\n        X_tr, y_tr,\n        validation_data=(X_v, y_val),\n        epochs=150,\n        batch_size=32,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Save final model\n    model.save(os.path.join(RESULTS_DIR, f\"{model_name}_final_model.keras\"))\n    \n    y_proba = model.predict(X_v)\n    y_pred = np.argmax(y_proba, axis=1)\n\n    # Calculate weighted F1 score\n    f1_weighted = f1_score(y_val, y_pred, average='weighted')\n    print(f\"\\n✅ {model_cfg['name']} Validation Weighted F1: {f1_weighted:.4f}\")\n    \n    return generate_reports(model_name, y_val, y_pred, y_proba)\n\ndef train_evaluate_hybrid_model(model_cfg, X_train, y_train, X_val, y_val):\n    model_name = model_cfg[\"name\"]\n    print(f\"\\nTraining {model_name} with CNN feature extraction\")\n    \n    # Feature extraction with batch processing\n    feature_extractor = model_cfg[\"build_func\"](model_cfg[\"input_shape\"])\n    \n    # Process training data in batches\n    batch_size = 64  # Reduced batch size to prevent memory issues\n    X_train_features = []\n    for i in range(0, len(X_train), batch_size):\n        batch = X_train[i:i+batch_size]\n        features = feature_extractor.predict(batch, verbose=0)\n        X_train_features.append(features)\n    X_train_features = np.vstack(X_train_features)\n    \n    # Process validation data in batches\n    X_val_features = []\n    for i in range(0, len(X_val), batch_size):\n        batch = X_val[i:i+batch_size]\n        features = feature_extractor.predict(batch, verbose=0)\n        X_val_features.append(features)\n    X_val_features = np.vstack(X_val_features)\n    \n    # Train traditional model\n    if \"RF\" in model_name:\n        model = RandomForestClassifier(n_estimators=300, max_depth=15, \n                                      min_samples_split=5, n_jobs=-1,\n                                      class_weight='balanced', random_state=42)\n    else:  # XGBoost\n        # Calculate scale_pos_weight for binary classification\n        num_pos = np.sum(y_train == 1)\n        num_neg = len(y_train) - num_pos\n        scale_pos_weight = num_neg / num_pos if num_pos > 0 else 1.0\n        \n        model = XGBClassifier(n_estimators=500, max_depth=8, learning_rate=0.05,\n                             subsample=0.8, colsample_bytree=0.8, \n                             scale_pos_weight=scale_pos_weight,\n                             use_label_encoder=False, eval_metric='logloss')\n    \n    model.fit(X_train_features, y_train)\n    \n    # Evaluate\n    y_pred = model.predict(X_val_features)\n    y_proba = model.predict_proba(X_val_features) if hasattr(model, \"predict_proba\") else None\n    \n    # Save model\n    joblib.dump(model, os.path.join(RESULTS_DIR, f\"{model_name}_model.joblib\"))\n    \n    return generate_reports(model_name, y_val, y_pred, y_proba)\n\ndef train_evaluate_handcrafted_model(model_cfg, X_train, y_train, X_val, y_val):\n    model_name = model_cfg[\"name\"]\n    print(f\"\\nTraining {model_name} with hand-crafted features\")\n    \n    # Extract features\n    X_train_feats = extract_handcrafted_features(X_train)\n    X_val_feats = extract_handcrafted_features(X_val)\n    \n    # Build model\n    if \"SVM\" in model_name:\n        # SVM with class weighting\n        num_pos = np.sum(y_train == 1)\n        num_neg = len(y_train) - num_pos\n        class_weight = {0: 1, 1: num_neg/num_pos} if num_pos > 0 else {0: 1, 1: 1}\n        \n        model = SVC(\n            C=1.0,\n            kernel='rbf',\n            gamma='scale',\n            class_weight=class_weight,\n            probability=True,\n            random_state=42\n        )\n    else:  # Logistic Regression\n        model = LogisticRegression(\n            penalty='elasticnet',\n            solver='saga',\n            C=1.0,\n            l1_ratio=0.5,\n            class_weight='balanced',\n            max_iter=1000,\n            random_state=42\n        )\n    \n    # Train model\n    model.fit(X_train_feats, y_train)\n    \n    # Evaluate\n    y_pred = model.predict(X_val_feats)\n    y_proba = model.predict_proba(X_val_feats) if hasattr(model, \"predict_proba\") else None\n    \n    # Save model\n    joblib.dump(model, os.path.join(RESULTS_DIR, f\"{model_name}_model.joblib\"))\n    \n    return generate_reports(model_cfg['name'], y_val, y_pred, y_proba)\n\ndef generate_reports(model_name, y_true, y_pred, y_proba=None):\n    # Generate classification report\n    clf_report = classification_report(y_true, y_pred, target_names=CLASSES)\n    cm = confusion_matrix(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    acc = accuracy_score(y_true, y_pred)\n    \n    print(f\"\\nClassification Report for {model_name}:\\n{clf_report}\")\n    print(f\"Confusion Matrix for {model_name}:\\n{cm}\")\n    print(f\"✅ Validation F1 Score: {f1:.4f}, Accuracy: {acc:.4f}\")\n    \n    # Create enhanced confusion matrix with percentages and counts\n    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n    cm_percent = np.round(cm_percent, 1)  # Round to 1 decimal place\n    \n    plt.figure(figsize=(8, 6))\n    ax = sns.heatmap(cm_percent, annot=False, fmt=\".1f\", cmap=\"Blues\",\n                    xticklabels=CLASSES, yticklabels=CLASSES,\n                    cbar=True, linewidths=1, linecolor='gray')\n    \n    # Add percentage annotations\n    for i in range(len(CLASSES)):\n        for j in range(len(CLASSES)):\n            color = \"white\" if cm_percent[i, j] > 50 else \"black\"\n            ax.text(j + 0.5, i + 0.3, \n                    f\"{cm_percent[i, j]:.1f}%\", \n                    ha='center', va='center', \n                    color=color, fontsize=10)\n            # Add count below percentage\n            ax.text(j + 0.5, i + 0.7, \n                    f\"({cm[i, j]})\", \n                    ha='center', va='center', \n                    color=color, fontsize=9)\n    \n    plt.title(f'{model_name} Confusion Matrix\\nAccuracy: {acc:.4f}, F1: {f1:.4f}', fontsize=14)\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.ylabel('True Label', fontsize=12)\n    plt.xticks(fontsize=10)\n    plt.yticks(fontsize=10, rotation=0)\n    \n    # Add border\n    for _, spine in ax.spines.items():\n        spine.set_visible(True)\n        spine.set_linewidth(1.5)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(RESULTS_DIR, f'{model_name}_cm.png'), dpi=300)\n    plt.close()\n    \n    # Save classification report to text file\n    with open(os.path.join(RESULTS_DIR, f'{model_name}_report.txt'), 'w') as f:\n        f.write(f\"Model: {model_name}\\n\")\n        f.write(f\"Accuracy: {acc:.4f}\\n\")\n        f.write(f\"F1 Score (weighted): {f1:.4f}\\n\\n\")\n        f.write(\"Classification Report:\\n\")\n        f.write(clf_report)\n        f.write(\"\\n\\nConfusion Matrix (counts):\\n\")\n        f.write(np.array2string(cm, separator=', '))\n    \n    return {\n        'model': model_name,\n        'accuracy': acc,\n        'f1_score': f1,\n        'y_true': y_true,\n        'y_pred': y_pred,\n        'y_proba': y_proba\n    }\n\n# Main training and validation loop\nresults = []\nfor model_cfg in models_config:\n    try:\n        if model_cfg[\"type\"] == \"keras\":\n            model_result = train_evaluate_keras_model(\n                model_cfg, \n                X_train_raw, y_train,\n                X_val_raw, y_val\n            )\n        elif model_cfg[\"type\"] == \"hybrid\":\n            model_result = train_evaluate_hybrid_model(\n                model_cfg, \n                X_train_raw, y_train,\n                X_val_raw, y_val\n            )\n        elif model_cfg[\"type\"] == \"handcrafted\":\n            model_result = train_evaluate_handcrafted_model(\n                model_cfg, \n                X_train_raw, y_train,\n                X_val_raw, y_val\n            )\n        results.append(model_result)\n    except Exception as e:\n        print(f\"Error training {model_cfg['name']}: {str(e)}\")\n\n# Save results\nresults_df = pd.DataFrame([{\n    'model': r['model'],\n    'accuracy': r['accuracy'],\n    'f1_score': r['f1_score']\n} for r in results])\n\nresults_df.to_csv(os.path.join(RESULTS_DIR, 'model_results.csv'), index=False)\n\n# Find best models\nbest_acc_model = results_df.loc[results_df['accuracy'].idxmax()]\nbest_f1_model = results_df.loc[results_df['f1_score'].idxmax()]\n\nprint(\"\\n\\n=== FINAL RESULTS ===\")\nprint(f\"Best Accuracy Model: {best_acc_model['model']} (Accuracy: {best_acc_model['accuracy']:.4f})\")\nprint(f\"Best F1 Model: {best_f1_model['model']} (F1 Score: {best_f1_model['f1_score']:.4f})\")\n\n# Generate predictions on test set using the best model\nprint(\"\\nGenerating predictions on test set...\")\nbest_model_name = best_f1_model['model']\nbest_model_type = next((m['type'] for m in models_config if m['name'] == best_model_name), None)\n\ndef predict_in_batches(model, X, batch_size=64):\n    \"\"\"Make predictions in batches to avoid memory issues\"\"\"\n    predictions = []\n    for i in range(0, len(X), batch_size):\n        batch = X[i:i+batch_size]\n        batch_pred = model.predict(batch, verbose=0)\n        predictions.append(batch_pred)\n    return np.vstack(predictions)\n\nif best_model_type == \"keras\":\n    # Load the best keras model\n    model = tf.keras.models.load_model(os.path.join(RESULTS_DIR, f\"{best_model_name}_best_model.keras\"))\n    test_predictions = predict_in_batches(model, X_test_raw)\n    test_pred_classes = np.argmax(test_predictions, axis=1)\nelif best_model_type == \"hybrid\":\n    # Load feature extractor and classifier\n    model_cfg = next(m for m in models_config if m['name'] == best_model_name)\n    feature_extractor = model_cfg[\"build_func\"](model_cfg[\"input_shape\"])\n    \n    # Extract test features in batches\n    X_test_features = []\n    batch_size = 64\n    for i in range(0, len(X_test_raw), batch_size):\n        batch = X_test_raw[i:i+batch_size]\n        features = feature_extractor.predict(batch, verbose=0)\n        X_test_features.append(features)\n    X_test_features = np.vstack(X_test_features)\n    \n    model = joblib.load(os.path.join(RESULTS_DIR, f\"{best_model_name}_model.joblib\"))\n    test_pred_classes = model.predict(X_test_features)\nelse:  # handcrafted\n    model = joblib.load(os.path.join(RESULTS_DIR, f\"{best_model_name}_model.joblib\"))\n    X_test_feats = extract_handcrafted_features(X_test_raw)\n    test_pred_classes = model.predict(X_test_feats)\n\n# Map predictions back to class names\ntest_pred_labels = [CLASSES[i] for i in test_pred_classes]\n\n# Save test predictions\ntest_predictions_df = pd.DataFrame({\n    'processed_path': test_mi['processed_path'],\n    'prediction': test_pred_labels\n})\ntest_predictions_df.to_csv(os.path.join(RESULTS_DIR, 'test_predictions.csv'), index=False)\n\nprint(\"\\nTest predictions saved to:\", os.path.join(RESULTS_DIR, 'test_predictions.csv'))\nprint(\"Detailed results saved to:\", RESULTS_DIR)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-26T06:46:30.880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============== PART 1: MODELS 1-6 ==============\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport joblib\nimport pywt\nfrom scipy import signal, stats\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom tensorflow.keras.models import Model, save_model\nfrom tensorflow.keras.layers import (Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, \n                                     Dense, Flatten, LSTM, Reshape, Dropout, BatchNormalization,\n                                     Attention, Multiply, GlobalAveragePooling1D, Permute, \n                                     concatenate, SimpleRNN, GRU)\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.activations import elu, relu\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mne.decoding import CSP\n\n# Configuration\nDATA_ROOT = '/kaggle/input/preprocessed-mtc-aic'  # Root directory of the dataset\nBASE_DATA_PATH = os.path.join(DATA_ROOT, 'mtc-aic3_dataset_preprocessed')  # Full path to preprocessed data\nFS = 250  # Sampling rate\nMI_CHANNELS = ['C3', 'CZ', 'C4']  # Focus on central channels for motor imagery\nCHANNEL_INDICES = [1, 2, 3]  # Indices of C3, CZ, C4 in the original data\nN_CHANNELS = len(MI_CHANNELS)\nCLASSES = ['left', 'right']\nN_CLASSES = len(CLASSES)\nRESULTS_DIR = '/kaggle/working/results'\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\n# Load dataframes\ntrain_df = pd.read_csv(os.path.join(BASE_DATA_PATH, 'train.csv'))\nval_df = pd.read_csv(os.path.join(BASE_DATA_PATH, 'validation.csv'))\ntest_df = pd.read_csv(os.path.join(BASE_DATA_PATH, 'test.csv'))\n\n# Filter MI trials and focus on central channels\ndef filter_mi_channels(df):\n    mi_df = df[df['task'] == 'MI'].copy()\n    return mi_df\n\ntrain_mi = filter_mi_channels(train_df)\nval_mi = filter_mi_channels(val_df)\ntest_mi = filter_mi_channels(test_df)\n\n# Load preprocessed data and select MI channels\ndef load_data(df):\n    data = []\n    for path in df['processed_path']:\n        # Construct full path by joining with DATA_ROOT\n        full_path = os.path.join(DATA_ROOT, path.lstrip('./'))\n        with np.load(full_path) as npz_file:\n            full_data = npz_file['data']\n            # Select only central channels (C3, CZ, C4)\n            data.append(full_data[:, CHANNEL_INDICES])\n    return np.array(data)\n\nX_train_raw = load_data(train_mi)\nX_val_raw = load_data(val_mi)\nX_test_raw = load_data(test_mi)  # Test data for final predictions\n\n# Convert labels - handle case sensitivity\ndef map_label(label):\n    # Normalize label to lowercase for case-insensitive matching\n    normalized = label.strip().lower()\n    if normalized == 'left':\n        return 0\n    elif normalized == 'right':\n        return 1\n    else:\n        raise ValueError(f\"Invalid label: {label}\")\n\n# Use case-insensitive column lookup\ndef get_label_column(df):\n    for col in df.columns:\n        if col.lower() == 'label':\n            return col\n    return None  # Return None if not found\n\n# Get labels for train and validation\nlabel_col_train = get_label_column(train_mi)\nlabel_col_val = get_label_column(val_mi)\n\nif label_col_train is None or label_col_val is None:\n    raise KeyError(\"No 'label' column found in training or validation data\")\n\ny_train = train_mi[label_col_train].apply(map_label).values\ny_val = val_mi[label_col_val].apply(map_label).values\n\n# Standardize data\nscaler = StandardScaler()\nX_train_raw = scaler.fit_transform(\n    X_train_raw.reshape(-1, X_train_raw.shape[-1])\n).reshape(X_train_raw.shape)\nX_val_raw = scaler.transform(\n    X_val_raw.reshape(-1, X_val_raw.shape[-1])\n).reshape(X_val_raw.shape)\nX_test_raw = scaler.transform(\n    X_test_raw.reshape(-1, X_test_raw.shape[-1])\n).reshape(X_test_raw.shape)\n\n# Frequency bands for feature extraction\nFREQ_BANDS = {\n    'delta': (1, 4),\n    'theta': (4, 8),\n    'alpha': (8, 13),\n    'beta': (13, 30),\n    'gamma': (30, 45)\n}\n\n# Feature extraction functions - UPDATED STFT FUNCTION\ndef compute_stft(data, nperseg=256, noverlap=192):\n    n_trials, n_times, n_channels = data.shape\n    # Desired time dimension (32)\n    n_time_desired = 32\n    \n    # Initialize array with fixed dimensions\n    stft_data = np.zeros((n_trials, n_time_desired, 129, n_channels), dtype=np.float32)\n    \n    for c in range(n_channels):\n        # Compute STFT without boundary extension\n        _, _, Zxx = signal.stft(\n            data[:, :, c], \n            fs=FS, \n            nperseg=nperseg, \n            noverlap=noverlap,\n            axis=1,\n            boundary=None  # Disable boundary extension\n        )\n        Zxx = np.abs(Zxx)\n        \n        # Current time dimension\n        current_time = Zxx.shape[-1]\n        \n        # Pad or truncate to exactly 32 time steps\n        if current_time < n_time_desired:\n            # Pad with zeros at the end\n            pad_width = n_time_desired - current_time\n            Zxx = np.pad(Zxx, ((0, 0), (0, 0), (0, pad_width)), mode='constant')\n        elif current_time > n_time_desired:\n            # Truncate to desired length\n            Zxx = Zxx[..., :n_time_desired]\n        \n        # Transpose to (trials, time, freq)\n        stft_data[..., c] = Zxx.transpose(0, 2, 1)\n    \n    return stft_data\n\n# Fixed CSP feature extraction function - CORRECTED TO USE 3 COMPONENTS\ndef compute_csp_features(X, y, n_components=3):  # Changed to 3 components\n    \"\"\"Return full CSP time-series (n_trials, timesteps, n_components)\"\"\"\n    # Create CSP with transform_into='csp_space' to get time-series output\n    # Set log=None as required by transform_into='csp_space'\n    csp = CSP(n_components=n_components, reg=None, log=None, norm_trace=False,\n              transform_into='csp_space')\n    # Input shape: (trials, channels, time)\n    X_csp_time = csp.fit_transform(X.transpose(0, 2, 1), y)\n    # Output is (trials, components, time) -> transpose to (trials, time, components)\n    return X_csp_time.transpose(0, 2, 1), csp\n\n# Compute STFT features\nX_train_stft = compute_stft(X_train_raw)\nX_val_stft = compute_stft(X_val_raw)\n\n# Print STFT shapes for verification\nprint(\"\\n\" + \"=\"*50)\nprint(\"STFT Shape Verification\")\nprint(\"=\"*50)\nprint(f\"STFT Training Shape: {X_train_stft.shape}\")\nprint(f\"STFT Validation Shape: {X_val_stft.shape}\")\nprint(\"=\"*50 + \"\\n\")\n\n# Compute CSP features - now using 3 components\nX_train_csp, csp = compute_csp_features(X_train_raw, y_train)\n# Transform validation set (ensure same format as training)\nX_val_csp = csp.transform(X_val_raw.transpose(0, 2, 1)).transpose(0, 2, 1)\n\n# Print CSP shapes for verification\nprint(\"\\n\" + \"=\"*50)\nprint(\"CSP Shape Verification\")\nprint(\"=\"*50)\nprint(f\"CSP Training Shape: {X_train_csp.shape}\")\nprint(f\"CSP Validation Shape: {X_val_csp.shape}\")\nprint(\"=\"*50 + \"\\n\")\n\n# Create unified dataset dictionary for all representations\ndatasets = {\n    'raw': (X_train_raw, X_val_raw),\n    'stft': (X_train_stft, X_val_stft),\n    'csp': (X_train_csp, X_val_csp)\n}\n\n# Handcrafted feature extraction (focused on MI channels)\ndef extract_handcrafted_features(X):\n    \"\"\"Extract domain-specific features for traditional ML models\"\"\"\n    n_trials, n_timesteps, n_channels = X.shape\n    features = []\n    \n    for i in range(n_trials):\n        trial_features = []\n        \n        # Channel-specific features\n        for ch in range(n_channels):\n            channel_data = X[i, :, ch]\n            \n            # Time-domain features\n            trial_features.append(np.mean(channel_data))\n            trial_features.append(np.std(channel_data))\n            trial_features.append(stats.skew(channel_data))\n            trial_features.append(stats.kurtosis(channel_data))\n            trial_features.append(np.median(np.abs(channel_data)))\n            \n            # Frequency-domain features\n            f, Pxx = signal.welch(channel_data, fs=FS, nperseg=256)\n            for band, (low, high) in FREQ_BANDS.items():\n                band_mask = (f >= low) & (f <= high)\n                trial_features.append(np.log1p(np.sum(Pxx[band_mask])))\n        \n        # Cross-channel features (C3-C4 asymmetry - most important for MI)\n        c3_data = X[i, :, 0]  # C3 is first channel\n        c4_data = X[i, :, 2]  # C4 is third channel\n            \n        # Time-domain asymmetry\n        trial_features.append(np.mean(c3_data - c4_data))\n        trial_features.append(np.mean(np.abs(c3_data) - np.mean(np.abs(c4_data))))\n        \n        # Frequency-domain asymmetry\n        for band in FREQ_BANDS:\n            c3_band = trial_features[5*0 + 4 + list(FREQ_BANDS.keys()).index(band) + 1]\n            c4_band = trial_features[5*2 + 4 + list(FREQ_BANDS.keys()).index(band) + 1]\n            trial_features.append(c3_band - c4_band)\n        \n        # Hjorth parameters\n        def hjorth_parameters(data):\n            diff1 = np.diff(data)\n            diff2 = np.diff(diff1)\n            var0 = np.var(data)\n            var1 = np.var(diff1)\n            var2 = np.var(diff2)\n            activity = var0\n            mobility = np.sqrt(var1 / var0)\n            complexity = np.sqrt(var2 / var1) / mobility\n            return activity, mobility, complexity\n        \n        for ch, name in zip([0, 2], ['C3', 'C4']):\n            activity, mobility, complexity = hjorth_parameters(X[i, :, ch])\n            trial_features.extend([activity, mobility, complexity])\n        \n        features.append(trial_features)\n    \n    return np.array(features)\n\n# Data augmentation for CNN models\ndef augment_data(X, y, augmentation_factor=1):\n    X_aug = [X]\n    y_aug = [y]\n    \n    for _ in range(augmentation_factor):\n        # Gaussian noise\n        noise = np.random.normal(0, 0.05, X.shape)\n        X_aug.append(X + noise)\n        y_aug.append(y)\n        \n        # Time warping\n        warp_factor = 0.2\n        warp_points = int(X.shape[1] * warp_factor)\n        X_warped = np.zeros_like(X)\n        for i in range(X.shape[0]):\n            start = np.random.randint(0, warp_points)\n            end = np.random.randint(X.shape[1] - warp_points, X.shape[1])\n            for c in range(X.shape[2]):\n                X_warped[i, :, c] = np.interp(\n                    np.arange(X.shape[1]),\n                    np.linspace(0, X.shape[1]-1, num=end-start),\n                    X[i, start:end, c]\n                )\n        X_aug.append(X_warped)\n        y_aug.append(y)\n    \n    return np.vstack(X_aug), np.hstack(y_aug)\n\n# Model definitions\ndef build_model1(input_shape):\n    model = tf.keras.Sequential([\n        Conv1D(32, 5, activation='relu', input_shape=input_shape),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Conv1D(64, 5, activation='relu'),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Flatten(),\n        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n        Dropout(0.5),\n        Dense(N_CLASSES, activation='softmax')\n    ])\n    return model\n\ndef build_model2(input_shape):\n    model = tf.keras.Sequential([\n        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Conv2D(64, (3, 3), activation='relu'),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Conv2D(128, (3, 3), activation='relu'),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Flatten(),\n        Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n        Dropout(0.5),\n        Dense(128, activation='relu'),\n        Dense(N_CLASSES, activation='softmax')\n    ])\n    return model\n\ndef build_model3(input_shape):\n    inputs = Input(shape=input_shape)\n    x = Conv1D(32, 5, activation='elu')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(64, 5, activation='elu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(128, 5, activation='elu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(256, 5, activation='elu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Flatten()(x)\n    x = Dense(256, activation='elu', kernel_regularizer=regularizers.l2(0.001))(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(N_CLASSES, activation='softmax')(x)\n    return Model(inputs, outputs)\n\ndef build_model4(input_shape):\n    model = tf.keras.Sequential([\n        Conv1D(32, 5, activation='elu', input_shape=input_shape),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Conv1D(64, 5, activation='elu'),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Conv1D(128, 5, activation='elu'),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Flatten(),\n        Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.001)),\n        Dropout(0.4),\n        Dense(N_CLASSES, activation='softmax')\n    ])\n    return model\n\ndef build_model5(input_shape):\n    inputs = Input(shape=input_shape)\n    x = Conv1D(32, 5, activation='relu')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(64, 5, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(128, 5, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = LSTM(64, return_sequences=True)(x)\n    x = LSTM(32)(x)\n    x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n    x = Dropout(0.5)(x)\n    x = Dense(64, activation='relu')(x)\n    outputs = Dense(N_CLASSES, activation='softmax')(x)\n    return Model(inputs, outputs)\n\ndef build_model6(input_shape):\n    model = tf.keras.Sequential([\n        Conv1D(32, 5, activation='elu', input_shape=input_shape),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Conv1D(64, 5, activation='elu'),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Conv1D(128, 5, activation='elu'),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Conv1D(256, 5, activation='elu'),\n        BatchNormalization(),\n        MaxPooling1D(2),\n        Flatten(),\n        Dense(512, activation='elu', kernel_regularizer=regularizers.l2(0.001)),\n        Dropout(0.6),\n        Dense(256, activation='elu'),\n        Dense(128, activation='elu'),\n        Dense(N_CLASSES, activation='softmax')\n    ])\n    return model\n\n# Model configurations with corrected input shapes\nmodels_config = [\n    {\"name\": \"SimpleCNN\", \"build_func\": build_model1, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"keras\"},\n    {\"name\": \"STFT_CNN\", \"build_func\": build_model2, \"input_shape\": (32, 129, N_CHANNELS), \"rep\": \"stft\", \"type\": \"keras\"},  # Corrected shape\n    {\"name\": \"7Conv_ELU\", \"build_func\": build_model3, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"keras\"},\n    {\"name\": \"3Conv_ELU\", \"build_func\": build_model4, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"keras\"},\n    {\"name\": \"CNN_LSTM\", \"build_func\": build_model5, \"input_shape\": (2250, 3), \"rep\": \"csp\", \"type\": \"keras\"},  # Corrected to 3 components\n    {\"name\": \"CSP_CNN\", \"build_func\": build_model6, \"input_shape\": (2250, 3), \"rep\": \"csp\", \"type\": \"keras\"},  # Corrected to 3 components\n]\n\n# Training and evaluation functions\ndef train_evaluate_keras_model(model_cfg, datasets, y_train, y_val):\n    model_name = model_cfg[\"name\"]\n    rep = model_cfg[\"rep\"]\n    print(f\"\\n{'='*50}\\nTraining {model_name} (using {rep} representation)\\n{'='*50}\")\n    \n    # Get data from representation dictionary\n    X_tr, X_v = datasets[rep]\n    \n    # Data augmentation only for raw data\n    if rep == \"raw\":\n        X_tr, y_tr = augment_data(X_tr, y_train, augmentation_factor=2)\n    else:\n        y_tr = y_train\n\n    # Verify input shape\n    expected_shape = model_cfg[\"input_shape\"]\n    print(f\"Expected input shape: {expected_shape}, Actual shape: {X_tr.shape[1:]}\")\n    \n    if X_tr.shape[1:] != expected_shape:\n        raise ValueError(\n            f\"Input shape mismatch in {model_name}! Expected {expected_shape}, \"\n            f\"got {X_tr.shape[1:]}. Full shape: {X_tr.shape}\"\n        )\n    \n    # Build and compile model\n    model = model_cfg[\"build_func\"](model_cfg[\"input_shape\"])\n    \n    # Special compilation for regression hybrid\n    if model_name == \"CNN_Regression\":\n        model.compile(optimizer=Adam(learning_rate=0.001),\n                      loss=['sparse_categorical_crossentropy', 'mse'],\n                      metrics={'dense_2': 'accuracy'},\n                      loss_weights=[0.9, 0.1])\n    else:\n        model.compile(optimizer=Adam(learning_rate=0.001),\n                      loss='sparse_categorical_crossentropy',\n                      metrics=['accuracy'])\n    \n    # Callbacks\n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n        ModelCheckpoint(os.path.join(RESULTS_DIR, f\"{model_name}_best_model.keras\"), \n                        save_best_only=True, monitor='val_accuracy'),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n    ]\n    \n    # Train model\n    history = model.fit(\n        X_tr, y_tr,\n        validation_data=(X_v, y_val),\n        epochs=150,\n        batch_size=32,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Save final model\n    model.save(os.path.join(RESULTS_DIR, f\"{model_name}_final_model.keras\"))\n    \n    y_proba = model.predict(X_v)\n    y_pred = np.argmax(y_proba, axis=1)\n\n    # Calculate weighted F1 score\n    f1_weighted = f1_score(y_val, y_pred, average='weighted')\n    print(f\"\\n✅ {model_cfg['name']} Validation Weighted F1: {f1_weighted:.4f}\")\n    \n    return generate_reports(model_name, y_val, y_pred, y_proba)\n\ndef generate_reports(model_name, y_true, y_pred, y_proba=None):\n    # Generate classification report\n    clf_report = classification_report(y_true, y_pred, target_names=CLASSES)\n    cm = confusion_matrix(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    acc = accuracy_score(y_true, y_pred)\n    \n    print(f\"\\nClassification Report for {model_name}:\\n{clf_report}\")\n    print(f\"Confusion Matrix for {model_name}:\\n{cm}\")\n    print(f\"✅ Validation F1 Score: {f1:.4f}, Accuracy: {acc:.4f}\")\n    \n    # Create enhanced confusion matrix with percentages and counts\n    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n    cm_percent = np.round(cm_percent, 1)  # Round to 1 decimal place\n    \n    plt.figure(figsize=(8, 6))\n    ax = sns.heatmap(cm_percent, annot=False, fmt=\".1f\", cmap=\"Blues\",\n                    xticklabels=CLASSES, yticklabels=CLASSES,\n                    cbar=True, linewidths=1, linecolor='gray')\n    \n    # Add percentage annotations\n    for i in range(len(CLASSES)):\n        for j in range(len(CLASSES)):\n            color = \"white\" if cm_percent[i, j] > 50 else \"black\"\n            ax.text(j + 0.5, i + 0.3, \n                    f\"{cm_percent[i, j]:.1f}%\", \n                    ha='center', va='center', \n                    color=color, fontsize=10)\n            # Add count below percentage\n            ax.text(j + 0.5, i + 0.7, \n                    f\"({cm[i, j]})\", \n                    ha='center', va='center', \n                    color=color, fontsize=9)\n    \n    plt.title(f'{model_name} Confusion Matrix\\nAccuracy: {acc:.4f}, F1: {f1:.4f}', fontsize=14)\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.ylabel('True Label', fontsize=12)\n    plt.xticks(fontsize=10)\n    plt.yticks(fontsize=10, rotation=0)\n    \n    # Add border\n    for _, spine in ax.spines.items():\n        spine.set_visible(True)\n        spine.set_linewidth(1.5)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(RESULTS_DIR, f'{model_name}_cm.png'), dpi=300)\n    plt.close()\n    \n    # Save classification report to text file\n    with open(os.path.join(RESULTS_DIR, f'{model_name}_report.txt'), 'w') as f:\n        f.write(f\"Model: {model_name}\\n\")\n        f.write(f\"Accuracy: {acc:.4f}\\n\")\n        f.write(f\"F1 Score (weighted): {f1:.4f}\\n\\n\")\n        f.write(\"Classification Report:\\n\")\n        f.write(clf_report)\n        f.write(\"\\n\\nConfusion Matrix (counts):\\n\")\n        f.write(np.array2string(cm, separator=', '))\n    \n    return {\n        'model': model_name,\n        'accuracy': acc,\n        'f1_score': f1,\n        'y_true': y_true,\n        'y_pred': y_pred,\n        'y_proba': y_proba\n    }\n\n# Main training and validation loop for models 1-6\nresults = []\nfor model_cfg in models_config:\n    try:\n        if model_cfg[\"type\"] == \"keras\":\n            model_result = train_evaluate_keras_model(\n                model_cfg, \n                datasets,  # Pass representations dictionary\n                y_train, \n                y_val\n            )\n            results.append(model_result)\n    except Exception as e:\n        print(f\"\\n❌ Error training {model_cfg['name']}: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n\n# Save results\nresults_df = pd.DataFrame([{\n    'model': r['model'],\n    'accuracy': r['accuracy'],\n    'f1_score': r['f1_score']\n} for r in results])\n\nresults_df.to_csv(os.path.join(RESULTS_DIR, 'model_results_part1.csv'), index=False)\nprint(\"\\nPart 1 completed. Results saved for models 1-6.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-26T06:46:30.880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============== PART 2: MODELS 7-13 ==============\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport joblib\nimport pywt\nfrom scipy import signal, stats\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom tensorflow.keras.models import Model, save_model\nfrom tensorflow.keras.layers import (Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, \n                                     Dense, Flatten, LSTM, Reshape, Dropout, BatchNormalization,\n                                     Attention, Multiply, GlobalAveragePooling1D, Permute, \n                                     concatenate, SimpleRNN, GRU)\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.activations import elu, relu\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mne.decoding import CSP\n\n# Configuration\nDATA_ROOT = '/kaggle/input/preprocessed-mtc-aic'  # Root directory of the dataset\nBASE_DATA_PATH = os.path.join(DATA_ROOT, 'mtc-aic3_dataset_preprocessed')  # Full path to preprocessed data\nFS = 250  # Sampling rate\nMI_CHANNELS = ['C3', 'CZ', 'C4']  # Focus on central channels for motor imagery\nCHANNEL_INDICES = [1, 2, 3]  # Indices of C3, CZ, C4 in the original data\nN_CHANNELS = len(MI_CHANNELS)\nCLASSES = ['left', 'right']\nN_CLASSES = len(CLASSES)\nRESULTS_DIR = '/kaggle/working/results'\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\n# Load dataframes\ntrain_df = pd.read_csv(os.path.join(BASE_DATA_PATH, 'train.csv'))\nval_df = pd.read_csv(os.path.join(BASE_DATA_PATH, 'validation.csv'))\ntest_df = pd.read_csv(os.path.join(BASE_DATA_PATH, 'test.csv'))\n\n# Filter MI trials and focus on central channels\ndef filter_mi_channels(df):\n    mi_df = df[df['task'] == 'MI'].copy()\n    return mi_df\n\ntrain_mi = filter_mi_channels(train_df)\nval_mi = filter_mi_channels(val_df)\ntest_mi = filter_mi_channels(test_df)\n\n# Load preprocessed data and select MI channels\ndef load_data(df):\n    data = []\n    for path in df['processed_path']:\n        # Construct full path by joining with DATA_ROOT\n        full_path = os.path.join(DATA_ROOT, path.lstrip('./'))\n        with np.load(full_path) as npz_file:\n            full_data = npz_file['data']\n            # Select only central channels (C3, CZ, C4)\n            data.append(full_data[:, CHANNEL_INDICES])\n    return np.array(data)\n\nX_train_raw = load_data(train_mi)\nX_val_raw = load_data(val_mi)\nX_test_raw = load_data(test_mi)  # Test data for final predictions\n\n# Convert labels - handle case sensitivity\ndef map_label(label):\n    # Normalize label to lowercase for case-insensitive matching\n    normalized = label.strip().lower()\n    if normalized == 'left':\n        return 0\n    elif normalized == 'right':\n        return 1\n    else:\n        raise ValueError(f\"Invalid label: {label}\")\n\n# Use case-insensitive column lookup\ndef get_label_column(df):\n    for col in df.columns:\n        if col.lower() == 'label':\n            return col\n    return None  # Return None if not found\n\n# Get labels for train and validation\nlabel_col_train = get_label_column(train_mi)\nlabel_col_val = get_label_column(val_mi)\n\nif label_col_train is None or label_col_val is None:\n    raise KeyError(\"No 'label' column found in training or validation data\")\n\ny_train = train_mi[label_col_train].apply(map_label).values\ny_val = val_mi[label_col_val].apply(map_label).values\n\n# Standardize data\nscaler = StandardScaler()\nX_train_raw = scaler.fit_transform(\n    X_train_raw.reshape(-1, X_train_raw.shape[-1])\n).reshape(X_train_raw.shape)\nX_val_raw = scaler.transform(\n    X_val_raw.reshape(-1, X_val_raw.shape[-1])\n).reshape(X_val_raw.shape)\nX_test_raw = scaler.transform(\n    X_test_raw.reshape(-1, X_test_raw.shape[-1])\n).reshape(X_test_raw.shape)\n\n# Frequency bands for feature extraction\nFREQ_BANDS = {\n    'delta': (1, 4),\n    'theta': (4, 8),\n    'alpha': (8, 13),\n    'beta': (13, 30),\n    'gamma': (30, 45)\n}\n\n# Feature extraction functions\ndef compute_stft(data, nperseg=256, noverlap=192):\n    n_trials, n_times, n_channels = data.shape\n    f, t, Zxx = signal.stft(data[:, :, 0], fs=FS, nperseg=nperseg, noverlap=noverlap, axis=1)\n    n_freq, n_time = Zxx.shape[1:3]\n    stft_data = np.zeros((n_trials, n_time, n_freq, n_channels), dtype=np.float32)\n    \n    for c in range(n_channels):\n        _, _, Zxx = signal.stft(data[:, :, c], fs=FS, nperseg=nperseg, noverlap=noverlap, axis=1)\n        stft_data[..., c] = np.abs(Zxx).transpose(0, 2, 1)\n    return stft_data\n\n# Fixed CSP feature extraction function\ndef compute_csp_features(X, y, n_components=4):\n    \"\"\"Return full CSP time-series (n_trials, timesteps, n_components)\"\"\"\n    # Create CSP with transform_into='csp_space' to get time-series output\n    # Set log=None as required by transform_into='csp_space'\n    csp = CSP(n_components=n_components, reg=None, log=None, norm_trace=False,\n              transform_into='csp_space')\n    # Input shape: (trials, channels, time)\n    X_csp_time = csp.fit_transform(X.transpose(0, 2, 1), y)\n    # Output is (trials, components, time) -> transpose to (trials, time, components)\n    return X_csp_time.transpose(0, 2, 1), csp\n\n# Compute STFT features\nX_train_stft = compute_stft(X_train_raw)\nX_val_stft = compute_stft(X_val_raw)\n\n# Compute CSP features\nX_train_csp, csp = compute_csp_features(X_train_raw, y_train)\n# Transform validation set (ensure same format as training)\nX_val_csp = csp.transform(X_val_raw.transpose(0, 2, 1)).transpose(0, 2, 1)\n\n# Handcrafted feature extraction (focused on MI channels)\ndef extract_handcrafted_features(X):\n    \"\"\"Extract domain-specific features for traditional ML models\"\"\"\n    n_trials, n_timesteps, n_channels = X.shape\n    features = []\n    \n    for i in range(n_trials):\n        trial_features = []\n        \n        # Channel-specific features\n        for ch in range(n_channels):\n            channel_data = X[i, :, ch]\n            \n            # Time-domain features\n            trial_features.append(np.mean(channel_data))\n            trial_features.append(np.std(channel_data))\n            trial_features.append(stats.skew(channel_data))\n            trial_features.append(stats.kurtosis(channel_data))\n            trial_features.append(np.median(np.abs(channel_data)))\n            \n            # Frequency-domain features\n            f, Pxx = signal.welch(channel_data, fs=FS, nperseg=256)\n            for band, (low, high) in FREQ_BANDS.items():\n                band_mask = (f >= low) & (f <= high)\n                trial_features.append(np.log1p(np.sum(Pxx[band_mask])))\n        \n        # Cross-channel features (C3-C4 asymmetry - most important for MI)\n        c3_data = X[i, :, 0]  # C3 is first channel\n        c4_data = X[i, :, 2]  # C4 is third channel\n            \n        # Time-domain asymmetry\n        trial_features.append(np.mean(c3_data - c4_data))\n        trial_features.append(np.mean(np.abs(c3_data) - np.mean(np.abs(c4_data))))\n        \n        # Frequency-domain asymmetry\n        for band in FREQ_BANDS:\n            c3_band = trial_features[5*0 + 4 + list(FREQ_BANDS.keys()).index(band) + 1]\n            c4_band = trial_features[5*2 + 4 + list(FREQ_BANDS.keys()).index(band) + 1]\n            trial_features.append(c3_band - c4_band)\n        \n        # Hjorth parameters\n        def hjorth_parameters(data):\n            diff1 = np.diff(data)\n            diff2 = np.diff(diff1)\n            var0 = np.var(data)\n            var1 = np.var(diff1)\n            var2 = np.var(diff2)\n            activity = var0\n            mobility = np.sqrt(var1 / var0)\n            complexity = np.sqrt(var2 / var1) / mobility\n            return activity, mobility, complexity\n        \n        for ch, name in zip([0, 2], ['C3', 'C4']):\n            activity, mobility, complexity = hjorth_parameters(X[i, :, ch])\n            trial_features.extend([activity, mobility, complexity])\n        \n        features.append(trial_features)\n    \n    return np.array(features)\n\n# Model definitions for 7-13\ndef build_model7(input_shape):\n    inputs = Input(shape=input_shape)\n    \n    # CNN Branch\n    x = Conv1D(32, 5, activation='relu')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(64, 5, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    \n    # Self-Attention\n    attention = Dense(1, activation='tanh')(x)\n    attention = Flatten()(attention)\n    attention = tf.keras.activations.softmax(attention)\n    attention = Reshape((-1, 1))(attention)\n    x = Multiply()([x, attention])\n    \n    x = Flatten()(x)\n    x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(N_CLASSES, activation='softmax')(x)\n    return Model(inputs, outputs)\n\ndef build_model8(input_shape):\n    inputs = Input(shape=input_shape)\n    \n    # CNN Part\n    x = Conv1D(32, 5, activation='relu')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(64, 5, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Flatten()(x)\n    \n    # MLP Part\n    x = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n    x = Dropout(0.5)(x)\n    x = Dense(128, activation='relu')(x)\n    outputs = Dense(N_CLASSES, activation='softmax')(x)\n    return Model(inputs, outputs)\n\ndef build_model9_simple(input_shape):\n    \"\"\"Single-output version without regression head\"\"\"\n    inputs = Input(shape=input_shape)\n    x = Conv1D(32, 5, activation='relu')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(64, 5, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Flatten()(x)\n    x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n    outputs = Dense(N_CLASSES, activation='softmax')(x)\n    return Model(inputs, outputs)\n\ndef build_model10(input_shape):\n    inputs = Input(shape=input_shape)\n    x = Conv1D(32, 5, activation='relu')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Conv1D(64, 5, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n    x = Flatten()(x)\n    feature_extractor = Model(inputs, x)\n    return feature_extractor\n\ndef build_model11(input_shape):\n    return build_model10(input_shape)\n\n# Model configurations for 7-13\nmodels_config = [\n    {\"name\": \"CNN_Attention\", \"build_func\": build_model7, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"keras\"},\n    {\"name\": \"CNN_MLP\", \"build_func\": build_model8, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"keras\"},\n    {\"name\": \"CNN_Regression\", \"build_func\": build_model9_simple, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"keras\"},\n    {\"name\": \"CNN_RF\", \"build_func\": build_model10, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"hybrid\"},\n    {\"name\": \"CNN_XGB\", \"build_func\": build_model11, \"input_shape\": (2250, N_CHANNELS), \"rep\": \"raw\", \"type\": \"hybrid\"},\n    {\"name\": \"Handcrafted_SVM\", \"type\": \"handcrafted\"},\n    {\"name\": \"Handcrafted_LR\", \"type\": \"handcrafted\"},\n]\n\n# Training and evaluation functions\ndef train_evaluate_keras_model(model_cfg, X_train, y_train, X_val, y_val):\n    model_name = model_cfg[\"name\"]\n    print(f\"\\n{'='*50}\\nTraining {model_name}\\n{'='*50}\")\n    \n    # Data preparation\n    if model_cfg[\"rep\"] == \"raw\":\n        X_tr, X_v = X_train, X_val\n    elif model_cfg[\"rep\"] == \"stft\":\n        X_tr, X_v = X_train_stft, X_val_stft\n    elif model_cfg[\"rep\"] == \"csp\":\n        X_tr, X_v = X_train_csp, X_val_csp\n    \n    # Data augmentation\n    if model_cfg[\"rep\"] == \"raw\":\n        X_tr, y_tr = augment_data(X_tr, y_train, augmentation_factor=2)\n    else:\n        X_tr, y_tr = X_tr, y_train\n\n    expected_shape = model_cfg[\"input_shape\"]\n    if X_tr.shape[1:] != expected_shape:\n        raise ValueError(f\"Input shape mismatch! Expected {expected_shape}, \"f\"got {X_tr.shape[1:]} for {model_cfg['name']}\")\n    \n    # Build and compile model\n    model = model_cfg[\"build_func\"](model_cfg[\"input_shape\"])\n    \n    # Special compilation for regression hybrid\n    if model_name == \"CNN_Regression\":\n        model.compile(optimizer=Adam(learning_rate=0.001),\n                      loss=['sparse_categorical_crossentropy', 'mse'],\n                      metrics={'dense_2': 'accuracy'},\n                      loss_weights=[0.9, 0.1])\n    else:\n        model.compile(optimizer=Adam(learning_rate=0.001),\n                      loss='sparse_categorical_crossentropy',\n                      metrics=['accuracy'])\n    \n    # Callbacks\n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n        ModelCheckpoint(os.path.join(RESULTS_DIR, f\"{model_name}_best_model.keras\"), \n                        save_best_only=True, monitor='val_accuracy'),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n    ]\n    \n    # Train model\n    history = model.fit(\n        X_tr, y_tr,\n        validation_data=(X_v, y_val),\n        epochs=150,\n        batch_size=32,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Save final model\n    model.save(os.path.join(RESULTS_DIR, f\"{model_name}_final_model.keras\"))\n    \n    y_proba = model.predict(X_v)\n    y_pred = np.argmax(y_proba, axis=1)\n\n    # Calculate weighted F1 score\n    f1_weighted = f1_score(y_val, y_pred, average='weighted')\n    print(f\"\\n✅ {model_cfg['name']} Validation Weighted F1: {f1_weighted:.4f}\")\n    \n    return generate_reports(model_name, y_val, y_pred, y_proba)\n\ndef train_evaluate_hybrid_model(model_cfg, X_train, y_train, X_val, y_val):\n    model_name = model_cfg[\"name\"]\n    print(f\"\\nTraining {model_name} with CNN feature extraction\")\n    \n    # Feature extraction with batch processing\n    feature_extractor = model_cfg[\"build_func\"](model_cfg[\"input_shape\"])\n    \n    # Process training data in batches\n    batch_size = 64  # Reduced batch size to prevent memory issues\n    X_train_features = []\n    for i in range(0, len(X_train), batch_size):\n        batch = X_train[i:i+batch_size]\n        features = feature_extractor.predict(batch, verbose=0)\n        X_train_features.append(features)\n    X_train_features = np.vstack(X_train_features)\n    \n    # Process validation data in batches\n    X_val_features = []\n    for i in range(0, len(X_val), batch_size):\n        batch = X_val[i:i+batch_size]\n        features = feature_extractor.predict(batch, verbose=0)\n        X_val_features.append(features)\n    X_val_features = np.vstack(X_val_features)\n    \n    # Train traditional model\n    if \"RF\" in model_name:\n        model = RandomForestClassifier(n_estimators=300, max_depth=15, \n                                      min_samples_split=5, n_jobs=-1,\n                                      class_weight='balanced', random_state=42)\n    else:  # XGBoost\n        # Calculate scale_pos_weight for binary classification\n        num_pos = np.sum(y_train == 1)\n        num_neg = len(y_train) - num_pos\n        scale_pos_weight = num_neg / num_pos if num_pos > 0 else 1.0\n        \n        model = XGBClassifier(n_estimators=500, max_depth=8, learning_rate=0.05,\n                             subsample=0.8, colsample_bytree=0.8, \n                             scale_pos_weight=scale_pos_weight,\n                             use_label_encoder=False, eval_metric='logloss')\n    \n    model.fit(X_train_features, y_train)\n    \n    # Evaluate\n    y_pred = model.predict(X_val_features)\n    y_proba = model.predict_proba(X_val_features) if hasattr(model, \"predict_proba\") else None\n    \n    # Save model\n    joblib.dump(model, os.path.join(RESULTS_DIR, f\"{model_name}_model.joblib\"))\n    \n    return generate_reports(model_name, y_val, y_pred, y_proba)\n\ndef train_evaluate_handcrafted_model(model_cfg, X_train, y_train, X_val, y_val):\n    model_name = model_cfg[\"name\"]\n    print(f\"\\nTraining {model_name} with hand-crafted features\")\n    \n    # Extract features\n    X_train_feats = extract_handcrafted_features(X_train)\n    X_val_feats = extract_handcrafted_features(X_val)\n    \n    # Build model\n    if \"SVM\" in model_name:\n        # SVM with class weighting\n        num_pos = np.sum(y_train == 1)\n        num_neg = len(y_train) - num_pos\n        class_weight = {0: 1, 1: num_neg/num_pos} if num_pos > 0 else {0: 1, 1: 1}\n        \n        model = SVC(\n            C=1.0,\n            kernel='rbf',\n            gamma='scale',\n            class_weight=class_weight,\n            probability=True,\n            random_state=42\n        )\n    else:  # Logistic Regression\n        model = LogisticRegression(\n            penalty='elasticnet',\n            solver='saga',\n            C=1.0,\n            l1_ratio=0.5,\n            class_weight='balanced',\n            max_iter=1000,\n            random_state=42\n        )\n    \n    # Train model\n    model.fit(X_train_feats, y_train)\n    \n    # Evaluate\n    y_pred = model.predict(X_val_feats)\n    y_proba = model.predict_proba(X_val_feats) if hasattr(model, \"predict_proba\") else None\n    \n    # Save model\n    joblib.dump(model, os.path.join(RESULTS_DIR, f\"{model_name}_model.joblib\"))\n    \n    return generate_reports(model_cfg['name'], y_val, y_pred, y_proba)\n\ndef generate_reports(model_name, y_true, y_pred, y_proba=None):\n    # Generate classification report\n    clf_report = classification_report(y_true, y_pred, target_names=CLASSES)\n    cm = confusion_matrix(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    acc = accuracy_score(y_true, y_pred)\n    \n    print(f\"\\nClassification Report for {model_name}:\\n{clf_report}\")\n    print(f\"Confusion Matrix for {model_name}:\\n{cm}\")\n    print(f\"✅ Validation F1 Score: {f1:.4f}, Accuracy: {acc:.4f}\")\n    \n    # Create enhanced confusion matrix with percentages and counts\n    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n    cm_percent = np.round(cm_percent, 1)  # Round to 1 decimal place\n    \n    plt.figure(figsize=(8, 6))\n    ax = sns.heatmap(cm_percent, annot=False, fmt=\".1f\", cmap=\"Blues\",\n                    xticklabels=CLASSES, yticklabels=CLASSES,\n                    cbar=True, linewidths=1, linecolor='gray')\n    \n    # Add percentage annotations\n    for i in range(len(CLASSES)):\n        for j in range(len(CLASSES)):\n            color = \"white\" if cm_percent[i, j] > 50 else \"black\"\n            ax.text(j + 0.5, i + 0.3, \n                    f\"{cm_percent[i, j]:.1f}%\", \n                    ha='center', va='center', \n                    color=color, fontsize=10)\n            # Add count below percentage\n            ax.text(j + 0.5, i + 0.7, \n                    f\"({cm[i, j]})\", \n                    ha='center', va='center', \n                    color=color, fontsize=9)\n    \n    plt.title(f'{model_name} Confusion Matrix\\nAccuracy: {acc:.4f}, F1: {f1:.4f}', fontsize=14)\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.ylabel('True Label', fontsize=12)\n    plt.xticks(fontsize=10)\n    plt.yticks(fontsize=10, rotation=0)\n    \n    # Add border\n    for _, spine in ax.spines.items():\n        spine.set_visible(True)\n        spine.set_linewidth(1.5)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(RESULTS_DIR, f'{model_name}_cm.png'), dpi=300)\n    plt.close()\n    \n    # Save classification report to text file\n    with open(os.path.join(RESULTS_DIR, f'{model_name}_report.txt'), 'w') as f:\n        f.write(f\"Model: {model_name}\\n\")\n        f.write(f\"Accuracy: {acc:.4f}\\n\")\n        f.write(f\"F1 Score (weighted): {f1:.4f}\\n\\n\")\n        f.write(\"Classification Report:\\n\")\n        f.write(clf_report)\n        f.write(\"\\n\\nConfusion Matrix (counts):\\n\")\n        f.write(np.array2string(cm, separator=', '))\n    \n    return {\n        'model': model_name,\n        'accuracy': acc,\n        'f1_score': f1,\n        'y_true': y_true,\n        'y_pred': y_pred,\n        'y_proba': y_proba\n    }\n\n# Main training and validation loop for models 7-13\nresults = []\nfor model_cfg in models_config:\n    try:\n        if model_cfg[\"type\"] == \"keras\":\n            model_result = train_evaluate_keras_model(\n                model_cfg, \n                X_train_raw, y_train,\n                X_val_raw, y_val\n            )\n        elif model_cfg[\"type\"] == \"hybrid\":\n            model_result = train_evaluate_hybrid_model(\n                model_cfg, \n                X_train_raw, y_train,\n                X_val_raw, y_val\n            )\n        elif model_cfg[\"type\"] == \"handcrafted\":\n            model_result = train_evaluate_handcrafted_model(\n                model_cfg, \n                X_train_raw, y_train,\n                X_val_raw, y_val\n            )\n        results.append(model_result)\n    except Exception as e:\n        print(f\"Error training {model_cfg['name']}: {str(e)}\")\n\n# Save results for Part 2\nresults_df_part2 = pd.DataFrame([{\n    'model': r['model'],\n    'accuracy': r['accuracy'],\n    'f1_score': r['f1_score']\n} for r in results])\nresults_df_part2.to_csv(os.path.join(RESULTS_DIR, 'model_results_part2.csv'), index=False)\n\n# Load Part 1 results and combine\nresults_df_part1 = pd.read_csv(os.path.join(RESULTS_DIR, 'model_results_part1.csv'))\nresults_df = pd.concat([results_df_part1, results_df_part2], ignore_index=True)\n\n# Find best models\nbest_acc_model = results_df.loc[results_df['accuracy'].idxmax()]\nbest_f1_model = results_df.loc[results_df['f1_score'].idxmax()]\n\nprint(\"\\n\\n=== FINAL RESULTS ===\")\nprint(f\"Best Accuracy Model: {best_acc_model['model']} (Accuracy: {best_acc_model['accuracy']:.4f})\")\nprint(f\"Best F1 Model: {best_f1_model['model']} (F1 Score: {best_f1_model['f1_score']:.4f})\")\n\n# Generate predictions on test set using the best model\nprint(\"\\nGenerating predictions on test set...\")\nbest_model_name = best_f1_model['model']\nbest_model_type = next((m['type'] for m in models_config if m['name'] == best_model_name), None)\n\ndef predict_in_batches(model, X, batch_size=64):\n    \"\"\"Make predictions in batches to avoid memory issues\"\"\"\n    predictions = []\n    for i in range(0, len(X), batch_size):\n        batch = X[i:i+batch_size]\n        batch_pred = model.predict(batch, verbose=0)\n        predictions.append(batch_pred)\n    return np.vstack(predictions)\n\nif best_model_type == \"keras\":\n    # Load the best keras model\n    model = tf.keras.models.load_model(os.path.join(RESULTS_DIR, f\"{best_model_name}_best_model.keras\"))\n    test_predictions = predict_in_batches(model, X_test_raw)\n    test_pred_classes = np.argmax(test_predictions, axis=1)\nelif best_model_type == \"hybrid\":\n    # Load feature extractor and classifier\n    model_cfg = next(m for m in models_config if m['name'] == best_model_name)\n    feature_extractor = model_cfg[\"build_func\"](model_cfg[\"input_shape\"])\n    \n    # Extract test features in batches\n    X_test_features = []\n    batch_size = 64\n    for i in range(0, len(X_test_raw), batch_size):\n        batch = X_test_raw[i:i+batch_size]\n        features = feature_extractor.predict(batch, verbose=0)\n        X_test_features.append(features)\n    X_test_features = np.vstack(X_test_features)\n    \n    model = joblib.load(os.path.join(RESULTS_DIR, f\"{best_model_name}_model.joblib\"))\n    test_pred_classes = model.predict(X_test_features)\nelse:  # handcrafted\n    model = joblib.load(os.path.join(RESULTS_DIR, f\"{best_model_name}_model.joblib\"))\n    X_test_feats = extract_handcrafted_features(X_test_raw)\n    test_pred_classes = model.predict(X_test_feats)\n\n# Map predictions back to class names\ntest_pred_labels = [CLASSES[i] for i in test_pred_classes]\n\n# Save test predictions\ntest_predictions_df = pd.DataFrame({\n    'processed_path': test_mi['processed_path'],\n    'prediction': test_pred_labels\n})\ntest_predictions_df.to_csv(os.path.join(RESULTS_DIR, 'test_predictions.csv'), index=False)\n\nprint(\"\\nTest predictions saved to:\", os.path.join(RESULTS_DIR, 'test_predictions.csv'))\nprint(\"Detailed results saved to:\", RESULTS_DIR)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-26T06:46:30.881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import f1_score\nimport tensorflow as tf\n\n# Function to compute best threshold for a model\ndef find_best_threshold(model, val_x, y_true_bin):\n    probs = model.predict(val_x)[:, 1]  # Probabilities for class \"Right\"\n    thresholds = np.linspace(0.1, 0.9, 81)\n    best_f1 = 0\n    best_threshold = 0.5\n    for t in thresholds:\n        preds = (probs > t).astype(int)\n        f1 = f1_score(y_true_bin, preds)\n        if f1 > best_f1:\n            best_f1 = f1\n            best_threshold = t\n    return best_threshold, best_f1\n\n# Configuration for new preprocessed data\nPREPROCESSED_DATASET_ROOT = '/kaggle/input/preprocessed-mtc-aic'\nPREPROCESSED_BASE = os.path.join(PREPROCESSED_DATASET_ROOT, 'mtc-aic3_dataset_preprocessed')\nVAL_CSV_PATH = os.path.join(PREPROCESSED_BASE, 'validation.csv')\n\n# Load validation metadata\nval_df = pd.read_csv(VAL_CSV_PATH)\n\n# Filter for MI tasks only\nmi_val_df = val_df[val_df['task'] == 'MI']\n\n# Prepare data storage\nX_list = []\ny_list = []\n\n# Load and prepare each MI trial from preprocessed files\nfor _, row in mi_val_df.iterrows():\n    # Extract and clean the processed path\n    raw_path = row['processed_path']\n    \n    # Remove leading './' if present\n    if raw_path.startswith('./'):\n        raw_path = raw_path[2:]\n    \n    # Construct full path to preprocessed trial data\n    trial_path = os.path.join(PREPROCESSED_DATASET_ROOT, raw_path)\n    \n    # Load trial data (shape: [timesteps, channels])\n    trial_data = np.load(trial_path)['data']\n    \n    # FIX: Select only C3 and C4 channels (indices 1 and 3)\n    # Channel order: ['FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8']\n    trial_subset = trial_data[:, [1, 3]]  # Extract ONLY C3 and C4\n    \n    # Reshape to [1, timesteps, channels] and add to list\n    X_list.append(trial_subset[np.newaxis, ...])\n    y_list.append(row['label'])\n\n# Combine all trials\nX_val_all = np.concatenate(X_list, axis=0)  # [n_trials, 2250, 2] - MATCHES MODEL INPUT\ny_val = np.array(y_list)\n\n# Convert labels to binary: \"Right\" = 1, \"Left\" = 0\ny_val_bin = (y_val == \"Right\").astype(int)\n\n# Model directories to evaluate\nmodel_dirs = [\n    \"/kaggle/working/models3\",\n    \"/kaggle/working/models\",\n    \"/kaggle/working/models2\"\n]\n\n# Evaluate all models\nresults = {}\nfor model_dir in model_dirs:\n    if not os.path.exists(model_dir):\n        print(f\"Directory not found: {model_dir}\")\n        continue\n        \n    for model_file in os.listdir(model_dir):\n        if model_file.endswith(\".h5\"):\n            model_path = os.path.join(model_dir, model_file)\n            try:\n                print(f\"Loading model: {model_path}\")\n                model = tf.keras.models.load_model(model_path, compile=False)\n                print(f\"Evaluating model: {model_path}\")\n                threshold, f1 = find_best_threshold(model, X_val_all, y_val_bin)\n                results[model_path] = (threshold, f1)\n                print(f\"Completed: {model_path} → threshold={threshold:.3f}, F1={f1:.4f}\")\n            except Exception as e:\n                results[model_path] = f\"Error: {e}\"\n                print(f\"Error with {model_path}: {e}\")\n\n# Print evaluation results\nprint(\"\\nFinal Results:\")\nfor model_path, result in results.items():\n    if isinstance(result, tuple):\n        threshold, f1 = result\n        print(f\"{model_path} → Best threshold = {threshold:.3f}, F1 score = {f1:.4f}\")\n    else:\n        print(f\"{model_path} → {result}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-26T06:46:30.881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport random\n\n# Configuration\nPREPROCESSED_DATASET_ROOT = '/kaggle/input/preprocessed-mtc-aic'\nPREPROCESSED_BASE = os.path.join(PREPROCESSED_DATASET_ROOT, 'mtc-aic3_dataset_preprocessed')\nMODEL_PATH = '/kaggle/working/best/best_simple_raw.h5'  # Your best MI model\nTEST_CSV_PATH = os.path.join(PREPROCESSED_BASE, 'test.csv')\nSAMPLE_SUBMISSION_PATH = os.path.join(PREPROCESSED_BASE, 'sample_submission.csv')\nOUTPUT_PATH = '/kaggle/working/submission.csv'\n\n# Load the trained model\nmodel = tf.keras.models.load_model(MODEL_PATH, compile=False)\nprint(f\"Loaded model: {MODEL_PATH}\")\n\n# Load test metadata\ntest_df = pd.read_csv(TEST_CSV_PATH)\nprint(f\"Loaded test metadata with {len(test_df)} trials\")\n\n# Prepare to collect predictions\npredictions = []\n\n# Process each test trial\nfor _, row in test_df.iterrows():\n    trial_id = row['id']\n    \n    if row['task'] == 'MI':\n        # Load and preprocess MI trial data\n        raw_path = row['processed_path']\n        if raw_path.startswith('./'):\n            raw_path = raw_path[2:]\n        trial_path = os.path.join(PREPROCESSED_DATASET_ROOT, raw_path)\n        \n        # Load EEG data and select C3/C4 channels\n        trial_data = np.load(trial_path)['data']\n        trial_subset = trial_data[:, [1, 3]]  # C3 and C4 only\n        \n        # Prepare for model input\n        X = trial_subset[np.newaxis, ...].astype('float32')  # Shape: (1, 2250, 2)\n        \n        # Make prediction\n        prob = model.predict(X, verbose=0)[0][1]  # Probability of \"Right\"\n        label = \"Right\" if prob > 0.520 else \"Left\"  # Using 0.5 threshold\n    \n    else:  # SSVEP task\n        # Randomly select a label\n        label = random.choice([\"Left\", \"Right\", \"Forward\", \"Backward\"])\n    \n    predictions.append((trial_id, label))\n    if len(predictions) % 20 == 0:\n        print(f\"Processed {len(predictions)}/{len(test_df)} trials\")\n\n# Create submission DataFrame\nsubmission_df = pd.DataFrame(predictions, columns=['id', 'label'])\n\n# Ensure correct ordering by id\nsubmission_df = submission_df.sort_values('id')\n\n# Save to CSV\nsubmission_df.to_csv(OUTPUT_PATH, index=False)\nprint(f\"Submission saved to {OUTPUT_PATH}\")\nprint(\"\\nFirst 5 predictions:\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T18:47:10.140913Z","iopub.execute_input":"2025-06-20T18:47:10.141502Z","iopub.status.idle":"2025-06-20T18:47:14.124143Z","shell.execute_reply.started":"2025-06-20T18:47:10.141476Z","shell.execute_reply":"2025-06-20T18:47:14.123523Z"}},"outputs":[{"name":"stdout","text":"Loaded model: /kaggle/working/best/best_simple_raw.h5\nLoaded test metadata with 100 trials\nProcessed 20/100 trials\nProcessed 40/100 trials\nProcessed 60/100 trials\nProcessed 80/100 trials\nProcessed 100/100 trials\nSubmission saved to /kaggle/working/submission.csv\n\nFirst 5 predictions:\n     id  label\n0  4901  Right\n1  4902  Right\n2  4903  Right\n3  4904  Right\n4  4905  Right\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"mkdir ./best","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T18:31:14.675451Z","iopub.execute_input":"2025-06-20T18:31:14.676046Z","iopub.status.idle":"2025-06-20T18:31:14.890480Z","shell.execute_reply.started":"2025-06-20T18:31:14.676023Z","shell.execute_reply":"2025-06-20T18:31:14.889451Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"cp /kaggle/working/models/best_simple_raw.h5 /kaggle/working/best","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T18:31:50.891960Z","iopub.execute_input":"2025-06-20T18:31:50.892681Z","iopub.status.idle":"2025-06-20T18:31:51.100097Z","shell.execute_reply.started":"2025-06-20T18:31:50.892652Z","shell.execute_reply":"2025-06-20T18:31:51.099250Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}