{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":98188,"databundleVersionId":12673416,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## training SVM, LDA, RF","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy import signal\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import make_pipeline\nfrom mne.decoding import CSP\n\ndef detect_contralateral_pattern(trial_data, label, channels=['C3', 'C4'], fs=250):\n    \"\"\"\n    Detect contralateral ERD pattern in motor imagery data\n    Returns True if the expected pattern is present, False otherwise\n    \"\"\"\n    # Bandpass filter (alpha/beta rhythms)\n    b, a = signal.butter(4, [8, 30], btype='bandpass', fs=fs)\n    filtered_data = signal.filtfilt(b, a, trial_data[channels], axis=0)\n    \n    # Calculate band power (variance)\n    c3_power = np.var(filtered_data[:, 0])\n    c4_power = np.var(filtered_data[:, 1])\n    \n    # Check for expected contralateral pattern\n    if label == 'Left':\n        return c4_power < c3_power * 0.9  # At least 10% decrease contralaterally\n    elif label == 'Right':\n        return c3_power < c4_power * 0.9\n    return False\n\ndef extract_time_window(trial_data, start_time, end_time, fs=250):\n    \"\"\"Extract specific time window from trial data\"\"\"\n    start_idx = int(start_time * fs)\n    end_idx = int(end_time * fs)\n    return trial_data.iloc[start_idx:end_idx]\n\n# Configuration\nWINDOW = (5.75, 6.75)  # Target time window\nFS = 250  # Sampling rate\nCHANNELS = ['FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8']  # EEG channels\n\n# Load data index files\nbase_path = '/kaggle/input/mtcaic3/'\ntrain_df = pd.read_csv(base_path + 'train.csv')\nvalid_df = pd.read_csv(base_path + 'validation.csv')\n\n# Filter for MI tasks only\ntrain_mi = train_df[train_df['task'] == 'MI'].copy()\nvalid_mi = valid_df[valid_df['task'] == 'MI'].copy()\n\n# Function to load EEG data (from competition instructions)\ndef load_trial_data(row):\n    id_num = row['id']\n    if id_num <= 4800:\n        dataset = 'train'\n    elif id_num <= 4900:\n        dataset = 'validation'\n    else:\n        dataset = 'test'\n\n    # Construct the path to EEGdata.csv\n    eeg_path = f\"{base_path}/{row['task']}/{dataset}/{row['subject_id']}/{row['trial_session']}/EEGdata.csv\"\n\n    # Load the entire EEG file\n    eeg_data = pd.read_csv(eeg_path)\n\n    # Calculate indices for the specific trial\n    trial_num = int(row['trial'])\n    if row['task'] == 'MI':\n        samples_per_trial = 2250  # 9 seconds * 250 Hz\n    else:  # SSVEP\n        samples_per_trial = 1750  # 7 seconds * 250 Hz\n\n    start_idx = (trial_num - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial - 1\n\n    # Extract the trial data\n    trial_data = eeg_data.iloc[start_idx:end_idx+1]\n    return trial_data\n\n# 1. Filter trials with contralateral pattern in target window\ndef filter_pattern_trials(df, window):\n    pattern_mask = []\n    for _, row in df.iterrows():\n        full_trial = load_trial_data(row)\n        window_data = extract_time_window(full_trial, *window)\n        if detect_contralateral_pattern(window_data[CHANNELS], row['label']):\n            pattern_mask.append(True)\n        else:\n            pattern_mask.append(False)\n    return df[pattern_mask]\n\ntrain_filtered = filter_pattern_trials(train_mi, WINDOW)\nvalid_filtered = filter_pattern_trials(valid_mi, WINDOW)\n\nprint(f\"Original MI trials: Train={len(train_mi)}, Valid={len(valid_mi)}\")\nprint(f\"Trials with contralateral pattern: Train={len(train_filtered)}, Valid={len(valid_filtered)}\")\n\n# 2. Prepare data for modeling\ndef prepare_data(df, window):\n    X, y = [], []\n    for _, row in df.iterrows():\n        full_trial = load_trial_data(row)\n        window_data = extract_time_window(full_trial, *window)\n        X.append(window_data[CHANNELS].values.T)  # Shape: (channels, time)\n        y.append(row['label'])\n    return np.array(X), np.array(y)\n\nX_train, y_train = prepare_data(train_filtered, WINDOW)\nX_valid, y_valid = prepare_data(valid_filtered, WINDOW)\n\n# 3. Train and evaluate models\nmodels = {\n    \"CSP+LDA\": make_pipeline(CSP(n_components=4), LinearDiscriminantAnalysis()),\n    \"CSP+SVM\": make_pipeline(CSP(n_components=4), SVC(kernel='linear', probability=True)),\n    \"CSP+RF\": make_pipeline(CSP(n_components=4), RandomForestClassifier(n_estimators=100))\n}\n\nfor name, model in models.items():\n    print(f\"\\n=== Training {name} ===\")\n    \n    # Train model\n    model.fit(X_train, y_train)\n    \n    # Get predictions and confidences\n    if hasattr(model, 'predict_proba'):\n        probas = model.predict_proba(X_valid)\n        confidences = np.max(probas, axis=1)\n        predictions = model.classes_[np.argmax(probas, axis=1)]\n    else:  # For models without predict_proba\n        predictions = model.predict(X_valid)\n        decision = model.decision_function(X_valid)\n        confidences = 1 / (1 + np.exp(-np.abs(decision)))\n    \n    # Print classification report\n    print(classification_report(y_valid, predictions))\n    \n    # Print predictions with confidence\n    results = pd.DataFrame({\n        'True Label': y_valid,\n        'Predicted': predictions,\n        'Confidence': confidences\n    })\n    print(f\"\\nValidation set predictions with confidence ({name}):\")\n    print(results.head(10))\n    print(\"...\\n\")\n\nprint(\"Analysis complete. Focused on 5.75-6.75s window with contralateral pattern trials.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EEGNET","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F # Import functional for ELU\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom mne.decoding import CSP\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport copy # For deep copying model weights\n\n# EEGNet Architecture (PyTorch Implementation) - MODIFIED\nclass EEGNet(nn.Module):\n    def __init__(self, num_channels=4, num_classes=2, sampling_rate=250, dropout_rate=0.5): # Increased dropout\n        super(EEGNet, self).__init__()\n        self.T = int((6.75 - 5.75) * sampling_rate)\n        \n        # Layer 1\n        self.conv1 = nn.Conv2d(1, 16, (1, sampling_rate//2), padding=(0, sampling_rate//4), bias=False)\n        self.batchnorm1 = nn.BatchNorm2d(16)\n        \n        # Layer 2\n        self.conv2 = nn.Conv2d(16, 32, (num_channels, 1), groups=16, bias=False)\n        self.batchnorm2 = nn.BatchNorm2d(32)\n        self.pooling2 = nn.AvgPool2d((1, 4))\n        self.dropout2 = nn.Dropout(dropout_rate) # Using parameter\n        \n        # Layer 3\n        self.conv3 = nn.Conv2d(32, 32, (1, 16), padding=(0, 8), groups=32, bias=False)\n        self.conv3_1 = nn.Conv2d(32, 32, 1, bias=False)\n        self.batchnorm3 = nn.BatchNorm2d(32)\n        self.pooling3 = nn.AvgPool2d((1, 8))\n        self.dropout3 = nn.Dropout(dropout_rate) # Using parameter\n        \n        # FC Layer - Made robust with a helper function\n        fc_input_size = self._get_fc_input_size((1, num_channels, self.T))\n        self.fc = nn.Linear(fc_input_size, num_classes)\n        \n    def _get_fc_input_size(self, shape):\n        # Helper to dynamically calculate the flattened size for the FC layer\n        with torch.no_grad():\n            x = torch.rand(1, *shape)\n            x = self.forward_features(x)\n            return x.view(1, -1).size(1)\n\n    def forward_features(self, x):\n        # Feature extraction part of the forward pass\n        x = F.elu(self.batchnorm1(self.conv1(x))) # Using ELU activation\n        x = F.elu(self.batchnorm2(self.conv2(x)))\n        x = self.pooling2(x)\n        x = self.dropout2(x)\n        \n        x = F.elu(self.batchnorm3(self.conv3_1(self.conv3(x))))\n        x = self.pooling3(x)\n        x = self.dropout3(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# --- 1. OVERFITTING FIX: Data Augmentation ---\ndef augment_data(data, noise_level=0.01):\n    \"\"\"Adds Gaussian noise to the data.\"\"\"\n    noise = torch.randn_like(data) * noise_level\n    return data + noise\n\n# --- Assume X_train, y_train, X_valid, y_valid are loaded and prepared ---\n# Example placeholder data if you run this standalone\n# X_train = np.random.randn(1121, 8, 250)\n# y_train = np.random.choice(['Left', 'Right'], 1121, p=[0.7, 0.3]) # Imbalanced example\n# X_valid = np.random.randn(24, 8, 250)\n# y_valid = np.random.choice(['Left', 'Right'], 24)\n\n# Apply CSP with TIME-SERIES output\ncsp = CSP(n_components=4, transform_into='csp_space')\nX_train_csp = csp.fit_transform(X_train, y_train)\nX_valid_csp = csp.transform(X_valid)\n\n# Reshape data for EEGNet\nX_train_dl = X_train_csp[:, np.newaxis, :, :]\nX_valid_dl = X_valid_csp[:, np.newaxis, :, :]\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Normalize each channel independently\n# A better approach is to fit on train set and transform valid set to avoid data leakage\n# But we will keep your original method for now to focus on the key changes\ndef normalize_channels(data):\n    scaler = StandardScaler()\n    normalized = np.zeros_like(data)\n    for i in range(data.shape[0]):\n        for j in range(data.shape[2]):\n            channel_data = data[i, 0, j, :]\n            normalized[i, 0, j, :] = scaler.fit_transform(channel_data.reshape(-1, 1)).flatten()\n    return normalized\n\nX_train_norm = normalize_channels(X_train_dl)\nX_valid_norm = normalize_channels(X_valid_dl)\n\n# Convert to PyTorch tensors\ntrain_tensor = torch.tensor(X_train_norm, dtype=torch.float32)\nvalid_tensor = torch.tensor(X_valid_norm, dtype=torch.float32)\ntrain_labels_numerical = np.array([1 if label == 'Right' else 0 for label in y_train])\nvalid_labels_numerical = np.array([1 if label == 'Right' else 0 for label in y_valid])\n\ntrain_labels = torch.tensor(train_labels_numerical, dtype=torch.long)\nvalid_labels = torch.tensor(valid_labels_numerical, dtype=torch.long)\n\n# Create DataLoaders\ntrain_dataset = TensorDataset(train_tensor, train_labels)\nvalid_dataset = TensorDataset(valid_tensor, valid_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=32)\n\n# --- 2. CLASS IMBALANCE FIX: Calculate class weights ---\nclass_weights = compute_class_weight('balanced', classes=np.unique(train_labels_numerical), y=train_labels_numerical)\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\nprint(f\"Calculated class weights: {class_weights_tensor.cpu().numpy()}\")\n\n# Initialize model, loss, and optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = EEGNet(num_channels=4, num_classes=2, sampling_rate=250, dropout_rate=0.5).to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights_tensor) # Use weighted loss\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4) # Slightly increased weight decay\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n\n# Training loop with Early Stopping\ndef train_model(model, train_loader, valid_loader, epochs=100):\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': [], 'epoch_time': []}\n    \n    # --- 3. OVERFITTING FIX: Early Stopping parameters ---\n    best_val_loss = float('inf')\n    patience = 20  # Stop after 10 epochs of no improvement\n    patience_counter = 0\n    best_model_wts = None\n\n    for epoch in range(epochs):\n        epoch_start = time.time()\n        \n        # Training phase\n        model.train()\n        running_loss, correct_train, total_train = 0.0, 0, 0\n        train_iter = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]', leave=False)\n        for inputs, labels in train_iter:\n            # Apply augmentation\n            inputs = augment_data(inputs, noise_level=0.02).to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            total_train += labels.size(0)\n            correct_train += (predicted == labels).sum().item()\n            train_iter.set_postfix({'loss': running_loss/total_train, 'acc': correct_train/total_train})\n\n        epoch_train_loss = running_loss / len(train_loader.dataset)\n        epoch_train_acc = correct_train / total_train\n        history['train_loss'].append(epoch_train_loss)\n        history['train_acc'].append(epoch_train_acc)\n        \n        # Validation phase\n        model.eval()\n        val_loss, correct_val, total_val = 0.0, 0, 0\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            val_iter = tqdm(valid_loader, desc=f'Epoch {epoch+1}/{epochs} [Valid]', leave=False)\n            for inputs, labels in val_iter:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item() * inputs.size(0)\n                _, predicted = torch.max(outputs, 1)\n                total_val += labels.size(0)\n                correct_val += (predicted == labels).sum().item()\n                all_preds.extend(predicted.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                val_iter.set_postfix({'loss': val_loss/total_val, 'acc': correct_val/total_val})\n\n        epoch_val_loss = val_loss / len(valid_loader.dataset)\n        epoch_val_acc = correct_val / total_val\n        history['val_loss'].append(epoch_val_loss)\n        history['val_acc'].append(epoch_val_acc)\n        \n        current_lr = optimizer.param_groups[0]['lr']\n        history['lr'].append(current_lr)\n        scheduler.step(epoch_val_acc)\n        epoch_time = time.time() - epoch_start\n        history['epoch_time'].append(epoch_time)\n        \n        print(f'\\nEpoch {epoch+1}/{epochs} Summary:')\n        print(f'  Time: {epoch_time:.2f}s | LR: {current_lr:.6f}')\n        print(f'  Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f}')\n        print(f'  Val Loss:   {epoch_val_loss:.4f} | Val Acc:   {epoch_val_acc:.4f}')\n        \n        # Early Stopping Logic\n        if epoch_val_loss < best_val_loss:\n            best_val_loss = epoch_val_loss\n            patience_counter = 0\n            best_model_wts = copy.deepcopy(model.state_dict())\n            print(f\"  Validation loss improved. Saving best model.\")\n        else:\n            patience_counter += 1\n            print(f\"  Validation loss did not improve. Patience: {patience_counter}/{patience}\")\n\n        if patience_counter >= patience:\n            print(\"\\nEarly stopping triggered.\")\n            break\n            \n    # Load best model weights\n    if best_model_wts:\n        model.load_state_dict(best_model_wts)\n        \n    return all_preds, all_labels, history\n\n# ... (rest of the script for reporting and plotting remains the same) ...\n# Train the model\nprint(\"Training EEGNet model...\")\npreds, true_labels, history = train_model(model, train_loader, valid_loader, epochs=30)\n\n# Convert numerical labels back to string\nlabel_map = {0: 'Left', 1: 'Right'}\nstr_preds = [label_map[p] for p in preds]\nstr_true = [label_map[l] for l in true_labels]\n\n# Print final classification report\nprint(\"\\nFinal Classification Report:\")\nprint(classification_report(str_true, str_preds))\n\n# Print predictions with confidence\nmodel.eval()\nconfidences = []\nwith torch.no_grad():\n    for inputs, _ in valid_loader:\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n        probs = torch.softmax(outputs, dim=1)\n        conf, _ = torch.max(probs, dim=1)\n        confidences.extend(conf.cpu().numpy())\n\nresults = pd.DataFrame({\n    'True Label': str_true,\n    'Predicted': str_preds,\n    'Confidence': confidences\n})\nprint(\"\\nValidation set predictions with confidence (CSP + EEGNet):\")\nprint(results)\n\n# Save model\ntorch.save(model.state_dict(), 'csp_eegnet_model.pth')\nprint(\"Model saved to 'csp_eegnet_model.pth'\")\n\n# Print training summary\nprint(\"\\nTraining Summary:\")\nprint(f\"Best Validation Accuracy: {max(history['val_acc']):.4f}\")\nprint(f\"Total Training Time: {sum(history['epoch_time']):.2f} seconds\")\nprint(f\"Average Epoch Time: {np.mean(history['epoch_time']):.2f} seconds\")\nprint(f\"Final Learning Rate: {history['lr'][-1]:.6f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom mne.decoding import CSP\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport copy\n\n# EEGNet Architecture (PyTorch Implementation) - MODIFIED\nclass EEGNet(nn.Module):\n    def __init__(self, num_channels=4, num_classes=2, sampling_rate=250, dropout_rate=0.6): # Increased dropout\n        super(EEGNet, self).__init__()\n        self.T = int((6.75 - 5.75) * sampling_rate)\n\n        # Layer 1\n        self.conv1 = nn.Conv2d(1, 16, (1, sampling_rate // 2), padding=(0, sampling_rate // 4), bias=False)\n        self.batchnorm1 = nn.BatchNorm2d(16)\n\n        # Layer 2\n        self.conv2 = nn.Conv2d(16, 32, (num_channels, 1), groups=16, bias=False)\n        self.batchnorm2 = nn.BatchNorm2d(32)\n        self.pooling2 = nn.AvgPool2d((1, 4))\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        # Layer 3\n        self.conv3 = nn.Conv2d(32, 32, (1, 16), padding=(0, 8), groups=32, bias=False)\n        self.conv3_1 = nn.Conv2d(32, 32, 1, bias=False)\n        self.batchnorm3 = nn.BatchNorm2d(32)\n        self.pooling3 = nn.AvgPool2d((1, 8))\n        self.dropout3 = nn.Dropout(dropout_rate)\n\n        # FC Layer\n        fc_input_size = self._get_fc_input_size((1, num_channels, self.T))\n        self.fc = nn.Linear(fc_input_size, num_classes)\n        self.fc_dropout = nn.Dropout(0.7) # Higher dropout for the final classifier\n\n    def _get_fc_input_size(self, shape):\n        with torch.no_grad():\n            x = torch.rand(1, *shape)\n            x = self.forward_features(x)\n            return x.view(1, -1).size(1)\n\n    def forward_features(self, x):\n        x = F.elu(self.batchnorm1(self.conv1(x)))\n        x = F.elu(self.batchnorm2(self.conv2(x)))\n        x = self.pooling2(x)\n        x = self.dropout2(x)\n\n        x = F.elu(self.batchnorm3(self.conv3_1(self.conv3(x))))\n        x = self.pooling3(x)\n        x = self.dropout3(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc_dropout(x)\n        x = self.fc(x)\n        return x\n\n# --- 1. CLASS-WISE Data Augmentation ---\ndef augment_data_classwise(data, labels, minority_class=1, noise_level=0.02, shift_max=10):\n    \"\"\"\n    Applies augmentation more aggressively to the minority class.\n    `minority_class` is the numerical label of the minority class.\n    \"\"\"\n    augmented_data = data.clone()\n    for i in range(data.size(0)):\n        if labels[i] == minority_class:\n            # Augment minority class\n            # Add Gaussian Noise\n            noise = torch.randn_like(data[i]) * noise_level\n            augmented_data[i] += noise\n\n            # Time Shift\n            shift = np.random.randint(-shift_max, shift_max)\n            augmented_data[i] = torch.roll(augmented_data[i], shifts=shift, dims=-1)\n\n    return augmented_data\n\n# --- Assume X_train, y_train, X_valid, y_valid are loaded and prepared ---\n# Example placeholder data if you run this standalone\n# X_train = np.random.randn(1121, 8, 250)\n# y_train = np.random.choice(['Left', 'Right'], 1121, p=[0.7, 0.3]) # Imbalanced example\n# X_valid = np.random.randn(24, 8, 250)\n# y_valid = np.random.choice(['Left', 'Right'], 24)\n\n\n# Apply CSP with TIME-SERIES output\ncsp = CSP(n_components=4, transform_into='csp_space')\nX_train_csp = csp.fit_transform(X_train, y_train)\nX_valid_csp = csp.transform(X_valid)\n\n# Reshape data for EEGNet\nX_train_dl = X_train_csp[:, np.newaxis, :, :]\nX_valid_dl = X_valid_csp[:, np.newaxis, :, :]\n\n# Normalize each channel independently\ndef normalize_channels(data):\n    scaler = StandardScaler()\n    normalized = np.zeros_like(data)\n    for i in range(data.shape[0]):\n        for j in range(data.shape[2]): # Iterate through CSP components\n            channel_data = data[i, 0, j, :]\n            normalized[i, 0, j, :] = scaler.fit_transform(channel_data.reshape(-1, 1)).flatten()\n    return normalized\n\nX_train_norm = normalize_channels(X_train_dl)\nX_valid_norm = normalize_channels(X_valid_dl)\n\n# Convert to PyTorch tensors\ntrain_tensor = torch.tensor(X_train_norm, dtype=torch.float32)\nvalid_tensor = torch.tensor(X_valid_norm, dtype=torch.float32)\ntrain_labels_numerical = np.array([1 if label == 'Right' else 0 for label in y_train])\nvalid_labels_numerical = np.array([1 if label == 'Right' else 0 for label in y_valid])\n\ntrain_labels = torch.tensor(train_labels_numerical, dtype=torch.long)\nvalid_labels = torch.tensor(valid_labels_numerical, dtype=torch.long)\n\n# Create DataLoaders\ntrain_dataset = TensorDataset(train_tensor, train_labels)\nvalid_dataset = TensorDataset(valid_tensor, valid_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=32)\n\n# --- 2. CLASS IMBALANCE FIX: Calculate class weights ---\nclass_weights = compute_class_weight('balanced', classes=np.unique(train_labels_numerical), y=train_labels_numerical)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\nprint(f\"Calculated class weights: {class_weights_tensor.cpu().numpy()}\")\n\n# Initialize model, loss, and optimizer\nmodel = EEGNet(num_channels=4, num_classes=2, sampling_rate=250).to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3) # Slightly increased weight decay\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=8, verbose=True) # Increased patience\n\n# Training loop with Early Stopping\ndef train_model(model, train_loader, valid_loader, epochs=100):\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': [], 'epoch_time': []}\n\n    # --- 3. OVERFITTING FIX: Early Stopping parameters ---\n    best_val_acc = 0.0\n    patience = 15  # More aggressive early stopping\n    patience_counter = 0\n    best_model_wts = None\n\n    for epoch in range(epochs):\n        epoch_start = time.time()\n\n        # Training phase\n        model.train()\n        running_loss, correct_train, total_train = 0.0, 0, 0\n        train_iter = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]', leave=False)\n        for inputs, labels in train_iter:\n            # Apply class-wise augmentation\n            inputs = augment_data_classwise(inputs, labels, minority_class=1).to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            total_train += labels.size(0)\n            correct_train += (predicted == labels).sum().item()\n            train_iter.set_postfix({'loss': running_loss/total_train, 'acc': correct_train/total_train})\n\n        epoch_train_loss = running_loss / len(train_loader.dataset)\n        epoch_train_acc = correct_train / total_train\n        history['train_loss'].append(epoch_train_loss)\n        history['train_acc'].append(epoch_train_acc)\n\n        # Validation phase\n        model.eval()\n        val_loss, correct_val, total_val = 0.0, 0, 0\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            val_iter = tqdm(valid_loader, desc=f'Epoch {epoch+1}/{epochs} [Valid]', leave=False)\n            for inputs, labels in val_iter:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item() * inputs.size(0)\n                _, predicted = torch.max(outputs, 1)\n                total_val += labels.size(0)\n                correct_val += (predicted == labels).sum().item()\n                all_preds.extend(predicted.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                val_iter.set_postfix({'loss': val_loss/total_val, 'acc': correct_val/total_val})\n\n        epoch_val_loss = val_loss / len(valid_loader.dataset)\n        epoch_val_acc = correct_val / total_val\n        history['val_loss'].append(epoch_val_loss)\n        history['val_acc'].append(epoch_val_acc)\n\n        current_lr = optimizer.param_groups[0]['lr']\n        history['lr'].append(current_lr)\n        scheduler.step(epoch_val_acc)\n        epoch_time = time.time() - epoch_start\n        history['epoch_time'].append(epoch_time)\n\n        print(f'\\nEpoch {epoch+1}/{epochs} Summary:')\n        print(f'  Time: {epoch_time:.2f}s | LR: {current_lr:.6f}')\n        print(f'  Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f}')\n        print(f'  Val Loss:   {epoch_val_loss:.4f} | Val Acc:   {epoch_val_acc:.4f}')\n\n        # Early Stopping Logic based on validation accuracy\n        if epoch_val_acc > best_val_acc:\n            best_val_acc = epoch_val_acc\n            patience_counter = 0\n            best_model_wts = copy.deepcopy(model.state_dict())\n            print(f\"  Validation accuracy improved. Saving best model.\")\n        else:\n            patience_counter += 1\n            print(f\"  Validation accuracy did not improve. Patience: {patience_counter}/{patience}\")\n\n        if patience_counter >= patience:\n            print(\"\\nEarly stopping triggered.\")\n            break\n\n    # Load best model weights\n    if best_model_wts:\n        model.load_state_dict(best_model_wts)\n\n    return all_preds, all_labels, history\n\n# Train the model\nprint(\"Training EEGNet model with class-wise augmentation and enhanced regularization...\")\npreds, true_labels, history = train_model(model, train_loader, valid_loader, epochs=100)\n\n# Convert numerical labels back to string\nlabel_map = {0: 'Left', 1: 'Right'}\nstr_preds = [label_map[p] for p in preds]\nstr_true = [label_map[l] for l in true_labels]\n\n# Print final classification report\nprint(\"\\nFinal Classification Report:\")\nprint(classification_report(str_true, str_preds))\n\n# Print predictions with confidence\nmodel.eval()\nconfidences = []\nwith torch.no_grad():\n    for inputs, _ in valid_loader:\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n        probs = torch.softmax(outputs, dim=1)\n        conf, _ = torch.max(probs, dim=1)\n        confidences.extend(conf.cpu().numpy())\n\nresults = pd.DataFrame({\n    'True Label': str_true,\n    'Predicted': str_preds,\n    'Confidence': confidences\n})\nprint(\"\\nValidation set predictions with confidence (CSP + EEGNet):\")\nprint(results)\n\n# Save model\ntorch.save(model.state_dict(), 'csp_eegnet_model_v2.pth')\nprint(\"Model saved to 'csp_eegnet_model_v2.pth'\")\n\n# Print training summary\nprint(\"\\nTraining Summary:\")\nif history['val_acc']:\n    print(f\"Best Validation Accuracy: {max(history['val_acc']):.4f}\")\nprint(f\"Total Training Time: {sum(history['epoch_time']):.2f} seconds\")\nif history['epoch_time']:\n    print(f\"Average Epoch Time: {np.mean(history['epoch_time']):.2f} seconds\")\nif history['lr']:\n    print(f\"Final Learning Rate: {history['lr'][-1]:.6f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom scipy.signal import butter, lfilter\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom collections import Counter\n\n# --- 1. Configuration ---\n# Define the base path for the dataset on Kaggle\nBASE_PATH = '/kaggle/input/mtcaic3'\n# Target time window in seconds\nWINDOW = (5.75, 6.75)\n# EEG Sampling Frequency\nFS = 250\n# Subset of channels relevant to motor imagery\n# FZ, C3, CZ, C4, PZ, PO7, OZ, PO8 are the channels\n# C3, C4, Cz are most relevant for motor cortex activity\nCHANNELS = ['C3', 'CZ', 'C4', 'PZ']\n# Calculate number of samples in the window\nSTART_SAMP = int(WINDOW[0] * FS)\nEND_SAMP = int(WINDOW[1] * FS)\nN_SAMPLES_WINDOW = END_SAMP - START_SAMP\n\n# --- 2. Data Loading and Preprocessing Functions ---\n\ndef load_trial_data(row, base_path, task_folder):\n    \"\"\"Loads a trial's full EEG data from the correct folder (train/validation/test).\"\"\"\n    # Construct the path to the specific EEGdata.csv file\n    eeg_path = os.path.join(base_path, row['task'], task_folder, row['subject_id'], str(row['trial_session']), 'EEGdata.csv')\n    eeg_data = pd.read_csv(eeg_path)\n    \n    # Calculate indices for the specific trial\n    trial_num = int(row['trial'])\n    samples_per_trial = 2250 if row['task'] == 'MI' else 1750\n        \n    start_idx = (trial_num - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial\n    \n    # Extract the trial data\n    trial_data = eeg_data.iloc[start_idx:end_idx]\n    return trial_data\n\ndef butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n    \"\"\"Applies a bandpass filter to the data.\"\"\"\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    y = lfilter(b, a, data, axis=0)\n    return y\n\n# --- 3. Main Data Loading and Filtering Loop ---\n\nprint(\"Loading and preprocessing training data...\")\n# Load the index file\ntrain_df = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\n\nX_raw = []\ny_raw = []\n\n# Process only Motor Imagery (MI) tasks from the training set\nmi_train_df = train_df[train_df['task'] == 'MI'].reset_index(drop=True)\n\nfor _, row in mi_train_df.iterrows():\n    # Load the full data for the trial\n    trial_full_data = load_trial_data(row, BASE_PATH, 'train')\n    \n    # Extract the specified time window\n    trial_window_data = trial_full_data.iloc[START_SAMP:END_SAMP]\n    \n    # Select only the relevant channels\n    X_trial = trial_window_data[CHANNELS].values\n    \n    # Apply bandpass filter (mu and beta rhythms for MI are typically 8-30 Hz)\n    X_trial_filtered = butter_bandpass_filter(X_trial, lowcut=8.0, highcut=30.0, fs=FS)\n    \n    X_raw.append(X_trial_filtered)\n    # Encode labels: 0 for Left, 1 for Right\n    y_raw.append(0 if row['label'] == 'Left' else 1)\n\nX_raw = np.array(X_raw)\ny_raw = np.array(y_raw)\n\nprint(f\"Initial raw data shape: {X_raw.shape}\")\nprint(f\"Initial raw label distribution: {Counter(y_raw)}\")\n\n# --- 4. Contralateral Pattern Filtering ---\nprint(\"\\nFiltering trials based on contralateral activity...\")\nX_filtered = []\ny_filtered = []\n\n# Channel indices in our CHANNELS list: C3 is 0, C4 is 2\nc3_idx, c4_idx = 0, 2 \n\nfor i in range(X_raw.shape[0]):\n    trial_data = X_raw[i]\n    label = y_raw[i]\n    \n    # Calculate power (mean of squared amplitudes)\n    c3_power = np.mean(trial_data[:, c3_idx]**2)\n    c4_power = np.mean(trial_data[:, c4_idx]**2)\n    \n    # Keep trials with expected contralateral activity:\n    # Imagined Right Hand (label 1) -> Higher power in Left Hemisphere (C3)\n    # Imagined Left Hand (label 0) -> Higher power in Right Hemisphere (C4)\n    if (label == 1 and c3_power > c4_power) or \\\n       (label == 0 and c4_power > c3_power):\n        X_filtered.append(trial_data)\n        y_filtered.append(label)\n\nX_filtered = np.array(X_filtered)\ny_filtered = np.array(y_filtered)\n\nprint(f\"Shape after filtering: {X_filtered.shape}\")\nprint(f\"Label distribution after filtering: {Counter(y_filtered)}\")\n\n# --- 5. Class-wise Augmentation for Imbalance ---\nprint(\"\\nHandling class imbalance with augmentation...\")\n\ndef augment_minority_class(X, y):\n    \"\"\"Augments the minority class by adding scaled Gaussian noise.\"\"\"\n    class_counts = Counter(y)\n    if not class_counts or len(class_counts) < 2:\n        print(\"Not enough classes to augment.\")\n        return X, y\n\n    minority_class = min(class_counts, key=class_counts.get)\n    majority_class = max(class_counts, key=class_counts.get)\n    \n    n_majority = class_counts[majority_class]\n    n_minority = class_counts[minority_class]\n\n    if n_majority == n_minority:\n        print(\"Classes are already balanced.\")\n        return X, y\n\n    n_to_generate = n_majority - n_minority\n    print(f\"Balancing classes. Augmenting class '{minority_class}' with {n_to_generate} new samples.\")\n    \n    minority_indices = np.where(y == minority_class)[0]\n    \n    # Generate new samples from the minority class\n    random_indices = np.random.choice(minority_indices, size=n_to_generate, replace=True)\n    \n    X_minority_original = X[random_indices]\n    \n    # Add scaled Gaussian noise\n    noise_factor = 0.1 \n    noise = np.random.normal(0, np.std(X_minority_original) * noise_factor, X_minority_original.shape)\n    X_augmented = X_minority_original + noise\n    \n    # Append augmented data to the original dataset\n    X_balanced = np.concatenate((X, X_augmented), axis=0)\n    y_augmented = np.full(n_to_generate, minority_class)\n    y_balanced = np.concatenate((y, y_augmented), axis=0)\n    \n    return X_balanced, y_balanced\n\nX_balanced, y_balanced = augment_minority_class(X_filtered, y_filtered)\n\nprint(f\"Shape after augmentation: {X_balanced.shape}\")\nprint(f\"Label distribution after augmentation: {Counter(y_balanced)}\")\n\n\n# --- 6. Final Preparation and Model Training ---\nprint(\"\\nPreparing data for the model...\")\n\n# Split data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(\n    X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced\n)\n\n# One-hot encode the labels for the neural network\ny_train_cat = to_categorical(y_train, num_classes=2)\ny_val_cat = to_categorical(y_val, num_classes=2)\n\n# Define the input shape for the model\ninput_shape = (X_train.shape[1], X_train.shape[2]) # (n_samples_window, n_channels)\n\n# Define the deep learning model\nmodel = Sequential([\n    # Input Layer\n    Conv1D(filters=32, kernel_size=10, activation='relu', padding='same', input_shape=input_shape),\n    BatchNormalization(),\n    \n    # Spatial Filtering Layer\n    Conv1D(filters=64, kernel_size=3, activation='relu'),\n    BatchNormalization(),\n    MaxPooling1D(pool_size=2),\n    Dropout(0.5),\n\n    # Temporal Feature Extraction Layer\n    Conv1D(filters=128, kernel_size=3, activation='relu'),\n    BatchNormalization(),\n    MaxPooling1D(pool_size=2),\n    Dropout(0.5),\n    \n    # Classifier Head\n    Flatten(),\n    Dense(100, activation='relu'),\n    Dropout(0.5),\n    Dense(2, activation='softmax') # 2 classes: Left, Right\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n# Define callbacks for robust training\nearly_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, min_lr=0.00001)\n\nprint(\"\\nStarting model training...\")\n# Train the model\nhistory = model.fit(\n    X_train, y_train_cat,\n    epochs=100,\n    batch_size=32,\n    validation_data=(X_val, y_val_cat),\n    callbacks=[early_stopping, reduce_lr],\n    verbose=2\n)\n\nprint(\"\\nTraining finished.\")\n# Evaluate final model on validation set\nloss, accuracy = model.evaluate(X_val, y_val_cat, verbose=0)\nprint(f\"\\nFinal Validation Accuracy: {accuracy*100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom scipy.signal import butter, lfilter\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom collections import Counter\n\n# --- Import for evaluation ---\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# --- 1. Configuration ---\n# Define the base path for the dataset on Kaggle\nBASE_PATH = '/kaggle/input/mtcaic3'\n# Target time window in seconds\nWINDOW = (5.75, 6.75)\n# EEG Sampling Frequency\nFS = 250\n# Subset of channels relevant to motor imagery\nCHANNELS = ['C3', 'CZ', 'C4', 'PZ']\n# Calculate number of samples in the window\nSTART_SAMP = int(WINDOW[0] * FS)\nEND_SAMP = int(WINDOW[1] * FS)\nN_SAMPLES_WINDOW = END_SAMP - START_SAMP\n\n# --- 2. Data Loading and Preprocessing Functions ---\n\ndef load_trial_data(row, base_path, task_folder):\n    \"\"\"Loads a trial's full EEG data from the correct folder (train/validation/test).\"\"\"\n    eeg_path = os.path.join(base_path, row['task'], task_folder, row['subject_id'], str(row['trial_session']), 'EEGdata.csv')\n    eeg_data = pd.read_csv(eeg_path)\n    \n    trial_num = int(row['trial'])\n    samples_per_trial = 2250 if row['task'] == 'MI' else 1750\n        \n    start_idx = (trial_num - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial\n    \n    trial_data = eeg_data.iloc[start_idx:end_idx]\n    return trial_data\n\ndef butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n    \"\"\"Applies a bandpass filter to the data.\"\"\"\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    y = lfilter(b, a, data, axis=0)\n    return y\n\n# --- 3. Main Data Loading and Filtering Loop ---\n\nprint(\"Loading and preprocessing training data...\")\ntrain_df = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\nX_raw = []\ny_raw = []\nmi_train_df = train_df[train_df['task'] == 'MI'].reset_index(drop=True)\n\nfor _, row in mi_train_df.iterrows():\n    trial_full_data = load_trial_data(row, BASE_PATH, 'train')\n    trial_window_data = trial_full_data.iloc[START_SAMP:END_SAMP]\n    X_trial = trial_window_data[CHANNELS].values\n    X_trial_filtered = butter_bandpass_filter(X_trial, lowcut=8.0, highcut=30.0, fs=FS)\n    X_raw.append(X_trial_filtered)\n    y_raw.append(0 if row['label'] == 'Left' else 1)\n\nX_raw = np.array(X_raw)\ny_raw = np.array(y_raw)\n\n# --- 4. Contralateral Pattern Filtering ---\nprint(\"\\nFiltering trials based on contralateral activity...\")\nX_filtered = []\ny_filtered = []\nc3_idx, c4_idx = 0, 2 \n\nfor i in range(X_raw.shape[0]):\n    trial_data = X_raw[i]\n    label = y_raw[i]\n    c3_power = np.mean(trial_data[:, c3_idx]**2)\n    c4_power = np.mean(trial_data[:, c4_idx]**2)\n    \n    if (label == 1 and c3_power > c4_power) or \\\n       (label == 0 and c4_power > c3_power):\n        X_filtered.append(trial_data)\n        y_filtered.append(label)\n\nX_filtered = np.array(X_filtered)\ny_filtered = np.array(y_filtered)\n\nprint(f\"Shape of data before augmentation: {X_filtered.shape}\")\nprint(f\"Label distribution before augmentation: {Counter(y_filtered)}\")\n\n# --- 5. Class-wise Augmentation for Imbalance ---\nprint(\"\\nHandling class imbalance with augmentation...\")\n\ndef augment_minority_class(X, y):\n    \"\"\"Augments the minority class by adding scaled Gaussian noise.\"\"\"\n    class_counts = Counter(y)\n    if not class_counts or len(class_counts) < 2:\n        return X, y\n\n    minority_class = min(class_counts, key=class_counts.get)\n    majority_class = max(class_counts, key=class_counts.get)\n    \n    n_majority = class_counts[majority_class]\n    n_minority = class_counts[minority_class]\n\n    if n_majority == n_minority:\n        print(\"Classes are already balanced.\")\n        return X, y\n\n    n_to_generate = n_majority - n_minority\n    print(f\"Balancing classes. Augmenting class '{minority_class}' with {n_to_generate} new samples.\")\n    \n    minority_indices = np.where(y == minority_class)[0]\n    random_indices = np.random.choice(minority_indices, size=n_to_generate, replace=True)\n    X_minority_original = X[random_indices]\n    \n    noise_factor = 0.1 \n    noise = np.random.normal(0, np.std(X_minority_original) * noise_factor, X_minority_original.shape)\n    X_augmented = X_minority_original + noise\n    \n    X_balanced = np.concatenate((X, X_augmented), axis=0)\n    y_augmented = np.full(n_to_generate, minority_class)\n    y_balanced = np.concatenate((y, y_augmented), axis=0)\n    \n    return X_balanced, y_balanced\n\nX_balanced, y_balanced = augment_minority_class(X_filtered, y_filtered)\n\nprint(f\"Shape of data after augmentation: {X_balanced.shape}\")\nprint(f\"Label distribution after augmentation: {Counter(y_balanced)}\")\n\n# --- 6. Final Preparation and Model Training ---\nprint(\"\\nPreparing data for the model...\")\n\n# Split data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(\n    X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced\n)\n\n# --- EDITED SECTION: Print shapes of data fed into the model ---\nprint(\"\\n--- Data Fed Into Model ---\")\nprint(f\"Training data shape (X_train): {X_train.shape}\")\nprint(f\"Training labels shape (y_train): {y_train.shape}\")\nprint(f\"Validation data shape (X_val): {X_val.shape}\")\nprint(f\"Validation labels shape (y_val): {y_val.shape}\")\nprint(\"---------------------------\\n\")\n\n# One-hot encode the labels for the neural network\ny_train_cat = to_categorical(y_train, num_classes=2)\ny_val_cat = to_categorical(y_val, num_classes=2)\n\n# Define the input shape for the model\ninput_shape = (X_train.shape[1], X_train.shape[2])\n\nmodel = Sequential([\n    Conv1D(filters=32, kernel_size=10, activation='relu', padding='same', input_shape=input_shape),\n    BatchNormalization(),\n    Conv1D(filters=64, kernel_size=3, activation='relu'),\n    BatchNormalization(),\n    MaxPooling1D(pool_size=2),\n    Dropout(0.5),\n    Conv1D(filters=128, kernel_size=3, activation='relu'),\n    BatchNormalization(),\n    MaxPooling1D(pool_size=2),\n    Dropout(0.5),\n    Flatten(),\n    Dense(100, activation='relu'),\n    Dropout(0.5),\n    Dense(2, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, min_lr=0.00001)\n\nprint(\"\\nStarting model training...\")\n# Train the model with verbose=1 to show training logs\nhistory = model.fit(\n    X_train, y_train_cat,\n    epochs=100,\n    batch_size=32,\n    validation_data=(X_val, y_val_cat),\n    callbacks=[early_stopping, reduce_lr],\n    verbose=1 # EDITED: Changed from 2 to 1 for progress bar logs\n)\n\nprint(\"\\nTraining finished.\")\nloss, accuracy = model.evaluate(X_val, y_val_cat, verbose=0)\nprint(f\"\\nFinal Validation Accuracy: {accuracy*100:.2f}%\")\n\n# --- 7. EDITED SECTION: Model Evaluation ---\nprint(\"\\n--- Model Evaluation on Validation Set ---\")\n\n# Get model predictions\ny_pred_probs = model.predict(X_val)\ny_pred = np.argmax(y_pred_probs, axis=1)\n\n# Define class names (0=Left, 1=Right)\nclass_names = ['Left', 'Right']\n\n# 1. Classification Report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_val, y_pred, target_names=class_names))\n\n# 2. Confusion Matrix\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_val, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=class_names, yticklabels=class_names)\nplt.title('Confusion Matrix on Validation Data')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom scipy.signal import butter, lfilter\nfrom collections import Counter\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n\n# --- 1. Configuration ---\n# Define the base path for the dataset on Kaggle\nBASE_PATH = '/kaggle/input/mtcaic3'\n# Target time window in seconds\nWINDOW = (5.75, 6.75)\n# EEG Sampling Frequency\nFS = 250\n# Subset of channels relevant to motor imagery\nCHANNELS = ['C3', 'CZ', 'C4', 'PZ']\n# Calculate number of samples in the window\nSTART_SAMP = int(WINDOW[0] * FS)\nEND_SAMP = int(WINDOW[1] * FS)\n\n\n# --- 2. Data Loading and Preprocessing Functions ---\n\ndef load_trial_data(row, base_path, task_folder):\n    \"\"\"Loads a trial's full EEG data from the correct folder (train/validation/test).\"\"\"\n    eeg_path = os.path.join(base_path, row['task'], task_folder, row['subject_id'], str(row['trial_session']), 'EEGdata.csv')\n    eeg_data = pd.read_csv(eeg_path)\n    \n    trial_num = int(row['trial'])\n    samples_per_trial = 2250\n        \n    start_idx = (trial_num - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial\n    \n    return eeg_data.iloc[start_idx:end_idx]\n\ndef butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n    \"\"\"Applies a bandpass filter to the data.\"\"\"\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    return lfilter(b, a, data, axis=0)\n\ndef process_data_from_df(df, base_path, task_folder):\n    \"\"\"\n    Helper function to load, filter, and preprocess data from a given dataframe (train or validation).\n    Processes only MI tasks.\n    \"\"\"\n    X_data, y_data = [], []\n    mi_df = df[df['task'] == 'MI'].reset_index(drop=True)\n    \n    for _, row in mi_df.iterrows():\n        trial_full_data = load_trial_data(row, base_path, task_folder)\n        trial_window_data = trial_full_data.iloc[START_SAMP:END_SAMP]\n        X_trial = trial_window_data[CHANNELS].values\n        X_trial_filtered = butter_bandpass_filter(X_trial, lowcut=8.0, highcut=30.0, fs=FS)\n        X_data.append(X_trial_filtered)\n        y_data.append(0 if row['label'] == 'Left' else 1)\n        \n    return np.array(X_data), np.array(y_data)\n\n\n# --- 3. Load Official Train and Validation Data ---\nprint(\"--- Loading and Processing Data ---\")\ntrain_df = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\nvalidation_df = pd.read_csv(os.path.join(BASE_PATH, 'validation.csv'))\n\nX_train, y_train = process_data_from_df(train_df, BASE_PATH, 'train')\nX_val, y_val = process_data_from_df(validation_df, BASE_PATH, 'validation')\n\nprint(f\"\\nLoaded {X_train.shape[0]} raw training samples.\")\nprint(f\"Loaded {X_val.shape[0]} raw validation samples from new subjects.\")\n\n\n# --- 4. Contralateral Pattern Filtering ---\ndef filter_by_contralateral_pattern(X, y):\n    \"\"\"Keeps only trials that follow the expected contralateral MI pattern.\"\"\"\n    X_filtered, y_filtered = [], []\n    c3_idx, c4_idx = 0, 2\n    for i in range(X.shape[0]):\n        power_c3 = np.mean(X[i, :, c3_idx]**2)\n        power_c4 = np.mean(X[i, :, c4_idx]**2)\n        \n        if (y[i] == 1 and power_c3 > power_c4) or \\\n           (y[i] == 0 and power_c4 > power_c3):\n            X_filtered.append(X[i])\n            y_filtered.append(y[i])\n            \n    return np.array(X_filtered), np.array(y_filtered)\n\nprint(\"\\nFiltering training trials based on contralateral patterns...\")\nX_train_filtered, y_train_filtered = filter_by_contralateral_pattern(X_train, y_train)\nprint(f\"Kept {len(X_train_filtered)} of {len(X_train)} training samples after filtering.\")\n\nprint(\"\\nFiltering validation trials based on contralateral patterns...\")\nX_val_filtered, y_val_filtered = filter_by_contralateral_pattern(X_val, y_val)\nprint(f\"Kept {len(X_val_filtered)} of {len(X_val)} validation samples after filtering.\")\n\n\n# --- 5. Augment Filtered Training Data Only ---\ndef augment_training_data(X, y):\n    \"\"\"Augments the minority class in the training set by adding scaled Gaussian noise.\"\"\"\n    class_counts = Counter(y)\n    if not class_counts or len(class_counts) < 2:\n        return X, y\n    \n    minority_class = min(class_counts, key=class_counts.get)\n    n_to_generate = max(class_counts.values()) - min(class_counts.values())\n\n    if n_to_generate == 0:\n        print(\"\\nTraining data is already balanced. No augmentation needed.\")\n        return X, y\n\n    print(f\"\\nBalancing training data. Augmenting class '{minority_class}' with {n_to_generate} new samples.\")\n    \n    minority_indices = np.where(y == minority_class)[0]\n    random_indices = np.random.choice(minority_indices, size=n_to_generate, replace=True)\n    X_minority_original = X[random_indices]\n    noise_factor = 0.1 \n    noise = np.random.normal(0, np.std(X_minority_original) * noise_factor, X_minority_original.shape)\n    X_augmented = X_minority_original + noise\n    \n    return np.concatenate((X, X_augmented)), np.concatenate((y, np.full(n_to_generate, minority_class)))\n\nX_train_aug, y_train_aug = augment_training_data(X_train_filtered, y_train_filtered)\n\n\n# --- 6. Prepare Data for Model and Print Final Shapes ---\nprint(\"\\n--- Final Data Shapes Fed Into Model ---\")\nprint(f\"Augmented Training Data: {X_train_aug.shape}\")\nprint(f\"Augmented Training Labels: {y_train_aug.shape} (Distribution: {Counter(y_train_aug)})\")\nprint(f\"Filtered Validation Data: {X_val_filtered.shape}\")\nprint(f\"Filtered Validation Labels: {y_val_filtered.shape} (Distribution: {Counter(y_val_filtered)})\")\nprint(\"----------------------------------------\\n\")\n\n# One-hot encode the labels for the neural network\ny_train_cat = to_categorical(y_train_aug, num_classes=2)\ny_val_cat = to_categorical(y_val_filtered, num_classes=2)\n\n\n# --- 7. Model Definition and Training ---\ninput_shape = (X_train_aug.shape[1], X_train_aug.shape[2])\n\nmodel = Sequential([\n    Conv1D(filters=32, kernel_size=10, activation='relu', padding='same', input_shape=input_shape),\n    BatchNormalization(),\n    Conv1D(filters=64, kernel_size=3, activation='relu'),\n    BatchNormalization(),\n    MaxPooling1D(pool_size=2),\n    Dropout(0.5),\n    Conv1D(filters=128, kernel_size=3, activation='relu'),\n    BatchNormalization(),\n    MaxPooling1D(pool_size=2),\n    Dropout(0.5),\n    Flatten(),\n    Dense(100, activation='relu'),\n    Dropout(0.5),\n    Dense(2, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, min_lr=1e-5)\n]\n\nprint(\"\\nStarting model training...\")\n# Check if there is data to validate on\nif X_val_filtered.shape[0] > 0:\n    history = model.fit(\n        X_train_aug, y_train_cat,\n        epochs=100,\n        batch_size=32,\n        validation_data=(X_val_filtered, y_val_cat),\n        callbacks=callbacks,\n        verbose=1\n    )\nelse:\n    print(\"Skipping training with validation as the filtered validation set is empty.\")\n    history = model.fit(\n        X_train_aug, y_train_cat,\n        epochs=100,\n        batch_size=32,\n        callbacks=[],\n        verbose=1\n    )\n\n\n# --- 8. Final Evaluation on Filtered Validation Set ---\nprint(\"\\n--- Model Evaluation on Filtered Validation Set (Unseen Subjects) ---\")\n\nif X_val_filtered.shape[0] > 0:\n    loss, accuracy = model.evaluate(X_val_filtered, y_val_cat, verbose=0)\n    print(f\"\\nFinal Validation Accuracy on filtered data from unseen subjects: {accuracy*100:.2f}%\")\n    \n    y_pred_probs = model.predict(X_val_filtered)\n    y_pred = np.argmax(y_pred_probs, axis=1)\n    class_names = ['Left', 'Right']\n\n    # === FIX IMPLEMENTED HERE ===\n    \n    # Display Classification Report\n    print(\"\\nClassification Report:\")\n    # Using the 'labels' parameter prevents the error if a class is missing.\n    # 'zero_division=0' handles cases where a metric (like precision) can't be calculated.\n    print(classification_report(y_val_filtered, y_pred, target_names=class_names, labels=[0, 1], zero_division=0))\n\n    # Display Confusion Matrix\n    print(\"\\nConfusion Matrix:\")\n    # Using the 'labels' parameter here ensures the matrix is always 2x2.\n    cm = confusion_matrix(y_val_filtered, y_pred, labels=[0, 1])\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix on Filtered Validation Set')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n    \nelse:\n    print(\"\\nNo data in the filtered validation set to evaluate.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom scipy.signal import butter, lfilter\nfrom mne.decoding import CSP\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# --- 1. CONFIGURATION AND SETUP ---\n# Kaggle environment path\nBASE_PATH = '/kaggle/input/mtcaic3'\n\n# EEG constants\nSAMPLE_RATE = 250  # Hz\nMI_CHANNELS = ['C3', 'Cz', 'C4'] # Core channels for Motor Imagery\nALL_EEG_CHANNELS = ['FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8']\nWINDOW = (5.75, 6.75)  # Target time window in seconds\nMU_BAND = (8, 12) # Frequency band for contralateral check (Hz)\n\n# Model parameters\nCSP_COMPONENTS = 4 # Number of CSP components to use as features\nEPOCHS = 100\nBATCH_SIZE = 32\nVALIDATION_SPLIT = 0.2\n\n# --- 2. HELPER FUNCTIONS ---\n\ndef bandpass_filter(data, lowcut, highcut, fs, order=5):\n    \"\"\"Applies a bandpass filter to the data.\"\"\"\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    y = lfilter(b, a, data, axis=0)\n    return y\n\ndef load_trial_data(row, base_path, split):\n    \"\"\"Loads raw EEG data for a single trial specified by a metadata row.\"\"\"\n    eeg_path = os.path.join(base_path, row['task'], split, row['subject_id'], str(row['trial_session']), 'EEGdata.csv')\n    eeg_data = pd.read_csv(eeg_path)\n    \n    trial_num = int(row['trial'])\n    samples_per_trial = 2250 # MI: 9s * 250Hz\n    \n    start_idx = (trial_num - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial\n    \n    trial_data = eeg_data.iloc[start_idx:end_idx]\n    return trial_data\n\ndef check_contralateral_pattern(trial_data_window, label):\n    \"\"\"\n    Checks if a trial exhibits the expected contralateral ERD pattern.\n    - Left hand MI should show lower power in the right hemisphere (C4) vs left (C3).\n    - Right hand MI should show lower power in the left hemisphere (C3) vs right (C4).\n    \"\"\"\n    # Filter data for mu-band frequencies\n    c3_filtered = bandpass_filter(trial_data_window['C3'], MU_BAND[0], MU_BAND[1], SAMPLE_RATE)\n    c4_filtered = bandpass_filter(trial_data_window['C4'], MU_BAND[0], MU_BAND[1], SAMPLE_RATE)\n\n    # Calculate power (variance)\n    power_c3 = np.var(c3_filtered)\n    power_c4 = np.var(c4_filtered)\n\n    if label == 'Left':\n        return power_c4 < power_c3\n    elif label == 'Right':\n        return power_c3 < power_c4\n    else:\n        return False # Should not happen for MI tasks\n\ndef augment_trial(trial_data, label):\n    \"\"\"Augments data by flipping channels and adding noise.\"\"\"\n    # 1. Channel Flipping\n    flipped_data = trial_data.copy()\n    c3_data = flipped_data['C3'].copy()\n    flipped_data['C3'] = flipped_data['C4']\n    flipped_data['C4'] = c3_data\n    flipped_label = 'Right' if label == 'Left' else 'Left'\n    \n    # 2. Noise Addition\n    noise = np.random.normal(0, 0.1 * np.std(trial_data), trial_data.shape)\n    noisy_data = trial_data + noise\n    \n    return [(flipped_data, flipped_label), (noisy_data, label)]\n\n\ndef create_dl_model(input_shape):\n    \"\"\"Creates a simple Deep Learning model for classification.\"\"\"\n    model = Sequential([\n        Input(shape=(input_shape,)),\n        Dense(64, activation='relu'),\n        Dropout(0.5),\n        Dense(32, activation='relu'),\n        Dropout(0.5),\n        Dense(1, activation='sigmoid') # Binary classification\n    ])\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n# --- 3. DATA LOADING AND PREPROCESSING ---\n\nprint(\"Loading metadata...\")\ntrain_df = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\nvalidation_df = pd.read_csv(os.path.join(BASE_PATH, 'validation.csv'))\n\n# Filter for Motor Imagery (MI) tasks only\nmi_train_df = train_df[train_df['task'] == 'MI'].reset_index(drop=True)\nmi_validation_df = validation_df[validation_df['task'] == 'MI'].reset_index(drop=True)\n\ndef process_dataframe(df, split):\n    \"\"\"\n    Loads, filters, and processes EEG data based on the contralateral pattern.\n    \"\"\"\n    print(f\"\\nProcessing {split} data...\")\n    X, y = [], []\n    \n    # Calculate sample indices for the time window\n    start_sample = int(WINDOW[0] * SAMPLE_RATE)\n    end_sample = int(WINDOW[1] * SAMPLE_RATE)\n\n    dropped_trials = 0\n    for i, row in df.iterrows():\n        # Load data for the entire trial\n        full_trial_data = load_trial_data(row, BASE_PATH, split)\n        \n        # Extract the specific time window\n        windowed_data = full_trial_data.iloc[start_sample:end_sample]\n        \n        # Check for contralateral pattern\n        if check_contralateral_pattern(windowed_data, row['label']):\n            # Use only the specified MI channels\n            trial_channels = windowed_data[MI_CHANNELS]\n            X.append(trial_channels.to_numpy())\n            y.append(row['label'])\n        else:\n            dropped_trials += 1\n            \n    print(f\"Kept {len(X)} trials. Dropped {dropped_trials} trials that did not fit the contralateral pattern.\")\n    return np.array(X), np.array(y)\n\n# Process training and validation sets\nX_train_raw, y_train_raw = process_dataframe(mi_train_df, 'train')\nX_val_filtered, y_val_filtered = process_dataframe(mi_validation_df, 'validation')\n\n# --- 4. DATA AUGMENTATION (ON TRAINING SET ONLY) ---\nprint(\"\\nAugmenting training data...\")\nX_train_aug, y_train_aug = [], []\nfor trial, label in zip(X_train_raw, y_train_raw):\n    # Add original trial\n    X_train_aug.append(trial)\n    y_train_aug.append(label)\n    \n    # Create a DataFrame to use augment function\n    trial_df = pd.DataFrame(trial, columns=MI_CHANNELS)\n    \n    # Add augmented versions\n    augmented_versions = augment_trial(trial_df, label)\n    for aug_data, aug_label in augmented_versions:\n        X_train_aug.append(aug_data.to_numpy())\n        y_train_aug.append(aug_label)\n\nX_train_aug = np.array(X_train_aug)\ny_train_aug = np.array(y_train_aug)\nprint(f\"Original training size: {len(X_train_raw)}. Augmented training size: {len(X_train_aug)}\")\n\n# --- 5. LABEL ENCODING and CLASS WEIGHTS ---\nle = LabelEncoder()\ny_train_encoded = le.fit_transform(y_train_aug)\ny_val_encoded = le.transform(y_val_filtered)\n\n# Handle class imbalance after filtering and augmentation\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\nclass_weights_dict = dict(enumerate(class_weights))\nprint(f\"\\nClass weights for handling imbalance: {class_weights_dict}\")\n\n# --- 6. CSP FEATURE EXTRACTION ---\nprint(\"\\nFitting CSP and transforming data...\")\n# Reshape data for MNE: (n_epochs, n_channels, n_times)\nX_train_csp_input = X_train_aug.transpose(0, 2, 1)\nX_val_csp_input = X_val_filtered.transpose(0, 2, 1)\n\n# Initialize and fit CSP\ncsp = CSP(n_components=CSP_COMPONENTS, reg=None, log=True, norm_trace=False)\ncsp.fit(X_train_csp_input, y_train_encoded)\n\n# Transform data into CSP features\nX_train_features = csp.transform(X_train_csp_input)\nX_val_features = csp.transform(X_val_csp_input)\nprint(f\"Shape of data after CSP transformation: {X_train_features.shape}\")\n\n# --- 7. DEEP LEARNING MODEL TRAINING ---\nprint(\"\\nBuilding and training the Deep Learning model...\")\nmodel = create_dl_model(input_shape=X_train_features.shape[1])\nmodel.summary()\n\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True)\n\nhistory = model.fit(\n    X_train_features,\n    y_train_encoded,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    validation_data=(X_val_features, y_val_encoded),\n    class_weight=class_weights_dict,\n    callbacks=[early_stopping],\n    verbose=2\n)\n\n# --- 8. EVALUATION ---\nprint(\"\\nEvaluating model on the filtered validation set...\")\nloss, accuracy = model.evaluate(X_val_features, y_val_encoded)\nprint(f\"Validation Accuracy: {accuracy:.4f}\")\nprint(f\"Validation Loss: {loss:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom scipy.signal import butter, lfilter\nfrom mne.decoding import CSP\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# --- 1. CONFIGURATION AND SETUP ---\n# Kaggle environment path\nBASE_PATH = '/kaggle/input/mtcaic3'\n\n# EEG constants\nSAMPLE_RATE = 250  # Hz\nMI_CHANNELS = ['C3', 'Cz', 'C4'] # Core channels for Motor Imagery\nALL_EEG_CHANNELS = ['FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8']\nWINDOW = (5.75, 6.75)  # Target time window in seconds\nMU_BAND = (8, 12) # Frequency band for contralateral check (Hz)\n\n# Model parameters\nCSP_COMPONENTS = 4 # Number of CSP components to use as features\nEPOCHS = 100\nBATCH_SIZE = 32\nVALIDATION_SPLIT = 0.2\n\n# --- 2. HELPER FUNCTIONS ---\n\ndef bandpass_filter(data, lowcut, highcut, fs, order=5):\n    \"\"\"Applies a bandpass filter to the data.\"\"\"\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    y = lfilter(b, a, data, axis=0)\n    return y\n\ndef load_trial_data(row, base_path, split):\n    \"\"\"Loads raw EEG data for a single trial specified by a metadata row.\"\"\"\n    eeg_path = os.path.join(base_path, row['task'], split, row['subject_id'], str(row['trial_session']), 'EEGdata.csv')\n    eeg_data = pd.read_csv(eeg_path)\n    \n    trial_num = int(row['trial'])\n    samples_per_trial = 2250 # MI: 9s * 250Hz\n    \n    start_idx = (trial_num - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial\n    \n    trial_data = eeg_data.iloc[start_idx:end_idx]\n    return trial_data\n\ndef check_contralateral_pattern(trial_data_window, label):\n    \"\"\"\n    Checks if a trial exhibits the expected contralateral ERD pattern.\n    - Left hand MI should show lower power in the right hemisphere (C4) vs left (C3).\n    - Right hand MI should show lower power in the left hemisphere (C3) vs right (C4).\n    \"\"\"\n    # Filter data for mu-band frequencies\n    c3_filtered = bandpass_filter(trial_data_window['C3'], MU_BAND[0], MU_BAND[1], SAMPLE_RATE)\n    c4_filtered = bandpass_filter(trial_data_window['C4'], MU_BAND[0], MU_BAND[1], SAMPLE_RATE)\n\n    # Calculate power (variance)\n    power_c3 = np.var(c3_filtered)\n    power_c4 = np.var(c4_filtered)\n\n    if label == 'Left':\n        return power_c4 < power_c3\n    elif label == 'Right':\n        return power_c3 < power_c4\n    else:\n        return False # Should not happen for MI tasks\n\ndef augment_trial(trial_data, label):\n    \"\"\"Augments data by flipping channels and adding noise.\"\"\"\n    # 1. Channel Flipping\n    flipped_data = trial_data.copy()\n    c3_data = flipped_data['C3'].copy()\n    flipped_data['C3'] = flipped_data['C4']\n    flipped_data['C4'] = c3_data\n    flipped_label = 'Right' if label == 'Left' else 'Left'\n    \n    # 2. Noise Addition\n    noise = np.random.normal(0, 0.1 * np.std(trial_data), trial_data.shape)\n    noisy_data = trial_data + noise\n    \n    return [(flipped_data, flipped_label), (noisy_data, label)]\n\n\ndef create_dl_model(input_shape):\n    \"\"\"Creates a simple Deep Learning model for classification.\"\"\"\n    model = Sequential([\n        Input(shape=(input_shape,)),\n        Dense(64, activation='relu'),\n        Dropout(0.5),\n        Dense(32, activation='relu'),\n        Dropout(0.5),\n        Dense(1, activation='sigmoid') # Binary classification\n    ])\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n# --- 3. DATA LOADING AND PREPROCESSING ---\n\nprint(\"Loading metadata...\")\ntrain_df = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\nvalidation_df = pd.read_csv(os.path.join(BASE_PATH, 'validation.csv'))\n\n# Filter for Motor Imagery (MI) tasks only\nmi_train_df = train_df[train_df['task'] == 'MI'].reset_index(drop=True)\nmi_validation_df = validation_df[validation_df['task'] == 'MI'].reset_index(drop=True)\n\ndef process_dataframe(df, split):\n    \"\"\"\n    Loads, filters, and processes EEG data based on the contralateral pattern.\n    \"\"\"\n    print(f\"\\nProcessing {split} data...\")\n    X, y = [], []\n    \n    # Calculate sample indices for the time window\n    start_sample = int(WINDOW[0] * SAMPLE_RATE)\n    end_sample = int(WINDOW[1] * SAMPLE_RATE)\n\n    dropped_trials = 0\n    for i, row in df.iterrows():\n        # Load data for the entire trial\n        full_trial_data = load_trial_data(row, BASE_PATH, split)\n        \n        # Extract the specific time window\n        windowed_data = full_trial_data.iloc[start_sample:end_sample]\n        \n        # Check for contralateral pattern\n        if check_contralateral_pattern(windowed_data, row['label']):\n            # Use only the specified MI channels\n            trial_channels = windowed_data[MI_CHANNELS]\n            X.append(trial_channels.to_numpy())\n            y.append(row['label'])\n        else:\n            dropped_trials += 1\n            \n    print(f\"Kept {len(X)} trials. Dropped {dropped_trials} trials that did not fit the contralateral pattern.\")\n    return np.array(X), np.array(y)\n\n# Process training and validation sets\nX_train_raw, y_train_raw = process_dataframe(mi_train_df, 'train')\nX_val_filtered, y_val_filtered = process_dataframe(mi_validation_df, 'validation')\n\n# --- 4. DATA AUGMENTATION (ON TRAINING SET ONLY) ---\nprint(\"\\nAugmenting training data...\")\nX_train_aug, y_train_aug = [], []\nfor trial, label in zip(X_train_raw, y_train_raw):\n    # Add original trial\n    X_train_aug.append(trial)\n    y_train_aug.append(label)\n    \n    # Create a DataFrame to use augment function\n    trial_df = pd.DataFrame(trial, columns=MI_CHANNELS)\n    \n    # Add augmented versions\n    augmented_versions = augment_trial(trial_df, label)\n    for aug_data, aug_label in augmented_versions:\n        X_train_aug.append(aug_data.to_numpy())\n        y_train_aug.append(aug_label)\n\nX_train_aug = np.array(X_train_aug)\ny_train_aug = np.array(y_train_aug)\nprint(f\"Original training size: {len(X_train_raw)}. Augmented training size: {len(X_train_aug)}\")\n\n# --- 5. LABEL ENCODING and CLASS WEIGHTS ---\nle = LabelEncoder()\ny_train_encoded = le.fit_transform(y_train_aug)\ny_val_encoded = le.transform(y_val_filtered)\n\n# Handle class imbalance after filtering and augmentation\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\nclass_weights_dict = dict(enumerate(class_weights))\nprint(f\"\\nClass weights for handling imbalance: {class_weights_dict}\")\n\n# --- 6. CSP FEATURE EXTRACTION ---\nprint(\"\\nFitting CSP and transforming data...\")\n# Reshape data for MNE: (n_epochs, n_channels, n_times)\nX_train_csp_input = X_train_aug.transpose(0, 2, 1)\nX_val_csp_input = X_val_filtered.transpose(0, 2, 1)\n\n# Initialize and fit CSP\ncsp = CSP(n_components=CSP_COMPONENTS, reg=None, log=True, norm_trace=False)\ncsp.fit(X_train_csp_input, y_train_encoded)\n\n# Transform data into CSP features\nX_train_features = csp.transform(X_train_csp_input)\nX_val_features = csp.transform(X_val_csp_input)\nprint(f\"Shape of data after CSP transformation: {X_train_features.shape}\")\n\n# --- 7. DEEP LEARNING MODEL TRAINING ---\nprint(\"\\nBuilding and training the Deep Learning model...\")\nmodel = create_dl_model(input_shape=X_train_features.shape[1])\nmodel.summary()\n\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True)\n\nhistory = model.fit(\n    X_train_features,\n    y_train_encoded,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    validation_data=(X_val_features, y_val_encoded),\n    class_weight=class_weights_dict,\n    callbacks=[early_stopping],\n    verbose=2\n)\n\n# --- 8. EVALUATION ---\nprint(\"\\n--- Basic Model Evaluation ---\")\nloss, accuracy = model.evaluate(X_val_features, y_val_encoded, verbose=0)\nprint(f\"Validation Accuracy: {accuracy:.4f}\")\nprint(f\"Validation Loss: {loss:.4f}\")\n\n# --- 9. DETAILED EVALUATION AND REPORTING ---\nprint(\"\\n--- Detailed Performance Analysis ---\")\n# Get model predictions for the validation set\ny_pred_probs = model.predict(X_val_features)\ny_pred = (y_pred_probs > 0.5).astype(int).flatten() # Convert probabilities to binary classes (0 or 1)\n\n# Generate and print the classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_val_encoded, y_pred, target_names=le.classes_))\n\n# Generate and plot the confusion matrix\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_val_encoded, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=le.classes_, yticklabels=le.classes_)\nplt.title('Confusion Matrix for Validation Data')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom scipy.signal import butter, lfilter\nfrom mne.decoding import CSP\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# --- 1. CONFIGURATION AND SETUP ---\n# Kaggle environment path\nBASE_PATH = '/kaggle/input/mtcaic3'\n\n# EEG constants\nSAMPLE_RATE = 250  # Hz\nMI_CHANNELS = ['C3', 'CZ', 'C4'] # Core channels for Motor Imagery\nALL_EEG_CHANNELS = ['FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8']\nWINDOW = (2.75, 3.75)  # Target time window in seconds\nMU_BAND = (8, 12) # Frequency band for contralateral check (Hz)\n\n# Model parameters\nCSP_COMPONENTS = 4 # Number of CSP components to use as features\nEPOCHS = 100\nBATCH_SIZE = 32\nVALIDATION_SPLIT = 0.2\n\n# --- 2. HELPER FUNCTIONS ---\n\ndef bandpass_filter(data, lowcut, highcut, fs, order=5):\n    \"\"\"Applies a bandpass filter to the data.\"\"\"\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    y = lfilter(b, a, data, axis=0)\n    return y\n\ndef load_trial_data(row, base_path, split):\n    \"\"\"Loads raw EEG data for a single trial specified by a metadata row.\"\"\"\n    eeg_path = os.path.join(base_path, row['task'], split, row['subject_id'], str(row['trial_session']), 'EEGdata.csv')\n    eeg_data = pd.read_csv(eeg_path)\n    \n    trial_num = int(row['trial'])\n    samples_per_trial = 2250 # MI: 9s * 250Hz\n    \n    start_idx = (trial_num - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial\n    \n    trial_data = eeg_data.iloc[start_idx:end_idx]\n    return trial_data\n\ndef check_contralateral_pattern(trial_data_window, label):\n    \"\"\"\n    Checks if a trial exhibits the expected contralateral ERD pattern.\n    - Left hand MI should show lower power in the right hemisphere (C4) vs left (C3).\n    - Right hand MI should show lower power in the left hemisphere (C3) vs right (C4).\n    \"\"\"\n    # Filter data for mu-band frequencies\n    c3_filtered = bandpass_filter(trial_data_window['C3'], MU_BAND[0], MU_BAND[1], SAMPLE_RATE)\n    c4_filtered = bandpass_filter(trial_data_window['C4'], MU_BAND[0], MU_BAND[1], SAMPLE_RATE)\n\n    # Calculate power (variance)\n    power_c3 = np.var(c3_filtered)\n    power_c4 = np.var(c4_filtered)\n\n    if label == 'Left':\n        return power_c4 < power_c3\n    elif label == 'Right':\n        return power_c3 < power_c4\n    else:\n        return False # Should not happen for MI tasks\n\ndef augment_trial(trial_data, label):\n    \"\"\"Augments data by flipping channels and adding noise.\"\"\"\n    # 1. Channel Flipping\n    flipped_data = trial_data.copy()\n    c3_data = flipped_data['C3'].copy()\n    flipped_data['C3'] = flipped_data['C4']\n    flipped_data['C4'] = c3_data\n    flipped_label = 'Right' if label == 'Left' else 'Left'\n    \n    # 2. Noise Addition\n    noise = np.random.normal(0, 0.1 * np.std(trial_data), trial_data.shape)\n    noisy_data = trial_data + noise\n    \n    return [(flipped_data, flipped_label), (noisy_data, label)]\n\n\ndef create_dl_model(input_shape):\n    \"\"\"Creates a simple Deep Learning model for classification.\"\"\"\n    model = Sequential([\n        Input(shape=(input_shape,)),\n        Dense(64, activation='relu'),\n        Dropout(0.5),\n        Dense(32, activation='relu'),\n        Dropout(0.5),\n        Dense(1, activation='sigmoid') # Binary classification\n    ])\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n# --- 3. DATA LOADING AND PREPROCESSING ---\n\nprint(\"Loading metadata...\")\ntrain_df = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\nvalidation_df = pd.read_csv(os.path.join(BASE_PATH, 'validation.csv'))\n\n# Filter for Motor Imagery (MI) tasks only\nmi_train_df = train_df[train_df['task'] == 'MI'].reset_index(drop=True)\nmi_validation_df = validation_df[validation_df['task'] == 'MI'].reset_index(drop=True)\n\ndef process_dataframe(df, split):\n    \"\"\"\n    Loads, filters, and processes EEG data based on the contralateral pattern.\n    \"\"\"\n    print(f\"\\nProcessing {split} data...\")\n    X, y = [], []\n    \n    # Calculate sample indices for the time window\n    start_sample = int(WINDOW[0] * SAMPLE_RATE)\n    end_sample = int(WINDOW[1] * SAMPLE_RATE)\n\n    dropped_trials = 0\n    for i, row in df.iterrows():\n        # Load data for the entire trial\n        full_trial_data = load_trial_data(row, BASE_PATH, split)\n        \n        # Extract the specific time window\n        windowed_data = full_trial_data.iloc[start_sample:end_sample]\n        \n        # Check for contralateral pattern\n        if check_contralateral_pattern(windowed_data, row['label']):\n            # Use only the specified MI channels\n            trial_channels = windowed_data[MI_CHANNELS]\n            X.append(trial_channels.to_numpy())\n            y.append(row['label'])\n        else:\n            dropped_trials += 1\n            \n    print(f\"Kept {len(X)} trials. Dropped {dropped_trials} trials that did not fit the contralateral pattern.\")\n    return np.array(X), np.array(y)\n\n# Process training and validation sets\nX_train_raw, y_train_raw = process_dataframe(mi_train_df, 'train')\nX_val_filtered, y_val_filtered = process_dataframe(mi_validation_df, 'validation')\n\n# --- 4. DATA AUGMENTATION (ON TRAINING SET ONLY) ---\nprint(\"\\nAugmenting training data...\")\nX_train_aug, y_train_aug = [], []\nfor trial, label in zip(X_train_raw, y_train_raw):\n    # Add original trial\n    X_train_aug.append(trial)\n    y_train_aug.append(label)\n    \n    # Create a DataFrame to use augment function\n    trial_df = pd.DataFrame(trial, columns=MI_CHANNELS)\n    \n    # Add augmented versions\n    augmented_versions = augment_trial(trial_df, label)\n    for aug_data, aug_label in augmented_versions:\n        X_train_aug.append(aug_data.to_numpy())\n        y_train_aug.append(aug_label)\n\nX_train_aug = np.array(X_train_aug)\ny_train_aug = np.array(y_train_aug)\nprint(f\"Original training size: {len(X_train_raw)}. Augmented training size: {len(X_train_aug)}\")\n\n# --- 5. LABEL ENCODING and CLASS WEIGHTS ---\nle = LabelEncoder()\ny_train_encoded = le.fit_transform(y_train_aug)\ny_val_encoded = le.transform(y_val_filtered)\n\n# Handle class imbalance after filtering and augmentation\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\nclass_weights_dict = dict(enumerate(class_weights))\nprint(f\"\\nClass weights for handling imbalance: {class_weights_dict}\")\n\n# --- 6. CSP FEATURE EXTRACTION ---\nprint(\"\\nFitting CSP and transforming data...\")\n# Reshape data for MNE: (n_epochs, n_channels, n_times)\nX_train_csp_input = X_train_aug.transpose(0, 2, 1)\nX_val_csp_input = X_val_filtered.transpose(0, 2, 1)\n\n# Initialize and fit CSP\ncsp = CSP(n_components=CSP_COMPONENTS, reg=None, log=True, norm_trace=False)\ncsp.fit(X_train_csp_input, y_train_encoded)\n\n# Transform data into CSP features\nX_train_features = csp.transform(X_train_csp_input)\nX_val_features = csp.transform(X_val_csp_input)\nprint(f\"Shape of data after CSP transformation: {X_train_features.shape}\")\n\n# --- 7. DEEP LEARNING MODEL TRAINING ---\nprint(\"\\nBuilding and training the Deep Learning model...\")\nmodel = create_dl_model(input_shape=X_train_features.shape[1])\nmodel.summary()\n\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True)\n\nhistory = model.fit(\n    X_train_features,\n    y_train_encoded,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    validation_data=(X_val_features, y_val_encoded),\n    class_weight=class_weights_dict,\n    callbacks=[early_stopping],\n    verbose=2\n)\n\n# --- 8. EVALUATION ---\nprint(\"\\n--- Basic Model Evaluation ---\")\nloss, accuracy = model.evaluate(X_val_features, y_val_encoded, verbose=0)\nprint(f\"Validation Accuracy: {accuracy:.4f}\")\nprint(f\"Validation Loss: {loss:.4f}\")\n\n# --- 9. DETAILED EVALUATION AND REPORTING ---\nprint(\"\\n--- Detailed Performance Analysis ---\")\n# Get model predictions for the validation set\ny_pred_probs = model.predict(X_val_features)\ny_pred = (y_pred_probs > 0.5).astype(int).flatten() # Convert probabilities to binary classes (0 or 1)\n\n# Generate and print the classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_val_encoded, y_pred, target_names=le.classes_))\n\n# Generate and plot the confusion matrix\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_val_encoded, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=le.classes_, yticklabels=le.classes_)\nplt.title('Confusion Matrix for Validation Data')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom scipy.signal import butter, lfilter\n\n# --- 1. CONFIGURATION ---\n# Kaggle environment path\nBASE_PATH = '/kaggle/input/mtcaic3'\n\n# EEG constants\nSAMPLE_RATE = 250  # Hz\nMU_BAND = (8, 12)  # Frequency band (in Hz) for checking contralateral activity\n\n# Define all the time windows to be tested (in seconds)\nTIME_WINDOWS = [\n    (1.25, 2.25),\n    (2.75, 3.75),\n    (3.50, 4.50),\n    (4.25, 5.25),\n    (5.00, 6.00),\n    (5.75, 6.75),\n    (6.00, 7.00),\n    (7.00, 8.00)\n]\n\n# --- 2. HELPER FUNCTIONS ---\n\ndef bandpass_filter(data, lowcut, highcut, fs, order=5):\n    \"\"\"Applies a bandpass filter to the data.\"\"\"\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    y = lfilter(b, a, data, axis=0)\n    return y\n\ndef load_trial_data(row, base_path, split='validation'):\n    \"\"\"Loads raw EEG data for a single trial specified by a metadata row.\"\"\"\n    eeg_path = os.path.join(base_path, row['task'], split, row['subject_id'], str(row['trial_session']), 'EEGdata.csv')\n    eeg_data = pd.read_csv(eeg_path)\n    \n    trial_num = int(row['trial'])\n    samples_per_trial = 2250  # MI: 9s * 250Hz\n    \n    start_idx = (trial_num - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial\n    \n    trial_data = eeg_data.iloc[start_idx:end_idx]\n    return trial_data\n\ndef check_contralateral_pattern(trial_data_window, label):\n    \"\"\"\n    Checks if a trial exhibits the expected contralateral ERD pattern.\n    - Left hand MI should show lower power in the right hemisphere (C4) vs left (C3).\n    - Right hand MI should show lower power in the left hemisphere (C3) vs right (C4).\n    \"\"\"\n    # Filter data for mu-band frequencies\n    c3_filtered = bandpass_filter(trial_data_window['C3'], MU_BAND[0], MU_BAND[1], SAMPLE_RATE)\n    c4_filtered = bandpass_filter(trial_data_window['C4'], MU_BAND[0], MU_BAND[1], SAMPLE_RATE)\n\n    # Calculate power (variance)\n    power_c3 = np.var(c3_filtered)\n    power_c4 = np.var(c4_filtered)\n\n    if label == 'Left':\n        return power_c4 < power_c3\n    elif label == 'Right':\n        return power_c3 < power_c4\n    return False\n\n# --- 3. MAIN ANALYSIS SCRIPT ---\n\nprint(\"Loading and filtering validation metadata for MI tasks...\")\nvalidation_df = pd.read_csv(os.path.join(BASE_PATH, 'validation.csv'))\nmi_validation_df = validation_df[validation_df['task'] == 'MI'].reset_index(drop=True)\n\ntrials_with_pattern = []\ntrial_labels_with_pattern = []\n\nprint(f\"\\nAnalyzing {len(mi_validation_df)} MI trials across {len(TIME_WINDOWS)} time windows...\")\n\n# Iterate through each MI trial in the validation set\nfor index, row in mi_validation_df.iterrows():\n    full_trial_data = load_trial_data(row, BASE_PATH)\n    \n    found_pattern = False\n    # Check each defined time window for the pattern\n    for start_t, end_t in TIME_WINDOWS:\n        start_sample = int(start_t * SAMPLE_RATE)\n        end_sample = int(end_t * SAMPLE_RATE)\n        \n        windowed_data = full_trial_data.iloc[start_sample:end_sample]\n        \n        # If the pattern is found in any window, mark it and stop checking other windows\n        if check_contralateral_pattern(windowed_data, row['label']):\n            trials_with_pattern.append(row['id'])\n            trial_labels_with_pattern.append(row['label'])\n            found_pattern = True\n            break  # Move to the next trial once a pattern is found\n\n# --- 4. REPORTING RESULTS ---\n\ntotal_mi_trials = len(mi_validation_df)\nnum_trials_with_pattern = len(trials_with_pattern)\npercentage_with_pattern = (num_trials_with_pattern / total_mi_trials) * 100 if total_mi_trials > 0 else 0\n\n# Calculate the class distribution of the trials that showed the pattern\nclass_distribution = pd.Series(trial_labels_with_pattern).value_counts()\n\nprint(\"\\n--- Analysis Complete ---\")\nprint(f\"Total MI validation trials analyzed: {total_mi_trials}\")\nprint(f\"Number of trials showing a contralateral pattern in at least one window: {num_trials_with_pattern}\")\nprint(f\"Percentage of trials following the pattern: {percentage_with_pattern:.2f}%\\n\")\n\nprint(\"Class distribution of trials that followed the pattern:\")\nif not class_distribution.empty:\n    print(class_distribution.to_string())\nelse:\n    print(\"No trials found with the specified pattern.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom scipy.signal import butter, lfilter\n\n# --- 1. CONFIGURATION ---\n# Kaggle environment path\nBASE_PATH = '/kaggle/input/mtcaic3'\n\n# EEG constants\nSAMPLE_RATE = 250  # Hz\nMU_BAND = (8, 12)  # Frequency band (in Hz) for checking contralateral activity\n\n# Define all the time windows to be tested (in seconds)\nTIME_WINDOWS = [\n    (1.25, 2.25),\n    (2.75, 3.75),\n    (3.50, 4.50),\n    (4.25, 5.25),\n    (5.00, 6.00),\n    (5.75, 6.75),\n    (6.00, 7.00),\n    (7.00, 8.00)\n]\n\n# --- 2. HELPER FUNCTIONS ---\n\ndef bandpass_filter(data, lowcut, highcut, fs, order=5):\n    \"\"\"Applies a bandpass filter to the data.\"\"\"\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    y = lfilter(b, a, data, axis=0)\n    return y\n\ndef load_trial_data(row, base_path, split='test'):\n    \"\"\"Loads raw EEG data for a single trial specified by a metadata row.\"\"\"\n    eeg_path = os.path.join(base_path, row['task'], split, row['subject_id'], str(row['trial_session']), 'EEGdata.csv')\n    eeg_data = pd.read_csv(eeg_path)\n    \n    trial_num = int(row['trial'])\n    samples_per_trial = 2250  # MI: 9s * 250Hz\n    \n    start_idx = (trial_num - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial\n    \n    trial_data = eeg_data.iloc[start_idx:end_idx]\n    return trial_data\n\ndef check_contralateral_pattern(trial_data_window, assumed_label):\n    \"\"\"\n    Checks if a trial exhibits the expected contralateral ERD pattern for an ASSUMED label.\n    \"\"\"\n    # Filter data for mu-band frequencies\n    c3_filtered = bandpass_filter(trial_data_window['C3'], MU_BAND[0], MU_BAND[1], SAMPLE_RATE)\n    c4_filtered = bandpass_filter(trial_data_window['C4'], MU_BAND[0], MU_BAND[1], SAMPLE_RATE)\n\n    # Calculate power (variance)\n    power_c3 = np.var(c3_filtered)\n    power_c4 = np.var(c4_filtered)\n\n    if assumed_label == 'Left':\n        return power_c4 < power_c3\n    elif assumed_label == 'Right':\n        return power_c3 < power_c4\n    return False\n\n# --- 3. MAIN ANALYSIS SCRIPT ---\n\nprint(\"Loading and filtering test metadata for MI tasks...\")\ntest_df = pd.read_csv(os.path.join(BASE_PATH, 'test.csv'))\nmi_test_df = test_df[test_df['task'] == 'MI'].reset_index(drop=True)\n\ntrials_fitting_left = []\ntrials_fitting_right = []\n\nprint(f\"\\nAnalyzing {len(mi_test_df)} MI trials from the test set...\")\n\n# Iterate through each MI trial in the test set\nfor index, row in mi_test_df.iterrows():\n    full_trial_data = load_trial_data(row, BASE_PATH)\n    \n    # --- Test 1: Assume the label is 'Left' ---\n    found_left_pattern = False\n    for start_t, end_t in TIME_WINDOWS:\n        start_sample = int(start_t * SAMPLE_RATE)\n        end_sample = int(end_t * SAMPLE_RATE)\n        windowed_data = full_trial_data.iloc[start_sample:end_sample]\n        \n        if check_contralateral_pattern(windowed_data, 'Left'):\n            trials_fitting_left.append(row['id'])\n            found_left_pattern = True\n            break # Pattern found, no need to check other windows for this assumption\n            \n    # --- Test 2: Assume the label is 'Right' ---\n    found_right_pattern = False\n    for start_t, end_t in TIME_WINDOWS:\n        start_sample = int(start_t * SAMPLE_RATE)\n        end_sample = int(end_t * SAMPLE_RATE)\n        windowed_data = full_trial_data.iloc[start_sample:end_sample]\n\n        if check_contralateral_pattern(windowed_data, 'Right'):\n            trials_fitting_right.append(row['id'])\n            found_right_pattern = True\n            break # Pattern found, no need to check other windows for this assumption\n\n# --- 4. REPORTING RESULTS ---\n\ntotal_mi_trials = len(mi_test_df)\nset_fit_left = set(trials_fitting_left)\nset_fit_right = set(trials_fitting_right)\n\n# Calculate counts for different scenarios\nnum_fit_left = len(set_fit_left)\nnum_fit_right = len(set_fit_right)\nnum_fit_both = len(set_fit_left.intersection(set_fit_right))\nnum_fit_only_left = num_fit_left - num_fit_both\nnum_fit_only_right = num_fit_right - num_fit_both\nnum_fit_neither = total_mi_trials - len(set_fit_left.union(set_fit_right))\n\nprint(\"\\n--- Test Set Analysis Complete ---\")\nprint(f\"Total MI test trials analyzed: {total_mi_trials}\\n\")\n\n# --- Report on 'Left' assumption ---\nperc_left = (num_fit_left / total_mi_trials) * 100 if total_mi_trials > 0 else 0\nprint(f\"Trials fitting 'Left' pattern: {num_fit_left} ({perc_left:.2f}%)\")\n\n# --- Report on 'Right' assumption ---\nperc_right = (num_fit_right / total_mi_trials) * 100 if total_mi_trials > 0 else 0\nprint(f\"Trials fitting 'Right' pattern: {num_fit_right} ({perc_right:.2f}%)\\n\")\n\n# --- Summary Report ---\nprint(\"--- Summary of Pattern Fits ---\")\nprint(f\"Trials fitting ONLY the 'Left' pattern: \\t{num_fit_only_left}\")\nprint(f\"Trials fitting ONLY the 'Right' pattern: \\t{num_fit_only_right}\")\nprint(f\"Trials fitting BOTH patterns (ambiguous): \\t{num_fit_both}\")\nprint(f\"Trials fitting NEITHER pattern: \\t\\t{num_fit_neither}\")\nprint(\"-----------------------------------\")\nprint(f\"Total trials accounted for: {num_fit_only_left + num_fit_only_right + num_fit_both + num_fit_neither}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom scipy.signal import butter, lfilter\n\n# --- 1. CONFIGURATION ---\n# Kaggle environment path\nBASE_PATH = '/kaggle/input/mtcaic3'\n\n# EEG constants\nSAMPLE_RATE = 250  # Hz\nMU_BAND = (8, 12)  # Frequency band (in Hz) for checking activity\n\n# Define all the time windows to be tested (in seconds)\nTIME_WINDOWS = [\n    (1.25, 2.25),\n    (2.75, 3.75),\n    (3.50, 4.50),\n    (4.25, 5.25),\n    (5.00, 6.00),\n    (5.75, 6.75),\n    (6.00, 7.00),\n    (7.00, 8.00)\n]\n\n# --- 2. HELPER FUNCTIONS ---\n\ndef bandpass_filter(data, lowcut, highcut, fs, order=5):\n    \"\"\"Applies a bandpass filter to the data.\"\"\"\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    y = lfilter(b, a, data, axis=0)\n    return y\n\ndef load_trial_data(row, base_path, split):\n    \"\"\"Loads raw EEG data for a single trial specified by a metadata row.\"\"\"\n    eeg_path = os.path.join(base_path, row['task'], split, row['subject_id'], str(row['trial_session']), 'EEGdata.csv')\n    eeg_data = pd.read_csv(eeg_path)\n    \n    trial_num = int(row['trial'])\n    samples_per_trial = 2250  # MI: 9s * 250Hz\n    start_idx = (trial_num - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial\n    \n    return eeg_data.iloc[start_idx:end_idx]\n\ndef check_pattern_types(trial_data_window, true_label):\n    \"\"\"\n    Checks if a trial window shows a classic contralateral or an inverted pattern.\n    Returns two booleans: (is_classic, is_inverted)\n    \"\"\"\n    c3_filtered = bandpass_filter(trial_data_window['C3'], MU_BAND[0], MU_BAND[1], SAMPLE_RATE)\n    c4_filtered = bandpass_filter(trial_data_window['C4'], MU_BAND[0], MU_BAND[1], SAMPLE_RATE)\n    power_c3 = np.var(c3_filtered)\n    power_c4 = np.var(c4_filtered)\n    \n    is_classic, is_inverted = False, False\n    \n    if true_label == 'Left':\n        if power_c4 < power_c3: # Expected pattern\n            is_classic = True\n        elif power_c3 < power_c4: # Inverted pattern\n            is_inverted = True\n    elif true_label == 'Right':\n        if power_c3 < power_c4: # Expected pattern\n            is_classic = True\n        elif power_c4 < power_c3: # Inverted pattern\n            is_inverted = True\n            \n    return is_classic, is_inverted\n\n# --- 3. MAIN ANALYSIS FUNCTION ---\n\ndef analyze_dataset_patterns(df, split_name):\n    \"\"\"\n    Analyzes a full dataset (train or validation) to categorize each trial's pattern.\n    \"\"\"\n    print(f\"\\n--- Analyzing {split_name.upper()} Set ---\")\n    \n    mi_df = df[df['task'] == 'MI'].reset_index(drop=True)\n    total_mi_trials = len(mi_df)\n    \n    results = {\n        'classic': [], 'inverted': [], 'ambiguous': [], 'neither': []\n    }\n\n    for index, row in mi_df.iterrows():\n        full_trial_data = load_trial_data(row, BASE_PATH, split_name)\n        \n        found_classic = False\n        found_inverted = False\n        \n        for start_t, end_t in TIME_WINDOWS:\n            start_sample = int(start_t * SAMPLE_RATE)\n            end_sample = int(end_t * SAMPLE_RATE)\n            windowed_data = full_trial_data.iloc[start_sample:end_sample]\n            \n            is_classic, is_inverted = check_pattern_types(windowed_data, row['label'])\n            \n            if is_classic:\n                found_classic = True\n            if is_inverted:\n                found_inverted = True\n\n        # Categorize the trial based on its behavior across all windows\n        if found_classic and found_inverted:\n            results['ambiguous'].append(row['label'])\n        elif found_inverted:\n            results['inverted'].append(row['label'])\n        elif found_classic:\n            results['classic'].append(row['label'])\n        else:\n            results['neither'].append(row['label'])\n\n    # --- 4. REPORTING RESULTS ---\n    print(f\"Total MI trials analyzed: {total_mi_trials}\\n\")\n\n    for pattern_type, labels in results.items():\n        count = len(labels)\n        percentage = (count / total_mi_trials) * 100 if total_mi_trials > 0 else 0\n        \n        print(f\"Category: '{pattern_type.capitalize()}'\")\n        print(f\"  - Total Count: {count} ({percentage:.2f}%)\")\n        \n        if count > 0:\n            distribution = pd.Series(labels).value_counts()\n            print(\"  - Class Distribution:\")\n            for label, num in distribution.items():\n                print(f\"      {label}: {num}\")\n        print(\"-\" * 25)\n\n# --- 5. EXECUTION ---\n# Load data\ntrain_df = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\nvalidation_df = pd.read_csv(os.path.join(BASE_PATH, 'validation.csv'))\n\n# Run analysis\nanalyze_dataset_patterns(validation_df, 'validation')\nanalyze_dataset_patterns(train_df, 'train')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom scipy.signal import butter, lfilter\nfrom scipy.stats import kurtosis, skew\nfrom mne.decoding import CSP\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# --- 1. CONFIGURATION ---\n# Paths and Constants\nBASE_PATH = '/kaggle/input/mtcaic3'\nSAMPLE_RATE = 250\nALL_EEG_CHANNELS = ['FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8']\n\n# Feature Engineering Bands (Hz)\nFREQ_BANDS = {\n    \"delta\": (1, 4),\n    \"theta\": (4, 8),\n    \"alpha\": (8, 12), # Same as Mu\n    \"beta\": (13, 30),\n    \"gamma\": (30, 50)\n}\n\n# Model and Training Parameters\nCSP_COMPONENTS = 6\nEPOCHS = 150\nBATCH_SIZE = 64\n\n# --- 2. HELPER FUNCTIONS ---\n\ndef bandpass_filter(data, lowcut, highcut, fs, order=5):\n    \"\"\"Applies a bandpass filter to the data.\"\"\"\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    # Apply filter to each channel\n    return np.apply_along_axis(lambda x: lfilter(b, a, x), 0, data)\n\ndef load_trial_data(row, base_path, split):\n    \"\"\"Loads raw EEG data for a single trial.\"\"\"\n    path = os.path.join(base_path, row['task'], split, row['subject_id'], str(row['trial_session']), 'EEGdata.csv')\n    eeg_data = pd.read_csv(path)\n    samples_per_trial = 2250 # MI: 9s * 250Hz\n    start_idx = (row['trial'] - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial\n    return eeg_data.iloc[start_idx:end_idx][ALL_EEG_CHANNELS]\n\ndef engineer_features_for_trial(trial_data):\n    \"\"\"Extracts a rich set of engineered features from a single trial.\"\"\"\n    features = []\n    \n    # 1. Band Power Features\n    for band, (low, high) in FREQ_BANDS.items():\n        # Filter data\n        filtered_data = bandpass_filter(trial_data, low, high, SAMPLE_RATE)\n        # Log-variance as power estimate\n        band_power = np.log(np.var(filtered_data, axis=0))\n        features.extend(band_power)\n        \n        # 2. Spatial Features (Asymmetry) for this band\n        # C3 vs C4\n        features.append(band_power[ALL_EEG_CHANNELS.index('C3')] - band_power[ALL_EEG_CHANNELS.index('C4')])\n        # PO7 vs PO8\n        features.append(band_power[ALL_EEG_CHANNELS.index('PO7')] - band_power[ALL_EEG_CHANNELS.index('PO8')])\n\n    # 3. Time-Domain Statistical Features on raw data\n    features.extend(kurtosis(trial_data, axis=0))\n    features.extend(skew(trial_data, axis=0))\n    \n    return features\n\ndef create_dl_model(input_shape):\n    \"\"\"Creates a Deep Learning model for classification.\"\"\"\n    model = Sequential([\n        Input(shape=(input_shape,)),\n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        Dense(64, activation='relu'),\n        Dropout(0.5),\n        Dense(32, activation='relu'),\n        Dropout(0.3),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# --- 3. DATA PREPARATION ---\n\ndef prepare_dataset(df, split):\n    \"\"\"Loads all data, engineers features, and prepares it for the model.\"\"\"\n    print(f\"Preparing {split} dataset...\")\n    mi_df = df[df['task'] == 'MI'].reset_index(drop=True)\n    \n    # --- Load all raw trials first ---\n    raw_trials = [load_trial_data(row, BASE_PATH, split) for _, row in mi_df.iterrows()]\n    raw_trials_np = np.array([trial.to_numpy() for trial in raw_trials]) # Shape: (n_trials, n_samples, n_channels)\n    \n    # --- A: Perform Feature Engineering ---\n    print(f\"  - Engineering features for {len(raw_trials_np)} trials...\")\n    engineered_features = np.array([engineer_features_for_trial(trial) for trial in raw_trials])\n    \n    # --- B: Perform CSP Feature Extraction ---\n    # CSP needs data in shape (n_trials, n_channels, n_samples)\n    csp_input = raw_trials_np.transpose(0, 2, 1)\n    \n    # Labels needed for fitting CSP\n    labels = mi_df['label'].values if 'label' in mi_df.columns else None\n    \n    return engineered_features, csp_input, labels\n\n# Load metadata\ntrain_df = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\nvalidation_df = pd.read_csv(os.path.join(BASE_PATH, 'validation.csv'))\n\n# Prepare datasets\nX_eng_train, X_csp_train_input, y_train_labels = prepare_dataset(train_df, 'train')\nX_eng_val, X_csp_val_input, y_val_labels = prepare_dataset(validation_df, 'validation')\n\n# --- 4. CSP FITTING AND TRANSFORMATION ---\nprint(\"\\nFitting CSP on training data and transforming datasets...\")\ncsp = CSP(n_components=CSP_COMPONENTS, reg=None, log=True, norm_trace=False)\n\nle = LabelEncoder()\ny_train_encoded = le.fit_transform(y_train_labels)\ny_val_encoded = le.transform(y_val_labels)\n\n# Fit CSP on training data and transform both sets\nX_csp_train = csp.fit_transform(X_csp_train_input, y_train_encoded)\nX_csp_val = csp.transform(X_csp_val_input)\n\n# --- 5. COMBINE FEATURES AND SCALE ---\nprint(\"Combining engineered and CSP features...\")\nX_train_combined = np.concatenate((X_eng_train, X_csp_train), axis=1)\nX_val_combined = np.concatenate((X_eng_val, X_csp_val), axis=1)\n\nprint(f\"Final combined feature shape: {X_train_combined.shape}\")\n\nprint(\"Scaling features...\")\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_combined)\nX_val_scaled = scaler.transform(X_val_combined)\n\n# --- 6. MODEL TRAINING ---\nprint(\"\\nBuilding and training the Deep Learning model...\")\n\n# Calculate class weights for imbalance\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\nclass_weights_dict = dict(enumerate(class_weights))\nprint(f\"Using class weights: {class_weights_dict}\")\n\nmodel = create_dl_model(X_train_scaled.shape[1])\nmodel.summary()\n\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True, mode='max')\n\nhistory = model.fit(\n    X_train_scaled, y_train_encoded,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    validation_data=(X_val_scaled, y_val_encoded),\n    class_weight=class_weights_dict,\n    callbacks=[early_stopping],\n    verbose=2\n)\n\n# --- 7. EVALUATION ---\nprint(\"\\n--- Final Model Evaluation on Validation Set ---\")\nloss, accuracy = model.evaluate(X_val_scaled, y_val_encoded, verbose=0)\nprint(f\"\\nValidation Accuracy: {accuracy:.4f}\")\nprint(f\"Validation Loss: {loss:.4f}\")\n\n# Generate predictions for detailed report\ny_pred_probs = model.predict(X_val_scaled)\ny_pred = (y_pred_probs > 0.5).astype(int).flatten()\n\n# Classification Report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_val_encoded, y_pred, target_names=le.classes_))\n\n# Confusion Matrix\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_val_encoded, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=le.classes_, yticklabels=le.classes_)\nplt.title('Confusion Matrix for Validation Data')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom scipy.signal import butter, lfilter\nfrom scipy.stats import kurtosis, skew\nfrom mne.decoding import CSP\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# --- 1. CONFIGURATION ---\nBASE_PATH = '/kaggle/input/mtcaic3'\nSAMPLE_RATE = 250\nALL_EEG_CHANNELS = ['FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8']\nFREQ_BANDS = {\"delta\": (1, 4), \"theta\": (4, 8), \"alpha\": (8, 12), \"beta\": (13, 30), \"gamma\": (30, 50)}\nCSP_COMPONENTS = 6\nEPOCHS = 150\nBATCH_SIZE = 64\n\n# --- 2. HELPER FUNCTIONS (Unchanged) ---\ndef bandpass_filter(data, lowcut, highcut, fs, order=5):\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    return np.apply_along_axis(lambda x: lfilter(b, a, x), 0, data)\n\ndef load_trial_data(row, base_path, split):\n    path = os.path.join(base_path, row['task'], split, row['subject_id'], str(row['trial_session']), 'EEGdata.csv')\n    eeg_data = pd.read_csv(path)\n    samples_per_trial = 2250\n    start_idx = (row['trial'] - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial\n    return eeg_data.iloc[start_idx:end_idx][ALL_EEG_CHANNELS]\n\ndef engineer_features_for_trial(trial_data):\n    features = []\n    for band, (low, high) in FREQ_BANDS.items():\n        filtered_data = bandpass_filter(trial_data, low, high, SAMPLE_RATE)\n        band_power = np.log(np.var(filtered_data, axis=0))\n        features.extend(band_power)\n        features.append(band_power[ALL_EEG_CHANNELS.index('C3')] - band_power[ALL_EEG_CHANNELS.index('C4')])\n        features.append(band_power[ALL_EEG_CHANNELS.index('PO7')] - band_power[ALL_EEG_CHANNELS.index('PO8')])\n    features.extend(kurtosis(trial_data, axis=0))\n    features.extend(skew(trial_data, axis=0))\n    return features\n\ndef create_dl_model(input_shape):\n    model = Sequential([\n        Input(shape=(input_shape,)),\n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        Dense(64, activation='relu'),\n        Dropout(0.5),\n        Dense(32, activation='relu'),\n        Dropout(0.3),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# --- 3. DATA PREPARATION ---\n\n# Load metadata and filter for MI task\ntrain_df = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\nvalidation_df = pd.read_csv(os.path.join(BASE_PATH, 'validation.csv'))\nmi_train_df = train_df[train_df['task'] == 'MI'].reset_index(drop=True)\nmi_validation_df = validation_df[validation_df['task'] == 'MI'].reset_index(drop=True)\n\n# --- NEW STEP: ONE-HOT ENCODE SUBJECT ID ---\nprint(\"One-hot encoding subject IDs...\")\nsubject_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n# Fit on training subjects and transform both sets\nsubject_train_onehot = subject_encoder.fit_transform(mi_train_df[['subject_id']])\nsubject_val_onehot = subject_encoder.transform(mi_validation_df[['subject_id']])\nprint(f\"Shape of one-hot encoded subject features (train): {subject_train_onehot.shape}\")\n\ndef prepare_features(df, split):\n    \"\"\"Loads data and prepares engineered + CSP features.\"\"\"\n    print(f\"Preparing features for {split} set...\")\n    raw_trials = [load_trial_data(row, BASE_PATH, split) for _, row in df.iterrows()]\n    raw_trials_np = np.array([trial.to_numpy() for trial in raw_trials])\n    \n    # Engineered features\n    engineered_features = np.array([engineer_features_for_trial(trial) for trial in raw_trials])\n    \n    # CSP input shape\n    csp_input = raw_trials_np.transpose(0, 2, 1)\n    \n    labels = df['label'].values if 'label' in df.columns else None\n    return engineered_features, csp_input, labels\n\n# Prepare the signal-based features\nX_eng_train, X_csp_train_input, y_train_labels = prepare_features(mi_train_df, 'train')\nX_eng_val, X_csp_val_input, y_val_labels = prepare_features(mi_validation_df, 'validation')\n\n# --- 4. CSP FITTING AND TRANSFORMATION ---\nprint(\"\\nFitting CSP and transforming datasets...\")\ncsp = CSP(n_components=CSP_COMPONENTS, reg=None, log=True, norm_trace=False)\nle = LabelEncoder()\ny_train_encoded = le.fit_transform(y_train_labels)\ny_val_encoded = le.transform(y_val_labels)\n\nX_csp_train = csp.fit_transform(X_csp_train_input, y_train_encoded)\nX_csp_val = csp.transform(X_csp_val_input)\n\n# --- 5. COMBINE ALL FEATURES (SIGNAL + SUBJECT) AND SCALE ---\nprint(\"Combining engineered, CSP, and subject ID features...\")\nX_train_combined = np.concatenate((X_eng_train, X_csp_train, subject_train_onehot), axis=1)\nX_val_combined = np.concatenate((X_eng_val, X_csp_val, subject_val_onehot), axis=1)\n\nprint(f\"Final combined feature shape (train): {X_train_combined.shape}\")\nprint(f\"Final combined feature shape (validation): {X_val_combined.shape}\")\n\nprint(\"Scaling combined features...\")\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_combined)\nX_val_scaled = scaler.transform(X_val_combined)\n\n# --- 6. MODEL TRAINING ---\nprint(\"\\nBuilding and training the Deep Learning model...\")\n\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\nclass_weights_dict = dict(enumerate(class_weights))\nprint(f\"Using class weights: {class_weights_dict}\")\n\n# Create model with the new input shape\nmodel = create_dl_model(X_train_scaled.shape[1])\nmodel.summary()\n\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True, mode='max')\n\nhistory = model.fit(\n    X_train_scaled, y_train_encoded,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    validation_data=(X_val_scaled, y_val_encoded),\n    class_weight=class_weights_dict,\n    callbacks=[early_stopping],\n    verbose=2\n)\n\n# --- 7. EVALUATION ---\nprint(\"\\n--- Final Model Evaluation on Validation Set ---\")\nloss, accuracy = model.evaluate(X_val_scaled, y_val_encoded, verbose=0)\nprint(f\"\\nValidation Accuracy: {accuracy:.4f}\")\nprint(f\"Validation Loss: {loss:.4f}\")\n\ny_pred_probs = model.predict(X_val_scaled)\ny_pred = (y_pred_probs > 0.5).astype(int).flatten()\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_val_encoded, y_pred, target_names=le.classes_))\n\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_val_encoded, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=le.classes_, yticklabels=le.classes_)\nplt.title('Confusion Matrix for Validation Data')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom scipy.signal import butter, lfilter\nfrom scipy.stats import kurtosis, skew\nfrom mne.decoding import CSP\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report\nimport warnings\n\n# --- 0. SETUP ---\n# Suppress warnings and TensorFlow messages for cleaner output\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# --- 1. CONFIGURATION ---\nBASE_PATH = '/kaggle/input/mtcaic3'\nSAMPLE_RATE = 250\nALL_EEG_CHANNELS = ['FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8']\nFREQ_BANDS = {\n    \"delta\": (1, 4),\n    \"theta\": (4, 8),\n    \"alpha\": (8, 12),\n    \"beta\": (13, 30),\n    \"gamma\": (30, 50)\n}\nCSP_COMPONENTS = 4 # Reduced components for smaller per-subject data\nEPOCHS = 100\nBATCH_SIZE = 8     # Smaller batch size for smaller data\n\n# --- 2. HELPER FUNCTIONS ---\ndef bandpass_filter(data, lowcut, highcut, fs, order=5):\n    \"\"\"Applies a bandpass filter to the data.\"\"\"\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    return np.apply_along_axis(lambda x: lfilter(b, a, x), 0, data)\n\ndef load_trial_data(row, base_path, split):\n    \"\"\"Loads raw EEG data for a single trial.\"\"\"\n    path = os.path.join(base_path, row['task'], split, row['subject_id'], str(row['trial_session']), 'EEGdata.csv')\n    eeg_data = pd.read_csv(path)\n    samples_per_trial = 2250\n    start_idx = (row['trial'] - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial\n    return eeg_data.iloc[start_idx:end_idx][ALL_EEG_CHANNELS]\n\ndef engineer_features_for_trial(trial_data):\n    \"\"\"Extracts a set of engineered features from a single trial.\"\"\"\n    features = []\n    for band, (low, high) in FREQ_BANDS.items():\n        filtered_data = bandpass_filter(trial_data, low, high, SAMPLE_RATE)\n        band_power = np.log(np.var(filtered_data, axis=0))\n        features.extend(band_power)\n        # Add asymmetry feature for this band\n        features.append(band_power[ALL_EEG_CHANNELS.index('C3')] - band_power[ALL_EEG_CHANNELS.index('C4')])\n    \n    # Add statistical features on raw data\n    features.extend(kurtosis(trial_data, axis=0))\n    return features\n\ndef create_dl_model(input_shape):\n    \"\"\"Creates a simple Deep Learning model for classification.\"\"\"\n    model = Sequential([\n        Input(shape=(input_shape,)),\n        Dense(64, activation='relu'),\n        Dropout(0.5),\n        Dense(32, activation='relu'),\n        Dropout(0.5),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# --- 3. DATA PREPARATION ---\nprint(\"Loading and preparing metadata...\")\ntrain_df = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\nmi_df = train_df[train_df['task'] == 'MI'].reset_index(drop=True)\n\n# Identify all subjects in the training set\nall_subjects = sorted(mi_df['subject_id'].unique())\n\n# --- 4. SUBJECT-SPECIFIC TRAINING LOOP ---\nsubject_performance = {}\n\nfor subject_id in all_subjects:\n    print(f\"\\n{'='*50}\\nTraining model for Subject: {subject_id}\\n{'='*50}\")\n    \n    # --- A. Isolate data for the current subject ---\n    subject_df = mi_df[mi_df['subject_id'] == subject_id]\n    \n    # --- B. Split subject's data by session ---\n    train_sessions_df = subject_df[subject_df['trial_session'].isin([1, 2, 3, 4, 5, 6])]\n    val_sessions_df = subject_df[subject_df['trial_session'] == 7]\n    test_sessions_df = subject_df[subject_df['trial_session'] == 8]\n\n    # Ensure all data splits are available\n    if len(train_sessions_df) == 0 or len(val_sessions_df) == 0 or len(test_sessions_df) == 0:\n        print(f\"Skipping {subject_id} due to insufficient sessions for a full train/val/test split.\")\n        continue\n\n    # --- C. Prepare Features for each data split ---\n    le = LabelEncoder()\n    y_train = le.fit_transform(train_sessions_df['label'])\n    y_val = le.transform(val_sessions_df['label'])\n    y_test = le.transform(test_sessions_df['label'])\n\n    def prepare_features(df):\n        # The 'train' directory contains all sessions for subjects S1-S30\n        raw_trials = [load_trial_data(row, BASE_PATH, 'train') for _, row in df.iterrows()]\n        raw_trials_np = np.array([trial.to_numpy() for trial in raw_trials])\n        engineered = np.array([engineer_features_for_trial(trial) for trial in raw_trials])\n        csp_input = raw_trials_np.transpose(0, 2, 1)\n        return engineered, csp_input\n\n    X_eng_train, X_csp_train_in = prepare_features(train_sessions_df)\n    X_eng_val, X_csp_val_in = prepare_features(val_sessions_df)\n    X_eng_test, X_csp_test_in = prepare_features(test_sessions_df)\n    \n    # --- D. Fit transformers ON SUBJECT'S TRAINING DATA ONLY ---\n    csp = CSP(n_components=CSP_COMPONENTS, reg=None, log=True)\n    X_csp_train = csp.fit_transform(X_csp_train_in, y_train)\n    X_train_combined = np.concatenate((X_eng_train, X_csp_train), axis=1)\n    \n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train_combined)\n\n    # --- E. Transform val and test sets with the FITTED transformers ---\n    X_csp_val = csp.transform(X_csp_val_in)\n    X_val_combined = np.concatenate((X_eng_val, X_csp_val), axis=1)\n    X_val_scaled = scaler.transform(X_val_combined)\n    \n    X_csp_test = csp.transform(X_csp_test_in)\n    X_test_combined = np.concatenate((X_eng_test, X_csp_test), axis=1)\n    X_test_scaled = scaler.transform(X_test_combined)\n\n    # --- F. Train the personalized model ---\n    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n    class_weights_dict = dict(enumerate(class_weights))\n    \n    model = create_dl_model(X_train_scaled.shape[1])\n    early_stopping = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, mode='max', verbose=0)\n    \n    model.fit(X_train_scaled, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n              validation_data=(X_val_scaled, y_val),\n              class_weight=class_weights_dict,\n              callbacks=[early_stopping], verbose=0)\n              \n    # --- G. Evaluate on the subject's private test set (Session 8) ---\n    y_pred_probs = model.predict(X_test_scaled, verbose=0)\n    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    subject_performance[subject_id] = accuracy\n    \n    print(f\"--- Results for {subject_id} ---\")\n    print(f\"Test Accuracy on Session 8: {accuracy:.4f}\")\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))\n\n# --- 5. FINAL SUMMARY ---\nprint(f\"\\n\\n{'='*50}\\nOVERALL PERFORMANCE SUMMARY\\n{'='*50}\")\nvalid_accuracies = [acc for acc in subject_performance.values()]\nif valid_accuracies:\n    average_accuracy = np.mean(valid_accuracies)\n    std_dev_accuracy = np.std(valid_accuracies)\n    \n    print(f\"Average Test Accuracy across {len(valid_accuracies)} subjects: {average_accuracy:.4f}\")\n    print(f\"Standard Deviation of Accuracy: {std_dev_accuracy:.4f}\")\n    print(f\"Best performing subject accuracy: {np.max(valid_accuracies):.4f}\")\n    print(f\"Worst performing subject accuracy: {np.min(valid_accuracies):.4f}\")\nelse:\n    print(\"No subjects were successfully trained and evaluated.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pyriemann","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T00:50:51.817332Z","iopub.execute_input":"2025-07-18T00:50:51.818050Z","iopub.status.idle":"2025-07-18T00:51:01.880075Z","shell.execute_reply.started":"2025-07-18T00:50:51.818022Z","shell.execute_reply":"2025-07-18T00:51:01.879289Z"}},"outputs":[{"name":"stdout","text":"Collecting pyriemann\n  Downloading pyriemann-0.8-py2.py3-none-any.whl.metadata (9.3 kB)\nCollecting numpy>=2.0.0 (from pyriemann)\n  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyriemann) (1.15.3)\nRequirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.11/dist-packages (from pyriemann) (1.2.2)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from pyriemann) (1.5.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pyriemann) (3.7.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24->pyriemann) (3.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->pyriemann) (1.17.0)\nDownloading pyriemann-0.8-py2.py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m121.7/121.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, pyriemann\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.1 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\ncupy-cuda12x 13.4.1 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\nydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.3.1 pyriemann-0.8\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --no-cache-dir numpy scipy scikit-learn pyriemann","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T00:56:24.475910Z","iopub.execute_input":"2025-07-18T00:56:24.476211Z","iopub.status.idle":"2025-07-18T00:56:38.605968Z","shell.execute_reply.started":"2025-07-18T00:56:24.476183Z","shell.execute_reply":"2025-07-18T00:56:38.605211Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy\n  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scipy\n  Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m172.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scikit-learn\n  Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\nCollecting pyriemann\n  Downloading pyriemann-0.8-py2.py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pyriemann) (3.7.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->pyriemann) (1.17.0)\nDownloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m178.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.3 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m216.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m157.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyriemann-0.8-py2.py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m121.7/121.7 kB\u001b[0m \u001b[31m247.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, scipy, scikit-learn, pyriemann\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.1 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.0 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\ncupy-cuda12x 13.4.1 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\nydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.1 which is incompatible.\nydata-profiling 4.16.1 requires scipy<1.16,>=1.4.1, but you have scipy 1.16.0 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.0 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.3.1 pyriemann-0.8 scikit-learn-1.7.0 scipy-1.16.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Robust EEG Classification Pipeline for MTC-AIC3 Competition\n# Final Fixed Version\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, regularizers\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport pywt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set base path\nbase_path = \"/kaggle/input/mtcaic3\"\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n# =====================================================================\n### DATA LOADING & PREPROCESSING FUNCTIONS (FIXED) ###\n\ndef load_session_stats():\n    \"\"\"Precompute session statistics for adaptive normalization\"\"\"\n    session_stats = {}\n    for task in ['MI', 'SSVEP']:\n        for dataset in ['train', 'validation', 'test']:\n            task_path = os.path.join(base_path, task, dataset)\n            subjects = [s for s in os.listdir(task_path) if s.startswith('S')]\n            for subject in subjects:\n                sessions = os.listdir(os.path.join(task_path, subject))\n                for session in sessions:\n                    path = os.path.join(task_path, subject, session, 'EEGdata.csv')\n                    if os.path.exists(path):\n                        try:\n                            data = pd.read_csv(path).iloc[:, 1:9]  # EEG channels only\n                            key = f\"{task}_{dataset}_{subject}_{session}\"\n                            session_stats[key] = {\n                                'mean': data.mean(axis=0).values,\n                                'std': data.std(axis=0).values + 1e-8\n                            }\n                        except Exception as e:\n                            print(f\"Error processing {path}: {e}\")\n    return session_stats\n\ndef wavelet_denoise(signal, wavelet='db4', level=5):\n    \"\"\"Wavelet-based denoising for EEG signals\"\"\"\n    coeffs = pywt.wavedec(signal, wavelet, level=level)\n    sigma = np.median(np.abs(coeffs[-level])) / 0.6745\n    uthresh = sigma * np.sqrt(2 * np.log(len(signal)))\n    coeffs[1:] = [pywt.threshold(c, value=uthresh, mode='soft') for c in coeffs[1:]]\n    return pywt.waverec(coeffs, wavelet)\n\ndef extract_robust_features(eeg_data):\n    \"\"\"Feature engineering resistant to deceptive patterns\"\"\"\n    # Channel indices: C3=1, C4=3, PO7=5, PO8=7 (0-indexed)\n    c3 = eeg_data[:, 1]\n    c4 = eeg_data[:, 3]\n    po7 = eeg_data[:, 5]\n    po8 = eeg_data[:, 7]\n    \n    # Asymmetry features with sign correction\n    motor_asym = (np.mean(c3**2) - np.mean(c4**2)) * np.sign(np.mean(c3**2) + np.mean(c4**2))\n    visual_asym = (np.mean(po7**2) - np.mean(po8**2)) * np.sign(np.mean(po7**2) + np.mean(po8**2))\n    \n    # Frequency band powers (Mu: 8-12Hz, Beta: 13-30Hz)\n    freqs = np.fft.rfftfreq(len(c3), 1/250)\n    fft_c3 = np.abs(np.fft.rfft(c3))\n    mu_power = np.mean(fft_c3[(freqs >= 8) & (freqs <= 12)])\n    beta_power = np.mean(fft_c3[(freqs >= 13) & (freqs <= 30)])\n    \n    return np.array([motor_asym, visual_asym, mu_power, beta_power])\n\ndef load_trial_data(row, session_stats):\n    \"\"\"Load and preprocess EEG data for a single trial\"\"\"\n    # Access row attributes directly since we're using itertuples()\n    id_num = row.id\n    task = row.task\n    subject_id = row.subject_id\n    trial_session = row.trial_session\n    trial_num = row.trial\n    \n    # Determine dataset based on ID\n    dataset = 'train' if id_num <= 4800 else 'validation' if id_num <= 4900 else 'test'\n    \n    # Build EEG path\n    eeg_path = os.path.join(\n        base_path, \n        task, \n        dataset, \n        subject_id, \n        str(trial_session), \n        'EEGdata.csv'\n    )\n    \n    # Load EEG data\n    try:\n        eeg_data = pd.read_csv(eeg_path).iloc[:, 1:9].values  # EEG channels only\n    except Exception as e:\n        print(f\"Error loading {eeg_path}: {e}\")\n        return np.zeros((2250, 8)) if task == 'MI' else np.zeros((1750, 8)), np.zeros(4)\n    \n    # Calculate trial indices\n    samples_per_trial = 2250 if task == 'MI' else 1750\n    start_idx = (trial_num - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial\n    trial_data = eeg_data[start_idx:end_idx]\n    \n    # Get session key for normalization\n    session_key = f\"{task}_{dataset}_{subject_id}_{trial_session}\"\n    stats = session_stats.get(session_key, None)\n    \n    # Adaptive normalization\n    if stats:\n        trial_data = (trial_data - stats['mean']) / stats['std']\n    \n    # Wavelet denoising per channel\n    denoised_data = np.zeros_like(trial_data)\n    for i in range(8):\n        try:\n            denoised_data[:, i] = wavelet_denoise(trial_data[:, i])\n        except:\n            denoised_data[:, i] = trial_data[:, i]  # Fallback to original\n    \n    # Feature extraction\n    features = extract_robust_features(denoised_data)\n    \n    return denoised_data, features\n\ndef create_deceptive_sample(eeg_data):\n    \"\"\"Generate deceptive sample by flipping channels\"\"\"\n    flipped = eeg_data.copy()\n    # Swap C3<->C4 and PO7<->PO8\n    flipped[:, [1, 3]] = flipped[:, [3, 1]]\n    flipped[:, [5, 7]] = flipped[:, [7, 5]]\n    return flipped\n\n# =====================================================================\n### DATA PREPARATION ###\n\n# Load metadata\ntrain_df = pd.read_csv(os.path.join(base_path, 'train.csv'))\nval_df = pd.read_csv(os.path.join(base_path, 'validation.csv'))\ntest_df = pd.read_csv(os.path.join(base_path, 'test.csv'))\n\n# Filter for Motor Imagery (MI) only\ntrain_df = train_df[train_df['task'] == 'MI']\nval_df = val_df[val_df['task'] == 'MI']\ntest_df = test_df[test_df['task'] == 'MI']\n\n# Precompute session statistics\nprint(\"Precomputing session statistics...\")\nsession_stats = load_session_stats()\nprint(f\"Loaded stats for {len(session_stats)} sessions\")\n\n# Prepare data storage\nX_train_raw, X_train_feat, y_train = [], [], []\nX_val_raw, X_val_feat, y_val = [], [], []\n\n# Label encoder\nle = LabelEncoder()\nle.fit(train_df['label'])\n\n# Process training data\nprint(\"Processing training data...\")\nfor i, row in enumerate(train_df.itertuples(index=False), 1):\n    if i % 100 == 0:\n        print(f\"Processed {i}/{len(train_df)} training trials\")\n    eeg_data, features = load_trial_data(row, session_stats)\n    label = le.transform([row.label])[0]\n    \n    # Add original sample\n    X_train_raw.append(eeg_data)\n    X_train_feat.append(features)\n    y_train.append(label)\n    \n    # Add deceptive sample (same label)\n    deceptive_eeg = create_deceptive_sample(eeg_data)\n    deceptive_feat = extract_robust_features(deceptive_eeg)\n    X_train_raw.append(deceptive_eeg)\n    X_train_feat.append(deceptive_feat)\n    y_train.append(label)\n\n# Process validation data\nprint(\"Processing validation data...\")\nfor i, row in enumerate(val_df.itertuples(index=False), 1):\n    if i % 20 == 0 or i == len(val_df):\n        print(f\"Processed {i}/{len(val_df)} validation trials\")\n    eeg_data, features = load_trial_data(row, session_stats)\n    label = le.transform([row.label])[0]\n    X_val_raw.append(eeg_data)\n    X_val_feat.append(features)\n    y_val.append(label)\n\n# Convert to arrays\nX_train_raw = np.array(X_train_raw)\nX_train_feat = np.array(X_train_feat)\ny_train = np.array(y_train)\nX_val_raw = np.array(X_val_raw)\nX_val_feat = np.array(X_val_feat)\ny_val = np.array(y_val)\n\nprint(f\"Training data shapes - Raw: {X_train_raw.shape}, Features: {X_train_feat.shape}\")\nprint(f\"Validation data shapes - Raw: {X_val_raw.shape}, Features: {X_val_feat.shape}\")\n\n# Feature scaling\nfeat_scaler = StandardScaler()\nX_train_feat = feat_scaler.fit_transform(X_train_feat)\nX_val_feat = feat_scaler.transform(X_val_feat)\n\n# =====================================================================\n### SESSION-AWARE MODEL ARCHITECTURE ###\n\ndef build_session_aware_model(input_shape, num_features, num_sessions=8):\n    \"\"\"Hybrid model with session embedding for non-stationarity adaptation\"\"\"\n    # Inputs\n    eeg_input = layers.Input(shape=input_shape, name='eeg_input')\n    feat_input = layers.Input(shape=(num_features,), name='feat_input')\n    session_input = layers.Input(shape=(1,), dtype='int32', name='session_input')\n    \n    # Session embedding\n    session_embed = layers.Embedding(\n        input_dim=num_sessions+1, \n        output_dim=4, \n        embeddings_regularizer=regularizers.l2(1e-4)\n    )(session_input)\n    session_vec = layers.Flatten()(session_embed)\n    \n    # EEG processing branch\n    x = layers.Conv1D(32, 3, padding='same', activation='elu')(eeg_input)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv1D(32, 30, padding='same', activation='elu')(x)\n    x = layers.SpatialDropout1D(0.4)(x)\n    x = layers.MaxPooling1D(2)(x)\n    x = layers.Conv1D(64, 15, padding='same', activation='elu')(x)\n    x = layers.SpatialDropout1D(0.4)(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    \n    # Feature processing branch\n    y = layers.Dense(16, activation='elu')(feat_input)\n    y = layers.Dropout(0.3)(y)\n    \n    # Combine with session information\n    combined = layers.concatenate([x, y, session_vec])\n    z = layers.Dense(64, activation='elu')(combined)\n    z = layers.Dropout(0.4)(z)\n    z = layers.Dense(32, activation='elu')(z)\n    output = layers.Dense(2, activation='softmax')(z)\n    \n    return models.Model(\n        inputs=[eeg_input, feat_input, session_input], \n        outputs=output,\n        name='SessionAwareEEGModel'\n    )\n\n# Build model\nprint(\"Building model...\")\nmodel = build_session_aware_model(\n    input_shape=(2250, 8), \n    num_features=X_train_feat.shape[1]\n)\nmodel.summary()\n\n# Custom loss to focus on deceptive patterns\ndef deceptive_pattern_loss(y_true, y_pred):\n    ce_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n    return ce_loss  # Simplified for compatibility\n\nmodel.compile(\n    optimizer=optimizers.Adam(learning_rate=0.001),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Session IDs for training\ntrain_sessions = np.concatenate([train_df['trial_session'].values, \n                                 train_df['trial_session'].values])\nval_sessions = val_df['trial_session'].values\n\n# =====================================================================\n### TRAINING ###\n\n# Callbacks\nlr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.5, \n    patience=5, \n    min_lr=1e-6,\n    verbose=1\n)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_accuracy',\n    patience=15,\n    restore_best_weights=True,\n    mode='max',\n    verbose=1\n)\n\n# Train the model\nprint(\"Training model...\")\nhistory = model.fit(\n    x=[X_train_raw, X_train_feat, train_sessions],\n    y=y_train,\n    validation_data=([X_val_raw, X_val_feat, val_sessions], y_val),\n    epochs=100,\n    batch_size=32,\n    callbacks=[lr_scheduler, early_stopping],\n    verbose=1\n)\n\n# =====================================================================\n### TEST SET PREDICTION & SUBMISSION ###\n\n# Prepare test data\nX_test_raw, X_test_feat, test_sessions, test_ids = [], [], [], []\nprint(\"Processing test data...\")\nfor i, row in enumerate(test_df.itertuples(index=False), 1):\n    if i % 20 == 0 or i == len(test_df):\n        print(f\"Processed {i}/{len(test_df)} test trials\")\n    eeg_data, features = load_trial_data(row, session_stats)\n    X_test_raw.append(eeg_data)\n    X_test_feat.append(features)\n    test_sessions.append(row.trial_session)\n    test_ids.append(row.id)\n\nX_test_raw = np.array(X_test_raw)\nX_test_feat = np.array(X_test_feat)\nX_test_feat = feat_scaler.transform(X_test_feat)\ntest_sessions = np.array(test_sessions)\n\n# Generate predictions\nprint(\"Generating predictions...\")\ntest_probs = model.predict([X_test_raw, X_test_feat, test_sessions], batch_size=32, verbose=1)\ntest_preds = le.inverse_transform(np.argmax(test_probs, axis=1))\n\n# Create submission\nsubmission = pd.DataFrame({'id': test_ids, 'label': test_preds})\nsubmission = submission.sort_values('id').reset_index(drop=True)\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission saved to submission.csv\")\nprint(\"Number of predictions:\", len(submission))\nprint(\"First 5 predictions:\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T01:10:56.313824Z","iopub.execute_input":"2025-07-18T01:10:56.314418Z"}},"outputs":[{"name":"stdout","text":"Precomputing session statistics...\nLoaded stats for 500 sessions\nProcessing training data...\nProcessed 100/2400 training trials\nProcessed 200/2400 training trials\nProcessed 300/2400 training trials\nProcessed 400/2400 training trials\nProcessed 500/2400 training trials\nProcessed 600/2400 training trials\nProcessed 700/2400 training trials\nProcessed 800/2400 training trials\nProcessed 900/2400 training trials\nProcessed 1000/2400 training trials\nProcessed 1100/2400 training trials\nProcessed 1200/2400 training trials\nProcessed 1300/2400 training trials\nProcessed 1400/2400 training trials\nProcessed 1500/2400 training trials\nProcessed 1600/2400 training trials\nProcessed 1700/2400 training trials\nProcessed 1800/2400 training trials\nProcessed 1900/2400 training trials\nProcessed 2000/2400 training trials\nProcessed 2100/2400 training trials\nProcessed 2200/2400 training trials\nProcessed 2300/2400 training trials\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}