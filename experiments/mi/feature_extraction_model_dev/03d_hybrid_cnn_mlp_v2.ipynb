{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":98188,"databundleVersionId":12673416,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =======================================================\n# 1) Setup and imports\n# =======================================================\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score\nfrom scipy.signal import butter, lfilter\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# =======================================================\n# 2) Config\n# =======================================================\nBASE_PATH = '/kaggle/input/mtcaic3'\nCHANNELS = ['FZ', 'C3', 'CZ', 'C4']\nFS = 250\nMU_BAND = (8, 13)\nBETA_BAND = (13, 30)\nMAX_LEN = 2250  # pad/truncate to this length\n\n# =======================================================\n# 3) Bandpass filter\n# =======================================================\ndef butter_bandpass(lowcut, highcut, fs, order=4):\n    nyq = 0.5 * fs\n    low, high = lowcut / nyq, highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    return b, a\n\ndef bandpass_filter(data, lowcut, highcut, fs):\n    b, a = butter_bandpass(lowcut, highcut, fs)\n    return lfilter(b, a, data)\n\n# =======================================================\n# 4) EEG Dataset (shared LabelEncoder)\n# =======================================================\nclass EEGDataset(Dataset):\n    def __init__(self, df, base_path, le):\n        self.df = df.reset_index(drop=True)\n        self.base_path = base_path\n        self.le = le\n        if 'label' in df:\n            self.labels = self.le.transform(df['label'])\n        else:\n            self.labels = None\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        dataset = 'train' if row['id'] <= 4800 else 'validation' if row['id'] <= 4900 else 'test'\n        eeg_path = f\"{self.base_path}/{row['task']}/{dataset}/{row['subject_id']}/{row['trial_session']}/EEGdata.csv\"\n        eeg_data = pd.read_csv(eeg_path)\n\n        samples_per_trial = 2250 if row['task'] == 'MI' else 1750\n        start = (row['trial'] - 1) * samples_per_trial\n        end = start + samples_per_trial\n\n        # Extract channels [C, T]\n        trial = eeg_data.iloc[start:end][CHANNELS].values.T  # (4, T)\n\n        # Pad or truncate\n        if trial.shape[1] < MAX_LEN:\n            pad_width = MAX_LEN - trial.shape[1]\n            trial = np.pad(trial, ((0, 0), (0, pad_width)), mode='constant')\n        elif trial.shape[1] > MAX_LEN:\n            trial = trial[:, :MAX_LEN]\n\n        # Bandpass\n        mu = bandpass_filter(trial, MU_BAND[0], MU_BAND[1], FS)\n        beta = bandpass_filter(trial, BETA_BAND[0], BETA_BAND[1], FS)\n\n        # Improved features: mean, variance, log power\n        means = np.mean(trial, axis=1)\n        variances = np.var(trial, axis=1)\n        diff_c3_c4 = means[1] - means[3]\n        mu_power = np.mean(mu ** 2, axis=1)\n        beta_power = np.mean(beta ** 2, axis=1)\n        log_mu_power = np.log1p(mu_power)\n        log_beta_power = np.log1p(beta_power)\n        tabular = np.concatenate([means, variances, [diff_c3_c4], mu_power, beta_power, log_mu_power, log_beta_power]).astype(np.float32)\n\n        waveform = torch.tensor(trial, dtype=torch.float32)\n        tabular = torch.tensor(tabular, dtype=torch.float32)\n\n        if self.labels is not None:\n            label = torch.tensor(self.labels[idx], dtype=torch.long)\n            return waveform, tabular, label\n        else:\n            return waveform, tabular\n\n# =======================================================\n# 5) Improved CNN + MLP Model\n# =======================================================\nclass CNN_MLP(nn.Module):\n    def __init__(self, in_channels, tabular_dim, num_classes):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv1d(in_channels, 32, 7, padding=3),\n            nn.ReLU(),\n            nn.Conv1d(32, 64, 7, padding=3),\n            nn.ReLU(),\n            nn.Conv1d(64, 128, 7, padding=3),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(1),\n            nn.Flatten()\n        )\n        self.mlp = nn.Sequential(\n            nn.Linear(tabular_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 32),\n            nn.ReLU()\n        )\n        self.final = nn.Linear(128 + 32, num_classes)\n\n    def forward(self, x_wave, x_tab):\n        x1 = self.cnn(x_wave)\n        x2 = self.mlp(x_tab)\n        x = torch.cat([x1, x2], dim=1)\n        return self.final(x)\n\n# =======================================================\n# 6) Prepare MI data only\n# =======================================================\ntrain_df = pd.read_csv(f\"{BASE_PATH}/train.csv\")\nval_df = pd.read_csv(f\"{BASE_PATH}/validation.csv\")\n\n# Filter MI only\ntrain_df = train_df[train_df['task'] == 'MI']\nval_df = val_df[val_df['task'] == 'MI']\n\nprint(\"Train MI shape:\", train_df.shape)\nprint(\"Val MI shape:\", val_df.shape)\n\n# Shared LabelEncoder\nall_labels = pd.concat([train_df['label'], val_df['label']])\nle = LabelEncoder()\nle.fit(all_labels)\n\nprint(\"Classes:\", le.classes_)\n\ntrain_set = EEGDataset(train_df, BASE_PATH, le)\nval_set = EEGDataset(val_df, BASE_PATH, le)\n\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_set, batch_size=32, shuffle=False, num_workers=2)\n\n# =======================================================\n# 7) Training Setup\n# =======================================================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = CNN_MLP(\n    in_channels=len(CHANNELS),\n    tabular_dim=4 + 4 + 1 + 4 + 4 + 4 + 4,  # means, vars, diff, mu, beta, log_mu, log_beta\n    num_classes=len(le.classes_)\n).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=5e-4)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n\nbest_val_f1 = 0\npatience = 15\npatience_counter = 0\n\nhistory = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n\n# =======================================================\n# 8) Train with Early Stopping\n# =======================================================\nfor epoch in range(100):\n    model.train()\n    train_loss = 0.0\n    for x_wave, x_tab, y in train_loader:\n        x_wave, x_tab, y = x_wave.to(device), x_tab.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x_wave, x_tab)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * y.size(0)\n\n    train_loss /= len(train_loader.dataset)\n\n    model.eval()\n    val_loss = 0.0\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for x_wave, x_tab, y in val_loader:\n            x_wave, x_tab, y = x_wave.to(device), x_tab.to(device), y.to(device)\n            out = model(x_wave, x_tab)\n            loss = criterion(out, y)\n            val_loss += loss.item() * y.size(0)\n            y_true.extend(y.cpu().numpy())\n            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n\n    val_loss /= len(val_loader.dataset)\n    val_f1 = f1_score(y_true, y_pred, average='macro')\n\n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['val_f1'].append(val_f1)\n\n    scheduler.step(val_loss)\n\n    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} | Val Loss={val_loss:.4f} | Val F1={val_f1:.4f}\")\n\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        patience_counter = 0\n        torch.save(model.state_dict(), 'best_model.pth')\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(\"✅ Early stopping triggered!\")\n            break\n\n# =======================================================\n# 9) Save logs\n# =======================================================\nnp.savez('training_log.npz', **history)\nprint(\"✅ Training log saved: training_log.npz\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:00:05.003569Z","iopub.execute_input":"2025-06-17T16:00:05.003908Z","iopub.status.idle":"2025-06-17T16:33:50.433883Z","shell.execute_reply.started":"2025-06-17T16:00:05.003883Z","shell.execute_reply":"2025-06-17T16:33:50.432729Z"}},"outputs":[{"name":"stdout","text":"Train MI shape: (2400, 6)\nVal MI shape: (50, 6)\nClasses: ['Left' 'Right']\nEpoch 1: Train Loss=296339.5184 | Val Loss=154853.1388 | Val F1=0.6124\nEpoch 2: Train Loss=88300.6461 | Val Loss=75875.0313 | Val F1=0.3056\nEpoch 3: Train Loss=66881.3079 | Val Loss=46775.6937 | Val F1=0.4624\nEpoch 4: Train Loss=41468.1601 | Val Loss=34253.2384 | Val F1=0.3633\nEpoch 5: Train Loss=33030.3793 | Val Loss=47311.9820 | Val F1=0.3056\nEpoch 6: Train Loss=27537.2437 | Val Loss=42371.2440 | Val F1=0.3779\nEpoch 7: Train Loss=20874.0005 | Val Loss=32207.4491 | Val F1=0.4900\nEpoch 8: Train Loss=17902.8322 | Val Loss=27178.2395 | Val F1=0.4419\nEpoch 9: Train Loss=15305.1729 | Val Loss=41492.5540 | Val F1=0.3056\nEpoch 10: Train Loss=13670.9867 | Val Loss=22681.2041 | Val F1=0.4419\nEpoch 11: Train Loss=11576.2297 | Val Loss=30003.8821 | Val F1=0.5192\nEpoch 12: Train Loss=11960.9229 | Val Loss=11019.2772 | Val F1=0.3905\nEpoch 13: Train Loss=10550.5656 | Val Loss=19623.2946 | Val F1=0.3990\nEpoch 14: Train Loss=11259.2456 | Val Loss=20385.6939 | Val F1=0.4580\nEpoch 15: Train Loss=8043.2511 | Val Loss=20764.5812 | Val F1=0.3485\nEpoch 16: Train Loss=7497.7885 | Val Loss=30514.0830 | Val F1=0.4026\n✅ Early stopping triggered!\n✅ Training log saved: training_log.npz\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"rm -r /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:37:58.469265Z","iopub.execute_input":"2025-06-17T16:37:58.469663Z","iopub.status.idle":"2025-06-17T16:37:58.614471Z","shell.execute_reply.started":"2025-06-17T16:37:58.469629Z","shell.execute_reply":"2025-06-17T16:37:58.613374Z"}},"outputs":[{"name":"stdout","text":"rm: cannot remove '/kaggle/working/*': No such file or directory\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(train_df['label'].value_counts())\nprint(val_df['label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T15:58:23.534324Z","iopub.execute_input":"2025-06-17T15:58:23.534710Z","iopub.status.idle":"2025-06-17T15:58:23.549183Z","shell.execute_reply.started":"2025-06-17T15:58:23.534671Z","shell.execute_reply":"2025-06-17T15:58:23.548233Z"}},"outputs":[{"name":"stdout","text":"label\nRight    1213\nLeft     1187\nName: count, dtype: int64\nlabel\nLeft     28\nRight    22\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm\n\n# -----------------------------------------------------------\n# 1) Dataset\n# -----------------------------------------------------------\n\nclass WaveTabularDataset(Dataset):\n    def __init__(self, wave_data, tabular_data, labels):\n        self.wave_data = wave_data  # (N, L)\n        self.tabular_data = tabular_data  # (N, F)\n        self.labels = labels  # (N,)\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        wave = torch.tensor(self.wave_data[idx], dtype=torch.float32).unsqueeze(0)  # (1, L)\n        tab = torch.tensor(self.tabular_data[idx], dtype=torch.float32)\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        return wave, tab, label\n\n# -----------------------------------------------------------\n# 2) Hybrid Model\n# -----------------------------------------------------------\n\nclass HybridNet(nn.Module):\n    def __init__(self, tab_input_dim=6, tab_hidden_dim=32, wave_output_dim=64, num_classes=2):\n        super(HybridNet, self).__init__()\n\n        # Wave CNN branch\n        self.cnn = nn.Sequential(\n            nn.Conv1d(1, 16, kernel_size=5, padding=2),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n            nn.Conv1d(16, 32, kernel_size=5, padding=2),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n            nn.Flatten(),\n            nn.Linear(32 * 125, wave_output_dim),  # adjust to match your signal length!\n            nn.ReLU()\n        )\n\n        # Tabular MLP branch\n        self.mlp = nn.Sequential(\n            nn.Linear(tab_input_dim, tab_hidden_dim),\n            nn.BatchNorm1d(tab_hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(tab_hidden_dim, tab_hidden_dim),\n            nn.BatchNorm1d(tab_hidden_dim),\n            nn.ReLU()\n        )\n\n        # Fusion MLP\n        self.final = nn.Sequential(\n            nn.Linear(wave_output_dim + tab_hidden_dim, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, num_classes)\n        )\n\n    def forward(self, x_wave, x_tab):\n        x1 = self.cnn(x_wave)\n        x2 = self.mlp(x_tab)\n        x = torch.cat([x1, x2], dim=1)\n        return self.final(x)\n\n# -----------------------------------------------------------\n# 3) Prepare Data (Example)\n# -----------------------------------------------------------\n\n# Simulate your wave and tabular MI data here:\n# Replace with your real data\nN_train, N_val = 2400, 50\nwave_length = 500  # adjust to your real wave length\ntab_features = 6\n\n# Fake data for illustration\nX_wave_train = np.random.randn(N_train, wave_length)\nX_tab_train = np.random.randn(N_train, tab_features)\ny_train = np.random.randint(0, 2, N_train)\n\nX_wave_val = np.random.randn(N_val, wave_length)\nX_tab_val = np.random.randn(N_val, tab_features)\ny_val = np.random.randint(0, 2, N_val)\n\n# Scale tabular MI data\nscaler = StandardScaler()\nX_tab_train = scaler.fit_transform(X_tab_train)\nX_tab_val = scaler.transform(X_tab_val)\n\nprint(f\"Train MI shape: {X_tab_train.shape}\")\nprint(f\"Val MI shape: {X_tab_val.shape}\")\n\n# -----------------------------------------------------------\n# 4) Loaders\n# -----------------------------------------------------------\n\ntrain_ds = WaveTabularDataset(X_wave_train, X_tab_train, y_train)\nval_ds = WaveTabularDataset(X_wave_val, X_tab_val, y_val)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n\n# -----------------------------------------------------------\n# 5) Training Loop\n# -----------------------------------------------------------\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = HybridNet(tab_input_dim=tab_features).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\nbest_f1 = 0\npatience = 10\ncounter = 0\n\ntrain_log = {\n    \"train_loss\": [],\n    \"val_loss\": [],\n    \"val_f1\": []\n}\n\nfor epoch in range(1, 101):\n    model.train()\n    train_losses = []\n    for wave, tab, label in train_loader:\n        wave, tab, label = wave.to(device), tab.to(device), label.to(device)\n        optimizer.zero_grad()\n        output = model(wave, tab)\n        loss = criterion(output, label)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n\n    # Validation\n    model.eval()\n    val_losses = []\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for wave, tab, label in val_loader:\n            wave, tab, label = wave.to(device), tab.to(device), label.to(device)\n            output = model(wave, tab)\n            loss = criterion(output, label)\n            val_losses.append(loss.item())\n            preds = output.argmax(dim=1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(label.cpu().numpy())\n\n    avg_train_loss = np.mean(train_losses)\n    avg_val_loss = np.mean(val_losses)\n    val_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n\n    train_log[\"train_loss\"].append(avg_train_loss)\n    train_log[\"val_loss\"].append(avg_val_loss)\n    train_log[\"val_f1\"].append(val_f1)\n\n    print(f\"Epoch {epoch}: Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Val F1={val_f1:.4f}\")\n\n    # Early stopping\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        counter = 0\n        torch.save(model.state_dict(), \"best_model.pt\")\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"✅ Early stopping triggered!\")\n            break\n\n# Save log\nnp.savez(\"training_log.npz\", **train_log)\nprint(\"✅ Training log saved: training_log.npz\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T18:12:05.766288Z","iopub.execute_input":"2025-06-17T18:12:05.766906Z","iopub.status.idle":"2025-06-17T18:12:39.073651Z","shell.execute_reply.started":"2025-06-17T18:12:05.766868Z","shell.execute_reply":"2025-06-17T18:12:39.072684Z"}},"outputs":[{"name":"stdout","text":"Train MI shape: (2400, 6)\nVal MI shape: (50, 6)\nEpoch 1: Train Loss=0.7331 | Val Loss=0.7601 | Val F1=0.3350\nEpoch 2: Train Loss=0.6687 | Val Loss=0.8044 | Val F1=0.5066\nEpoch 3: Train Loss=0.5496 | Val Loss=1.2042 | Val F1=0.3316\nEpoch 4: Train Loss=0.4290 | Val Loss=1.1002 | Val F1=0.5716\nEpoch 5: Train Loss=0.3097 | Val Loss=1.1869 | Val F1=0.4982\nEpoch 6: Train Loss=0.1894 | Val Loss=1.2206 | Val F1=0.4792\nEpoch 7: Train Loss=0.1278 | Val Loss=1.5125 | Val F1=0.5166\nEpoch 8: Train Loss=0.0905 | Val Loss=1.9723 | Val F1=0.5383\nEpoch 9: Train Loss=0.0627 | Val Loss=2.1319 | Val F1=0.5166\nEpoch 10: Train Loss=0.0466 | Val Loss=2.2201 | Val F1=0.5593\nEpoch 11: Train Loss=0.0383 | Val Loss=2.0463 | Val F1=0.5000\nEpoch 12: Train Loss=0.0452 | Val Loss=1.9553 | Val F1=0.5785\nEpoch 13: Train Loss=0.0536 | Val Loss=3.1011 | Val F1=0.5572\nEpoch 14: Train Loss=0.0447 | Val Loss=2.4327 | Val F1=0.5398\nEpoch 15: Train Loss=0.0318 | Val Loss=2.4301 | Val F1=0.5074\nEpoch 16: Train Loss=0.0401 | Val Loss=2.2330 | Val F1=0.5074\nEpoch 17: Train Loss=0.0368 | Val Loss=2.3007 | Val F1=0.5398\nEpoch 18: Train Loss=0.0432 | Val Loss=2.4761 | Val F1=0.6186\nEpoch 19: Train Loss=0.0595 | Val Loss=2.3806 | Val F1=0.5942\nEpoch 20: Train Loss=0.0504 | Val Loss=2.7010 | Val F1=0.6186\nEpoch 21: Train Loss=0.0440 | Val Loss=2.4062 | Val F1=0.5600\nEpoch 22: Train Loss=0.0454 | Val Loss=2.4864 | Val F1=0.5354\nEpoch 23: Train Loss=0.0287 | Val Loss=2.5128 | Val F1=0.5785\nEpoch 24: Train Loss=0.0265 | Val Loss=3.1882 | Val F1=0.5398\nEpoch 25: Train Loss=0.0220 | Val Loss=3.1744 | Val F1=0.5383\nEpoch 26: Train Loss=0.0173 | Val Loss=2.7776 | Val F1=0.5169\nEpoch 27: Train Loss=0.0168 | Val Loss=2.5781 | Val F1=0.5758\nEpoch 28: Train Loss=0.0144 | Val Loss=3.4479 | Val F1=0.4900\n✅ Early stopping triggered!\n✅ Training log saved: training_log.npz\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm\n\n# ------------------------\n# 1️⃣  Fake data for demo \n# (replace with your real data)\n# ------------------------\nnp.random.seed(0)\nN = 2500\nT = 128  # time steps\nC = 3    # channels\n\nX_wave = np.random.randn(N, C, T)\nX_tab = np.random.randn(N, 6)  # your MI features\ny = np.random.randint(0, 2, size=N)\n\n# ------------------------\n# 2️⃣  Extract TS stats \n# ------------------------\ndef extract_ts_features(X):\n    \"\"\"\n    X: [N, C, T]\n    Return: [N, C * num_stats]\n    \"\"\"\n    mean = X.mean(axis=2)\n    std = X.std(axis=2)\n    min_ = X.min(axis=2)\n    max_ = X.max(axis=2)\n    median = np.median(X, axis=2)\n    ptp = X.ptp(axis=2)  # peak-to-peak\n    # concat along last axis\n    features = np.concatenate([mean, std, min_, max_, median, ptp], axis=1)\n    return features\n\nX_wave_feat = extract_ts_features(X_wave)\nprint(\"Time series stats shape:\", X_wave_feat.shape)\n\n# Combine with MI features\nX_combined = np.concatenate([X_tab, X_wave_feat], axis=1)\nprint(\"Final tabular shape:\", X_combined.shape)\n\n# ------------------------\n# 3️⃣  Train/val split\n# ------------------------\nX_train, X_val, y_train, y_val = train_test_split(\n    X_combined, y, test_size=0.05, random_state=42, stratify=y\n)\n\nprint(f\"Train shape: {X_train.shape}\")\nprint(f\"Val shape: {X_val.shape}\")\n\n# ------------------------\n# 4️⃣  PyTorch Dataset & Loader\n# ------------------------\nclass TabularDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ntrain_ds = TabularDataset(X_train, y_train)\nval_ds = TabularDataset(X_val, y_val)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n\n# ------------------------\n# 5️⃣  Simple MLP Model\n# ------------------------\nclass MLP(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(64, 32),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(32, num_classes)\n        )\n    def forward(self, x):\n        return self.net(x)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MLP(input_dim=X_combined.shape[1], num_classes=2).to(device)\n\n# ------------------------\n# 6️⃣  Loss & Optimizer\n# ------------------------\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n\n# ------------------------\n# 7️⃣  Training loop with early stopping\n# ------------------------\nbest_f1 = 0\npatience = 100\ncounter = 0\nEPOCHS = 100\n\nfor epoch in range(1, EPOCHS+1):\n    # Train\n    model.train()\n    train_loss = 0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        out = model(xb)\n        loss = criterion(out, yb)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * xb.size(0)\n    train_loss /= len(train_loader.dataset)\n\n    # Val\n    model.eval()\n    val_loss = 0\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            loss = criterion(out, yb)\n            val_loss += loss.item() * xb.size(0)\n            preds = out.argmax(dim=1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(yb.cpu().numpy())\n    val_loss /= len(val_loader.dataset)\n    val_f1 = f1_score(all_labels, all_preds)\n\n    print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} | Val Loss={val_loss:.4f} | Val F1={val_f1:.4f}\")\n\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        counter = 0\n        torch.save(model.state_dict(), \"best_mlp.pth\")\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"✅ Early stopping triggered!\")\n            break\n\nprint(f\"Best Val F1: {best_f1:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T18:19:07.138634Z","iopub.execute_input":"2025-06-17T18:19:07.138993Z","iopub.status.idle":"2025-06-17T18:19:24.203006Z","shell.execute_reply.started":"2025-06-17T18:19:07.138964Z","shell.execute_reply":"2025-06-17T18:19:24.201977Z"}},"outputs":[{"name":"stdout","text":"Time series stats shape: (2500, 18)\nFinal tabular shape: (2500, 24)\nTrain shape: (2375, 24)\nVal shape: (125, 24)\nEpoch 1: Train Loss=0.7443 | Val Loss=0.6992 | Val F1=0.4211\nEpoch 2: Train Loss=0.7140 | Val Loss=0.6946 | Val F1=0.4038\nEpoch 3: Train Loss=0.7057 | Val Loss=0.6970 | Val F1=0.4103\nEpoch 4: Train Loss=0.7009 | Val Loss=0.6969 | Val F1=0.1579\nEpoch 5: Train Loss=0.7030 | Val Loss=0.6988 | Val F1=0.3529\nEpoch 6: Train Loss=0.6966 | Val Loss=0.6987 | Val F1=0.3011\nEpoch 7: Train Loss=0.6952 | Val Loss=0.6972 | Val F1=0.3208\nEpoch 8: Train Loss=0.6918 | Val Loss=0.6962 | Val F1=0.2529\nEpoch 9: Train Loss=0.6948 | Val Loss=0.6961 | Val F1=0.2637\nEpoch 10: Train Loss=0.6956 | Val Loss=0.6942 | Val F1=0.4444\nEpoch 11: Train Loss=0.6947 | Val Loss=0.6936 | Val F1=0.3111\nEpoch 12: Train Loss=0.6928 | Val Loss=0.6945 | Val F1=0.3107\nEpoch 13: Train Loss=0.6948 | Val Loss=0.6943 | Val F1=0.1928\nEpoch 14: Train Loss=0.6914 | Val Loss=0.6955 | Val F1=0.3469\nEpoch 15: Train Loss=0.6930 | Val Loss=0.6956 | Val F1=0.2680\nEpoch 16: Train Loss=0.6937 | Val Loss=0.6953 | Val F1=0.3670\nEpoch 17: Train Loss=0.6927 | Val Loss=0.6949 | Val F1=0.2970\nEpoch 18: Train Loss=0.6915 | Val Loss=0.6957 | Val F1=0.3366\nEpoch 19: Train Loss=0.6906 | Val Loss=0.6963 | Val F1=0.4355\nEpoch 20: Train Loss=0.6928 | Val Loss=0.6960 | Val F1=0.4298\nEpoch 21: Train Loss=0.6913 | Val Loss=0.6972 | Val F1=0.4274\nEpoch 22: Train Loss=0.6897 | Val Loss=0.6969 | Val F1=0.3774\nEpoch 23: Train Loss=0.6921 | Val Loss=0.6965 | Val F1=0.3333\nEpoch 24: Train Loss=0.6899 | Val Loss=0.6989 | Val F1=0.3137\nEpoch 25: Train Loss=0.6892 | Val Loss=0.6987 | Val F1=0.3564\nEpoch 26: Train Loss=0.6909 | Val Loss=0.6963 | Val F1=0.3495\nEpoch 27: Train Loss=0.6869 | Val Loss=0.6972 | Val F1=0.4425\nEpoch 28: Train Loss=0.6908 | Val Loss=0.7007 | Val F1=0.3883\nEpoch 29: Train Loss=0.6896 | Val Loss=0.7015 | Val F1=0.4667\nEpoch 30: Train Loss=0.6877 | Val Loss=0.7017 | Val F1=0.4839\nEpoch 31: Train Loss=0.6897 | Val Loss=0.7006 | Val F1=0.5197\nEpoch 32: Train Loss=0.6925 | Val Loss=0.7028 | Val F1=0.3200\nEpoch 33: Train Loss=0.6910 | Val Loss=0.6999 | Val F1=0.4211\nEpoch 34: Train Loss=0.6861 | Val Loss=0.7042 | Val F1=0.4370\nEpoch 35: Train Loss=0.6845 | Val Loss=0.7037 | Val F1=0.4425\nEpoch 36: Train Loss=0.6856 | Val Loss=0.7065 | Val F1=0.4248\nEpoch 37: Train Loss=0.6875 | Val Loss=0.7018 | Val F1=0.4915\nEpoch 38: Train Loss=0.6853 | Val Loss=0.7053 | Val F1=0.4754\nEpoch 39: Train Loss=0.6859 | Val Loss=0.6999 | Val F1=0.4272\nEpoch 40: Train Loss=0.6848 | Val Loss=0.7037 | Val F1=0.4522\nEpoch 41: Train Loss=0.6843 | Val Loss=0.7018 | Val F1=0.4706\nEpoch 42: Train Loss=0.6847 | Val Loss=0.7040 | Val F1=0.4754\nEpoch 43: Train Loss=0.6835 | Val Loss=0.7057 | Val F1=0.4310\nEpoch 44: Train Loss=0.6881 | Val Loss=0.7044 | Val F1=0.5231\nEpoch 45: Train Loss=0.6812 | Val Loss=0.7065 | Val F1=0.4074\nEpoch 46: Train Loss=0.6831 | Val Loss=0.7033 | Val F1=0.4821\nEpoch 47: Train Loss=0.6836 | Val Loss=0.7002 | Val F1=0.4833\nEpoch 48: Train Loss=0.6876 | Val Loss=0.7007 | Val F1=0.4643\nEpoch 49: Train Loss=0.6820 | Val Loss=0.6999 | Val F1=0.5323\nEpoch 50: Train Loss=0.6816 | Val Loss=0.7058 | Val F1=0.4464\nEpoch 51: Train Loss=0.6833 | Val Loss=0.7059 | Val F1=0.3889\nEpoch 52: Train Loss=0.6824 | Val Loss=0.7077 | Val F1=0.4074\nEpoch 53: Train Loss=0.6860 | Val Loss=0.7058 | Val F1=0.5000\nEpoch 54: Train Loss=0.6829 | Val Loss=0.7057 | Val F1=0.4174\nEpoch 55: Train Loss=0.6782 | Val Loss=0.7053 | Val F1=0.4715\nEpoch 56: Train Loss=0.6861 | Val Loss=0.7067 | Val F1=0.4348\nEpoch 57: Train Loss=0.6811 | Val Loss=0.7098 | Val F1=0.5000\nEpoch 58: Train Loss=0.6835 | Val Loss=0.7080 | Val F1=0.4553\nEpoch 59: Train Loss=0.6805 | Val Loss=0.7102 | Val F1=0.4248\nEpoch 60: Train Loss=0.6786 | Val Loss=0.7074 | Val F1=0.5037\nEpoch 61: Train Loss=0.6780 | Val Loss=0.7094 | Val F1=0.5362\nEpoch 62: Train Loss=0.6837 | Val Loss=0.7038 | Val F1=0.5203\nEpoch 63: Train Loss=0.6764 | Val Loss=0.7079 | Val F1=0.4921\nEpoch 64: Train Loss=0.6776 | Val Loss=0.7086 | Val F1=0.4444\nEpoch 65: Train Loss=0.6780 | Val Loss=0.7063 | Val F1=0.4915\nEpoch 66: Train Loss=0.6766 | Val Loss=0.7125 | Val F1=0.4874\nEpoch 67: Train Loss=0.6778 | Val Loss=0.7088 | Val F1=0.4793\nEpoch 68: Train Loss=0.6747 | Val Loss=0.7160 | Val F1=0.4615\nEpoch 69: Train Loss=0.6803 | Val Loss=0.7110 | Val F1=0.4957\nEpoch 70: Train Loss=0.6774 | Val Loss=0.7076 | Val F1=0.4074\nEpoch 71: Train Loss=0.6802 | Val Loss=0.7098 | Val F1=0.4957\nEpoch 72: Train Loss=0.6760 | Val Loss=0.7118 | Val F1=0.4074\nEpoch 73: Train Loss=0.6811 | Val Loss=0.7061 | Val F1=0.4587\nEpoch 74: Train Loss=0.6811 | Val Loss=0.7110 | Val F1=0.4833\nEpoch 75: Train Loss=0.6774 | Val Loss=0.7116 | Val F1=0.5042\nEpoch 76: Train Loss=0.6771 | Val Loss=0.7091 | Val F1=0.4259\nEpoch 77: Train Loss=0.6780 | Val Loss=0.7085 | Val F1=0.5000\nEpoch 78: Train Loss=0.6742 | Val Loss=0.7105 | Val F1=0.4615\nEpoch 79: Train Loss=0.6779 | Val Loss=0.7080 | Val F1=0.4724\nEpoch 80: Train Loss=0.6742 | Val Loss=0.7108 | Val F1=0.4561\nEpoch 81: Train Loss=0.6811 | Val Loss=0.7148 | Val F1=0.4516\nEpoch 82: Train Loss=0.6757 | Val Loss=0.7137 | Val F1=0.4800\nEpoch 83: Train Loss=0.6750 | Val Loss=0.7087 | Val F1=0.4839\nEpoch 84: Train Loss=0.6720 | Val Loss=0.7184 | Val F1=0.4779\nEpoch 85: Train Loss=0.6716 | Val Loss=0.7138 | Val F1=0.4425\nEpoch 86: Train Loss=0.6710 | Val Loss=0.7170 | Val F1=0.4655\nEpoch 87: Train Loss=0.6691 | Val Loss=0.7173 | Val F1=0.4324\nEpoch 88: Train Loss=0.6740 | Val Loss=0.7119 | Val F1=0.4340\nEpoch 89: Train Loss=0.6747 | Val Loss=0.7159 | Val F1=0.4000\nEpoch 90: Train Loss=0.6692 | Val Loss=0.7099 | Val F1=0.4833\nEpoch 91: Train Loss=0.6715 | Val Loss=0.7113 | Val F1=0.5238\nEpoch 92: Train Loss=0.6728 | Val Loss=0.7089 | Val F1=0.4464\nEpoch 93: Train Loss=0.6728 | Val Loss=0.7110 | Val F1=0.4144\nEpoch 94: Train Loss=0.6798 | Val Loss=0.7067 | Val F1=0.5082\nEpoch 95: Train Loss=0.6682 | Val Loss=0.7122 | Val F1=0.4425\nEpoch 96: Train Loss=0.6744 | Val Loss=0.7108 | Val F1=0.4286\nEpoch 97: Train Loss=0.6759 | Val Loss=0.7129 | Val F1=0.4828\nEpoch 98: Train Loss=0.6703 | Val Loss=0.7166 | Val F1=0.4248\nEpoch 99: Train Loss=0.6756 | Val Loss=0.7136 | Val F1=0.4655\nEpoch 100: Train Loss=0.6736 | Val Loss=0.7122 | Val F1=0.4364\nBest Val F1: 0.5362\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# =======================================================\n# ✅ 1) Setup and imports\n# =======================================================\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\nfrom scipy.signal import butter, lfilter\nfrom mne.decoding import CSP\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# =======================================================\n# ✅ 2) Config\n# =======================================================\n\nBASE_PATH = '/kaggle/input/mtcaic3'\nCHANNELS = ['FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8']\nFS = 250\nMU_BAND = (8, 13)\nBETA_BAND = (13, 30)\nMAX_LEN = 2250\nSHORT_LEN = 500\nN_CSP_COMPONENTS = 2\n\n# =======================================================\n# ✅ 3) Bandpass filter\n# =======================================================\n\ndef butter_bandpass(lowcut, highcut, fs, order=4):\n    nyq = 0.5 * fs\n    low, high = lowcut / nyq, highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    return b, a\n\ndef bandpass_filter(data, lowcut, highcut, fs):\n    b, a = butter_bandpass(lowcut, highcut, fs)\n    return lfilter(b, a, data)\n\n# =======================================================\n# ✅ 4) Load data & label encoder\n# =======================================================\n\ntrain_df = pd.read_csv(f\"{BASE_PATH}/train.csv\")\nval_df = pd.read_csv(f\"{BASE_PATH}/validation.csv\")\n\ntrain_df = train_df[train_df['task'] == 'MI']\nval_df = val_df[val_df['task'] == 'MI']\n\nprint(\"Train MI shape:\", train_df.shape)\nprint(\"Val MI shape:\", val_df.shape)\n\nle = LabelEncoder()\nle.fit(pd.concat([train_df['label'], val_df['label']]))\nprint(\"Classes:\", le.classes_)\n\n# =======================================================\n# ✅ 5) Fit CSP\n# =======================================================\n\nX_csp, y_csp = [], []\nfor _, row in train_df.iterrows():\n    eeg_path = f\"{BASE_PATH}/{row['task']}/train/{row['subject_id']}/{row['trial_session']}/EEGdata.csv\"\n    eeg_data = pd.read_csv(eeg_path)\n    start = (row['trial'] - 1) * 2250\n    end = start + 2250\n    trial = eeg_data.iloc[start:end][CHANNELS].values.T\n    X_csp.append(trial[:, :SHORT_LEN])\n    y_csp.append(le.transform([row['label']])[0])\n\nX_csp = np.stack(X_csp)\ny_csp = np.array(y_csp)\nprint(\"X_csp shape:\", X_csp.shape)\nprint(\"y_csp shape:\", y_csp.shape)\n\ncsp = CSP(n_components=N_CSP_COMPONENTS, reg='ledoit_wolf', log=True)\ncsp.fit(X_csp, y_csp)\nprint(\"✅ CSP fitted.\")\n\n# =======================================================\n# ✅ 6) Dataset\n# =======================================================\n\nclass EEGDataset(Dataset):\n    def __init__(self, df, base_path, le, csp):\n        self.df = df.reset_index(drop=True)\n        self.base_path = base_path\n        self.le = le\n        self.csp = csp\n        if 'label' in df:\n            self.labels = self.le.transform(df['label'])\n        else:\n            self.labels = None\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        dataset = 'train' if row['id'] <= 4800 else 'validation'\n        eeg_path = f\"{self.base_path}/{row['task']}/{dataset}/{row['subject_id']}/{row['trial_session']}/EEGdata.csv\"\n        eeg_data = pd.read_csv(eeg_path)\n        start = (row['trial'] - 1) * 2250\n        end = start + 2250\n        trial = eeg_data.iloc[start:end][CHANNELS].values.T\n\n        if trial.shape[1] < MAX_LEN:\n            pad = MAX_LEN - trial.shape[1]\n            trial = np.pad(trial, ((0, 0), (0, pad)), mode='constant')\n        elif trial.shape[1] > MAX_LEN:\n            trial = trial[:, :MAX_LEN]\n\n        mu = bandpass_filter(trial, MU_BAND[0], MU_BAND[1], FS)\n        beta = bandpass_filter(trial, BETA_BAND[0], BETA_BAND[1], FS)\n\n        means = np.mean(trial, axis=1)\n        vars_ = np.var(trial, axis=1)\n        diff = means[CHANNELS.index('C3')] - means[CHANNELS.index('C4')]\n        mu_power = np.mean(mu**2, axis=1)\n        beta_power = np.mean(beta**2, axis=1)\n        log_mu = np.log1p(mu_power)\n        log_beta = np.log1p(beta_power)\n\n        tabular = np.concatenate([means, vars_, [diff], mu_power, beta_power, log_mu, log_beta]).astype(np.float32)\n        csp_feats = self.csp.transform(trial[:, :SHORT_LEN][np.newaxis, ...]).squeeze().astype(np.float32)\n\n        waveform = torch.tensor(trial, dtype=torch.float32)\n        tabular = torch.tensor(tabular, dtype=torch.float32)\n        csp_feats = torch.tensor(csp_feats, dtype=torch.float32)\n\n        if self.labels is not None:\n            label = torch.tensor(self.labels[idx], dtype=torch.long)\n            return waveform, tabular, csp_feats, label\n        else:\n            return waveform, tabular, csp_feats\n\n# =======================================================\n# ✅ 7) Dataloaders\n# =======================================================\n\ntrain_set = EEGDataset(train_df, BASE_PATH, le, csp)\nval_set = EEGDataset(val_df, BASE_PATH, le, csp)\n\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_set, batch_size=32, shuffle=False, num_workers=2)\n\n# =======================================================\n# ✅ 8) Model: CNN + MLP + CSP\n# =======================================================\n\nclass CNN_MLP_CSP(nn.Module):\n    def __init__(self, in_channels, tabular_dim, csp_dim, num_classes):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv1d(in_channels, 32, 7, padding=3),\n            nn.ReLU(),\n            nn.Conv1d(32, 64, 7, padding=3),\n            nn.ReLU(),\n            nn.Conv1d(64, 128, 7, padding=3),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(1),\n            nn.Flatten()\n        )\n        self.mlp = nn.Sequential(\n            nn.Linear(tabular_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 32),\n            nn.ReLU()\n        )\n        self.csp_layer = nn.Sequential(\n            nn.Linear(csp_dim, 16),\n            nn.ReLU()\n        )\n        self.final = nn.Linear(128 + 32 + 16, num_classes)\n\n    def forward(self, x_wave, x_tab, x_csp):\n        x1 = self.cnn(x_wave)\n        x2 = self.mlp(x_tab)\n        x3 = self.csp_layer(x_csp)\n        x = torch.cat([x1, x2, x3], dim=1)\n        return self.final(x)\n\n# ✅ Corrected tabular_dim\nTABULAR_DIM = 8 * 6 + 1  # 49\n\n# =======================================================\n# ✅ 9) Train setup\n# =======================================================\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = CNN_MLP_CSP(\n    in_channels=len(CHANNELS),\n    tabular_dim=TABULAR_DIM,\n    csp_dim=N_CSP_COMPONENTS,\n    num_classes=len(le.classes_)\n).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=5e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n\nbest_f1 = 0\npatience = 15\ncounter = 0\n\nhistory = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n\n# =======================================================\n# ✅ 10) Training loop\n# =======================================================\n\nfor epoch in range(100):\n    model.train()\n    train_loss = 0.0\n    for x_wave, x_tab, x_csp, y in train_loader:\n        x_wave, x_tab, x_csp, y = x_wave.to(device), x_tab.to(device), x_csp.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x_wave, x_tab, x_csp)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * y.size(0)\n    train_loss /= len(train_loader.dataset)\n\n    model.eval()\n    val_loss = 0.0\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for x_wave, x_tab, x_csp, y in val_loader:\n            x_wave, x_tab, x_csp, y = x_wave.to(device), x_tab.to(device), x_csp.to(device), y.to(device)\n            out = model(x_wave, x_tab, x_csp)\n            val_loss += criterion(out, y).item() * y.size(0)\n            y_true.extend(y.cpu().numpy())\n            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n\n    val_loss /= len(val_loader.dataset)\n    val_f1 = f1_score(y_true, y_pred, average='macro')\n    scheduler.step(val_loss)\n\n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['val_f1'].append(val_f1)\n\n    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} | Val Loss={val_loss:.4f} | Val F1={val_f1:.4f}\")\n\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        counter = 0\n        torch.save(model.state_dict(), 'best_model.pth')\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"✅ Early stopping triggered.\")\n            break\n\nnp.savez('training_log.npz', **history)\nprint(\"✅ Training log saved.\")\n\n# =======================================================\n# ✅ 11) Confusion Matrix\n# =======================================================\n\nmodel.load_state_dict(torch.load('best_model.pth'))\nmodel.eval()\ny_true, y_pred = [], []\nwith torch.no_grad():\n    for x_wave, x_tab, x_csp, y in val_loader:\n        x_wave, x_tab, x_csp, y = x_wave.to(device), x_tab.to(device), x_csp.to(device), y.to(device)\n        out = model(x_wave, x_tab, x_csp)\n        y_true.extend(y.cpu().numpy())\n        y_pred.extend(out.argmax(dim=1).cpu().numpy())\n\ncm = confusion_matrix(y_true, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\nfig, ax = plt.subplots(figsize=(6,6))\ndisp.plot(ax=ax, cmap='Blues')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T20:12:29.708766Z","iopub.execute_input":"2025-06-17T20:12:29.709110Z","execution_failed":"2025-06-17T21:35:48.527Z"}},"outputs":[{"name":"stdout","text":"Train MI shape: (2400, 6)\nVal MI shape: (50, 6)\nClasses: ['Left' 'Right']\nX_csp shape: (2400, 8, 500)\ny_csp shape: (2400,)\nComputing rank from data with rank=None\n    Using tolerance 1.7e+06 (2.2e-16 eps * 8 dim * 9.7e+20  max singular value)\n    Estimated rank (data): 8\n    data: rank 8 computed from 8 data channels with 0 projectors\nReducing data rank from 8 -> 8\nEstimating class=0 covariance using LEDOIT_WOLF\nDone.\nEstimating class=1 covariance using LEDOIT_WOLF\nDone.\n✅ CSP fitted.\nEpoch 1: Train Loss=200686.4249 | Val Loss=152534.4675 | Val F1=0.4156\nEpoch 2: Train Loss=82880.4779 | Val Loss=61687.7453 | Val F1=0.4391\nEpoch 3: Train Loss=39837.5791 | Val Loss=45520.0488 | Val F1=0.4907\nEpoch 4: Train Loss=35179.7054 | Val Loss=15817.2716 | Val F1=0.3961\nEpoch 5: Train Loss=23479.1575 | Val Loss=30597.1006 | Val F1=0.3990\nEpoch 6: Train Loss=16628.1290 | Val Loss=8919.7069 | Val F1=0.4949\nEpoch 7: Train Loss=15095.2325 | Val Loss=39823.5575 | Val F1=0.3990\nEpoch 8: Train Loss=11274.9839 | Val Loss=22965.2550 | Val F1=0.3485\nEpoch 9: Train Loss=11071.7980 | Val Loss=17164.8013 | Val F1=0.3485\nEpoch 10: Train Loss=6637.4261 | Val Loss=12918.3518 | Val F1=0.3990\nEpoch 11: Train Loss=4932.4369 | Val Loss=7677.7070 | Val F1=0.3485\nEpoch 12: Train Loss=5418.1077 | Val Loss=7537.6962 | Val F1=0.3485\nEpoch 13: Train Loss=3494.2316 | Val Loss=5964.7882 | Val F1=0.3633\nEpoch 14: Train Loss=3920.3154 | Val Loss=6943.7711 | Val F1=0.4505\nEpoch 15: Train Loss=3927.4212 | Val Loss=5518.9263 | Val F1=0.3056\nEpoch 16: Train Loss=3805.7210 | Val Loss=1815.5944 | Val F1=0.3506\nEpoch 17: Train Loss=3395.3517 | Val Loss=6106.1632 | Val F1=0.3056\nEpoch 18: Train Loss=3289.8151 | Val Loss=5945.3059 | Val F1=0.3056\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}